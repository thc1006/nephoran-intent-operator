//go:build go1.24

package regression

import (
	"context"
	"fmt"
	"math"
	"sync"
	"time"

	"github.com/prometheus/client_golang/api"
	v1 "github.com/prometheus/client_golang/api/prometheus/v1"
	"github.com/prometheus/common/model"
	"k8s.io/klog/v2"
)

// IntelligentRegressionEngine provides ML-enhanced regression detection with NWDAF patterns
type IntelligentRegressionEngine struct {
	config              *IntelligentRegressionConfig
	prometheusClient    v1.API
	baselineManager     *BaselineManager
	anomalyDetector     *AnomalyDetector
	changePointDetector *ChangePointDetector
	forecastingEngine   *ForecastingEngine
	alertManager        *IntelligentAlertManager
	learningSystem      *ContinuousLearningSystem
	nwdafAnalyzer       *NWDAFAnalyzer
	metricStore         *MetricStore
	correlationEngine   *CorrelationEngine

	// State management
	activeBenchmarks  map[string]*BenchmarkExecution
	regressionHistory *RegressionHistory
	alertSuppressions map[string]*AlertSuppression

	mutex sync.RWMutex
}

// IntelligentRegressionConfig configures the intelligent regression detection
type IntelligentRegressionConfig struct {
	// Core detection parameters
	DetectionInterval      time.Duration `json:"detectionInterval"`
	BaselineUpdateInterval time.Duration `json:"baselineUpdateInterval"`
	MinDataPoints          int           `json:"minDataPoints"`
	ConfidenceThreshold    float64       `json:"confidenceThreshold"`

	// ML parameters
	AnomalyDetectionEnabled     bool `json:"anomalyDetectionEnabled"`
	ChangePointDetectionEnabled bool `json:"changePointDetectionEnabled"`
	SeasonalityDetectionEnabled bool `json:"seasonalityDetectionEnabled"`
	ForecastingEnabled          bool `json:"forecastingEnabled"`
	LearningEnabled             bool `json:"learningEnabled"`

	// NWDAF integration
	NWDAFPatternsEnabled      bool `json:"nwdafPatternsEnabled"`
	TelecomKPIAnalysisEnabled bool `json:"telecomKPIAnalysisEnabled"`
	NetworkSliceAwareAnalysis bool `json:"networkSliceAwareAnalysis"`

	// Alert configuration
	AlertLevels            []string      `json:"alertLevels"`
	AlertCooldownPeriod    time.Duration `json:"alertCooldownPeriod"`
	AlertCorrelationWindow time.Duration `json:"alertCorrelationWindow"`
	MaxConcurrentAlerts    int           `json:"maxConcurrentAlerts"`

	// Performance targets (aligned with Nephoran claims)
	PerformanceTargets   *PerformanceTargets   `json:"performanceTargets"`
	RegressionThresholds *RegressionThresholds `json:"regressionThresholds"`

	// Integration settings
	PrometheusEndpoint     string   `json:"prometheusEndpoint"`
	GrafanaIntegration     bool     `json:"grafanaIntegration"`
	WebhookEndpoints       []string `json:"webhookEndpoints"`
	ExternalSystemsEnabled bool     `json:"externalSystemsEnabled"`
}

// PerformanceTargets defines expected performance levels
type PerformanceTargets struct {
	LatencyP95Ms           float64 `json:"latencyP95Ms"`           // 2000ms target
	LatencyP99Ms           float64 `json:"latencyP99Ms"`           // 5000ms target
	ThroughputRpm          float64 `json:"throughputRpm"`          // 45 intents/min
	AvailabilityPct        float64 `json:"availabilityPct"`        // 99.95%
	CacheHitRatePct        float64 `json:"cacheHitRatePct"`        // 87%
	ErrorRatePct           float64 `json:"errorRatePct"`           // <0.5%
	ResourceUtilizationPct float64 `json:"resourceUtilizationPct"` // <80%
	ConcurrentIntents      int     `json:"concurrentIntents"`      // 200
}

// RegressionThresholds defines when to trigger regression alerts
type RegressionThresholds struct {
	LatencyIncreasePct      float64 `json:"latencyIncreasePct"`      // 15% increase
	ThroughputDecreasePct   float64 `json:"throughputDecreasePct"`   // 10% decrease
	AvailabilityDecreasePct float64 `json:"availabilityDecreasePct"` // 0.1% decrease
	ErrorRateIncreasePct    float64 `json:"errorRateIncreasePct"`    // 100% increase
	CacheHitRateDecreasePct float64 `json:"cacheHitRateDecreasePct"` // 5% decrease
	ResourceIncreaseAbsPct  float64 `json:"resourceIncreaseAbsPct"`  // 10% absolute increase
}

// BaselineManager handles dynamic baseline management
type BaselineManager struct {
	currentBaselines     map[string]*DynamicBaseline
	baselineHistory      []*BaselineSnapshot
	adaptationEnabled    bool
	seasonalAdjustment   bool
	outlierFilterEnabled bool
	baselineQualityScore float64
	mutex                sync.RWMutex
}

// DynamicBaseline represents an adaptive performance baseline
type DynamicBaseline struct {
	ID         string    `json:"id"`
	MetricName string    `json:"metricName"`
	CreatedAt  time.Time `json:"createdAt"`
	UpdatedAt  time.Time `json:"updatedAt"`
	ValidUntil time.Time `json:"validUntil"`

	// Statistical properties
	Mean              float64            `json:"mean"`
	StandardDeviation float64            `json:"standardDeviation"`
	Percentiles       map[string]float64 `json:"percentiles"`
	ConfidenceBounds  *ConfidenceBounds  `json:"confidenceBounds"`

	// Adaptive features
	SeasonalPattern *SeasonalPattern `json:"seasonalPattern"`
	TrendComponent  *TrendComponent  `json:"trendComponent"`
	CyclicalPattern *CyclicalPattern `json:"cyclicalPattern"`

	// Quality metrics
	DataQuality        float64 `json:"dataQuality"`
	Stability          float64 `json:"stability"`
	Representativeness float64 `json:"representativeness"`
	SampleSize         int     `json:"sampleSize"`

	// Metadata
	ContextTags     map[string]string `json:"contextTags"`
	EnvironmentInfo *EnvironmentInfo  `json:"environmentInfo"`
	SystemLoad      *SystemLoadInfo   `json:"systemLoad"`
}

// ConfidenceBounds represents statistical confidence intervals
type ConfidenceBounds struct {
	Lower95 float64 `json:"lower95"`
	Upper95 float64 `json:"upper95"`
	Lower99 float64 `json:"lower99"`
	Upper99 float64 `json:"upper99"`
}

// AnomalyDetector implements multiple anomaly detection algorithms
type AnomalyDetector struct {
	algorithms         []AnomalyAlgorithm
	ensembleWeights    map[string]float64
	adaptiveThresholds map[string]float64
	historicalData     *TimeSeriesData
	config             *AnomalyDetectionConfig
}

// AnomalyAlgorithm interface for different anomaly detection methods
type AnomalyAlgorithm interface {
	DetectAnomalies(data []float64, timestamps []time.Time) ([]*AnomalyEvent, error)
	GetName() string
	GetParameters() map[string]interface{}
	UpdateParameters(params map[string]interface{}) error
}

// StatisticalAnomalyDetector uses statistical methods (IQR, Z-score, etc.)
type StatisticalAnomalyDetector struct {
	name       string
	parameters map[string]interface{}
}

// IsolationForestDetector implements Isolation Forest algorithm
type IsolationForestDetector struct {
	name       string
	parameters map[string]interface{}
	trees      []*IsolationTree
}

// LocalOutlierFactorDetector implements LOF algorithm
type LocalOutlierFactorDetector struct {
	name       string
	parameters map[string]interface{}
	neighbors  int
}

// ChangePointDetector detects structural breaks in time series
type ChangePointDetector struct {
	algorithms        []ChangePointAlgorithm
	sensitivityLevel  float64
	minimumDistance   int
	historicalChanges []*ChangePoint
	config            *ChangePointConfig
}

// ChangePointAlgorithm interface for change point detection methods
type ChangePointAlgorithm interface {
	DetectChangePoints(data []float64, timestamps []time.Time) ([]*ChangePoint, error)
	GetName() string
}

// BayesianChangePointDetector implements Bayesian change point detection
type BayesianChangePointDetector struct {
	priorProb  float64
	hazardRate float64
}

// ForecastingEngine provides performance prediction capabilities
type ForecastingEngine struct {
	models          map[string]ForecastModel
	ensembleWeights map[string]float64
	forecastHorizon time.Duration
	updateInterval  time.Duration
	historicalData  *TimeSeriesData
	predictionCache *PredictionCache
}

// ForecastModel interface for different forecasting methods
type ForecastModel interface {
	Fit(data *TimeSeriesData) error
	Predict(horizon time.Duration) (*Forecast, error)
	GetAccuracy() *AccuracyMetrics
	GetName() string
}

// ARIMAModel implements ARIMA forecasting
type ARIMAModel struct {
	name       string
	p, d, q    int
	parameters []float64
	residuals  []float64
	accuracy   *AccuracyMetrics
}

// ExponentialSmoothingModel implements exponential smoothing
type ExponentialSmoothingModel struct {
	name     string
	alpha    float64
	beta     float64
	gamma    float64
	seasonal bool
	accuracy *AccuracyMetrics
}

// Forecast represents prediction results
type Forecast struct {
	Horizon        time.Duration        `json:"horizon"`
	Predictions    []ForecastPoint      `json:"predictions"`
	Confidence     []ConfidenceInterval `json:"confidence"`
	ModelUsed      string               `json:"modelUsed"`
	Accuracy       *AccuracyMetrics     `json:"accuracy"`
	RiskIndicators []*RiskIndicator     `json:"riskIndicators"`
	CreatedAt      time.Time            `json:"createdAt"`
}

// ForecastPoint represents a single prediction
type ForecastPoint struct {
	Timestamp      time.Time `json:"timestamp"`
	PredictedValue float64   `json:"predictedValue"`
	LowerBound     float64   `json:"lowerBound"`
	UpperBound     float64   `json:"upperBound"`
}

// RiskIndicator identifies potential future performance risks
type RiskIndicator struct {
	Type         string    `json:"type"`
	Severity     string    `json:"severity"`
	Probability  float64   `json:"probability"`
	ExpectedTime time.Time `json:"expectedTime"`
	Description  string    `json:"description"`
	Mitigation   []string  `json:"mitigation"`
}

// AlertRule defines conditions for triggering alerts
type AlertRule struct {
	ID               string            `json:"id"`
	Name             string            `json:"name"`
	Metric           string            `json:"metric"`
	Condition        *AlertCondition   `json:"condition"`
	Severity         string            `json:"severity"`
	Labels           map[string]string `json:"labels"`
	Annotations      map[string]string `json:"annotations"`
	CooldownPeriod   time.Duration     `json:"cooldownPeriod"`
	EscalationPolicy string            `json:"escalationPolicy"`
	Enabled          bool              `json:"enabled"`
}

// AlertCondition defines the condition for triggering an alert
type AlertCondition struct {
	Operator         string        `json:"operator"` // "gt", "lt", "eq", "ne", "increase", "decrease"
	Threshold        float64       `json:"threshold"`
	Duration         time.Duration `json:"duration"`         // Condition must be true for this duration
	ComparisonWindow time.Duration `json:"comparisonWindow"` // For trend-based conditions
}

// SuppressionRule defines when to suppress alerts
type SuppressionRule struct {
	ID        string            `json:"id"`
	Name      string            `json:"name"`
	Matcher   map[string]string `json:"matcher"`
	StartTime time.Time         `json:"startTime"`
	EndTime   time.Time         `json:"endTime"`
	Reason    string            `json:"reason"`
	CreatedBy string            `json:"createdBy"`
}

// ContinuousLearningSystem implements self-improving detection algorithms
type ContinuousLearningSystem struct {
	config             *LearningConfig
	feedbackStore      *FeedbackStore
	modelRepository    *ModelRepository
	trainingScheduler  *TrainingScheduler
	performanceTracker *LearningPerformanceTracker

	// Learning components
	feedbackProcessor *FeedbackProcessor
	featureExtractor  *FeatureExtractor
	modelOptimizer    *ModelOptimizer
	knowledgeBase     *KnowledgeBase
}

// FeedbackStore stores human feedback and system outcomes
type FeedbackStore struct {
	feedbackEntries []*FeedbackEntry
	outcomes        []*AlertOutcome
	annotations     []*HumanAnnotation
}

// FeedbackEntry represents feedback on alert quality
type FeedbackEntry struct {
	AlertID      string    `json:"alertId"`
	Timestamp    time.Time `json:"timestamp"`
	UserID       string    `json:"userId"`
	FeedbackType string    `json:"feedbackType"` // "true_positive", "false_positive", "false_negative"
	Confidence   float64   `json:"confidence"`
	Notes        string    `json:"notes"`
	Resolution   string    `json:"resolution"`
}

// NetworkSliceAnalyzer provides slice-aware performance analysis
type NetworkSliceAnalyzer struct {
	sliceProfiles      map[string]*SliceProfile
	performanceKPIs    map[string]*SliceKPIs
	slaViolations      []*SLAViolation
	resourceAllocation map[string]*ResourceAllocation
}

// SliceProfile defines performance characteristics of a network slice
type SliceProfile struct {
	SliceID         string                `json:"sliceId"`
	SliceType       string                `json:"sliceType"` // "eMBB", "URLLC", "mMTC"
	ServiceType     string                `json:"serviceType"`
	PerformanceReqs *SlicePerformanceReqs `json:"performanceReqs"`
	ResourceProfile *ResourceProfile      `json:"resourceProfile"`
	QoSProfile      *QoSProfile           `json:"qosProfile"`
	GeographicScope *GeographicScope      `json:"geographicScope"`
}

// SlicePerformanceReqs defines performance requirements for a slice
type SlicePerformanceReqs struct {
	MaxLatencyMs       float64 `json:"maxLatencyMs"`
	MinThroughputMbps  float64 `json:"minThroughputMbps"`
	MaxPacketLossRate  float64 `json:"maxPacketLossRate"`
	MinAvailabilityPct float64 `json:"minAvailabilityPct"`
	MaxJitterMs        float64 `json:"maxJitterMs"`
	SecurityLevel      string  `json:"securityLevel"`
}

// CorrelationEngine analyzes relationships between metrics and external factors
type CorrelationEngine struct {
	metricStore         *MetricStore
	externalDataSources []ExternalDataSource
	correlationCache    map[string]*CorrelationResult
	lagAnalyzer         *LagAnalyzer
	causalityAnalyzer   *CausalityAnalyzer
}

// CorrelationResult represents correlation analysis results
type CorrelationResult struct {
	MetricPair         [2]string           `json:"metricPair"`
	CorrelationCoeff   float64             `json:"correlationCoeff"`
	PValue             float64             `json:"pValue"`
	Significance       bool                `json:"significance"`
	CorrelationType    string              `json:"correlationType"` // "linear", "nonlinear", "lagged"
	LagPeriod          time.Duration       `json:"lagPeriod"`
	AnalysisMethod     string              `json:"analysisMethod"`
	SampleSize         int                 `json:"sampleSize"`
	ConfidenceInterval *ConfidenceInterval `json:"confidenceInterval"`
}

// MetricStore provides efficient storage and retrieval of time series data
type MetricStore struct {
	timeSeriesDB    TimeSeriesDatabase
	aggregator      *MetricAggregator
	compressor      *DataCompressor
	indexer         *MetricIndexer
	retentionPolicy *RetentionPolicy

	// Caching layer
	recentDataCache map[string]*CachedMetricData
	aggregateCache  map[string]*CachedAggregateData
	mutex           sync.RWMutex
}

// Advanced types for complex analysis

// RegressionAnalysisResult represents comprehensive regression analysis
type RegressionAnalysisResult struct {
	*RegressionAnalysis // Embed base analysis

	// Enhanced analysis
	AnomalyEvents       []*AnomalyEvent      `json:"anomalyEvents"`
	ChangePoints        []*ChangePoint       `json:"changePoints"`
	Forecast            *Forecast            `json:"forecast"`
	CorrelationAnalysis *CorrelationAnalysis `json:"correlationAnalysis"`
	NWDAFInsights       *NWDAFInsights       `json:"nwdafInsights"`
	LearningFeedback    *LearningFeedback    `json:"learningFeedback"`

	// Performance impact assessment
	ServiceImpact        *ServiceImpactAssessment  `json:"serviceImpact"`
	UserExperienceImpact *UserExperienceImpact     `json:"userExperienceImpact"`
	BusinessImpact       *BusinessImpactAssessment `json:"businessImpact"`

	// Remediation suggestions
	AutomatedRemediation []AutomatedAction   `json:"automatedRemediation"`
	ManualRemediation    []ManualAction      `json:"manualRemediation"`
	PreventiveMeasures   []PreventiveMeasure `json:"preventiveMeasures"`
}

// ServiceImpactAssessment evaluates impact on service delivery
type ServiceImpactAssessment struct {
	AffectedServices         []string      `json:"affectedServices"`
	ServiceDegradationPct    float64       `json:"serviceDegradationPct"`
	EstimatedUserImpact      int           `json:"estimatedUserImpact"`
	SLAViolationRisk         float64       `json:"slaViolationRisk"`
	RecoveryTimeEstimate     time.Duration `json:"recoveryTimeEstimate"`
	BusinessCriticalityScore float64       `json:"businessCriticalityScore"`
}

// AutomatedAction represents actions that can be taken automatically
type AutomatedAction struct {
	ActionType       string                 `json:"actionType"`
	Description      string                 `json:"description"`
	Parameters       map[string]interface{} `json:"parameters"`
	SafetyChecks     []string               `json:"safetyChecks"`
	RollbackPlan     *RollbackPlan          `json:"rollbackPlan"`
	SuccessMetrics   []string               `json:"successMetrics"`
	ExecutionTimeout time.Duration          `json:"executionTimeout"`
	ApprovalRequired bool                   `json:"approvalRequired"`
}

// NewIntelligentRegressionEngine creates a new intelligent regression detection engine
func NewIntelligentRegressionEngine(config *IntelligentRegressionConfig) (*IntelligentRegressionEngine, error) {
	if config == nil {
		config = getDefaultIntelligentRegressionConfig()
	}

	// Initialize Prometheus client
	promClient, err := api.NewClient(api.Config{
		Address: config.PrometheusEndpoint,
	})
	if err != nil {
		return nil, fmt.Errorf("failed to create Prometheus client: %w", err)
	}

	engine := &IntelligentRegressionEngine{
		config:              config,
		prometheusClient:    v1.NewAPI(promClient),
		baselineManager:     NewBaselineManager(),
		anomalyDetector:     NewAnomalyDetector(config),
		changePointDetector: NewChangePointDetector(config),
		forecastingEngine:   NewForecastingEngine(config),
		alertManager:        NewIntelligentAlertManager(convertToAlertManagerConfig(config)),
		learningSystem:      NewContinuousLearningSystem(config),
		nwdafAnalyzer:       NewNWDAFAnalyzer(convertToNWDAFConfig(config)),
		metricStore:         NewMetricStore(config),
		correlationEngine:   NewCorrelationEngine(config),

		activeBenchmarks:  make(map[string]*BenchmarkExecution),
		regressionHistory: NewRegressionHistory(),
		alertSuppressions: make(map[string]*AlertSuppression),
	}

	return engine, nil
}

// Start begins the continuous regression detection process
func (ire *IntelligentRegressionEngine) Start(ctx context.Context) error {
	klog.Info("Starting Intelligent Regression Detection Engine")

	// Start baseline management
	go ire.runBaselineManagement(ctx)

	// Start continuous monitoring
	go ire.runContinuousMonitoring(ctx)

	// Start learning system
	if ire.config.LearningEnabled {
		go ire.runLearningSystem(ctx)
	}

	// Start NWDAF analytics
	if ire.config.NWDAFPatternsEnabled {
		go ire.runNWDAFAnalytics(ctx)
	}

	return nil
}

// runContinuousMonitoring performs continuous regression detection
func (ire *IntelligentRegressionEngine) runContinuousMonitoring(ctx context.Context) {
	ticker := time.NewTicker(ire.config.DetectionInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			klog.Info("Stopping continuous monitoring")
			return
		case <-ticker.C:
			if err := ire.performRegressionAnalysis(ctx); err != nil {
				klog.Errorf("Regression analysis failed: %v", err)
			}
		}
	}
}

// performRegressionAnalysis executes comprehensive regression analysis
func (ire *IntelligentRegressionEngine) performRegressionAnalysis(ctx context.Context) error {
	klog.V(2).Info("Starting comprehensive regression analysis")

	// 1. Collect current metrics from Prometheus
	metrics, err := ire.collectCurrentMetrics(ctx)
	if err != nil {
		return fmt.Errorf("failed to collect metrics: %w", err)
	}

	// 2. Detect anomalies in current data
	anomalies, err := ire.anomalyDetector.DetectAnomaliesInMetrics(metrics)
	if err != nil {
		klog.Warningf("Anomaly detection failed: %v", err)
	}

	// 3. Detect change points
	changePoints, err := ire.changePointDetector.DetectChangePointsInMetrics(metrics)
	if err != nil {
		klog.Warningf("Change point detection failed: %v", err)
	}

	// 4. Perform baseline comparison
	regressions := make([]MetricRegression, 0)
	for metricName, currentValue := range metrics {
		baseline := ire.baselineManager.GetBaseline(metricName)
		if baseline == nil {
			continue
		}

		regression, err := ire.analyzeMetricRegression(metricName, currentValue, baseline)
		if err != nil {
			klog.Warningf("Failed to analyze regression for %s: %v", metricName, err)
			continue
		}

		if regression.IsSignificant {
			regressions = append(regressions, *regression)
		}
	}

	// 5. Generate forecast if significant regressions detected
	var forecast *Forecast
	if len(regressions) > 0 && ire.config.ForecastingEnabled {
		forecast, err = ire.forecastingEngine.GenerateForecast(ctx, metrics)
		if err != nil {
			klog.Warningf("Forecasting failed: %v", err)
		}
	}

	// 6. Perform correlation analysis
	correlations, err := ire.correlationEngine.AnalyzeCorrelations(metrics)
	if err != nil {
		klog.Warningf("Correlation analysis failed: %v", err)
	}

	// 7. Generate NWDAF insights if enabled
	var nwdafInsights *NWDAFInsights
	if ire.config.NWDAFPatternsEnabled {
		// Convert to pointer slice for NWDAF analyzer
		regressionPtrs := make([]*MetricRegression, len(regressions))
		for i := range regressions {
			regressionPtrs[i] = &regressions[i]
		}
		nwdafInsights, err = ire.nwdafAnalyzer.GenerateInsights(ctx, metrics, regressionPtrs)
		if err != nil {
			klog.Warningf("NWDAF analysis failed: %v", err)
		}
	}

	// 8. Create comprehensive analysis result
	analysis := &RegressionAnalysisResult{
		RegressionAnalysis: &RegressionAnalysis{
			AnalysisID:        fmt.Sprintf("intelligent-regression-%d", time.Now().Unix()),
			Timestamp:         time.Now(),
			HasRegression:     len(regressions) > 0,
			MetricRegressions: regressions,
			ConfidenceScore:   ire.calculateOverallConfidence(regressions, anomalies, changePoints),
		},
		AnomalyEvents:       anomalies,
		ChangePoints:        changePoints,
		Forecast:            forecast,
		CorrelationAnalysis: correlations,
		NWDAFInsights:       nwdafInsights,
	}

	// 9. Assess impact and generate recommendations
	ire.assessImpactAndGenerateRecommendations(analysis)

	// 10. Process through learning system
	if ire.config.LearningEnabled {
		ire.learningSystem.ProcessAnalysis(analysis)
	}

	// 11. Generate and send alerts if necessary
	if analysis.HasRegression {
		if err := ire.alertManager.ProcessRegressionAnalysis(analysis); err != nil {
			klog.Errorf("Failed to process alerts: %v", err)
		}
	}

	// 12. Update regression history
	ire.regressionHistory.Add(analysis)

	klog.V(2).Infof("Regression analysis completed: regressions=%d, anomalies=%d, changePoints=%d",
		len(regressions), len(anomalies), len(changePoints))

	return nil
}

// collectCurrentMetrics retrieves current performance metrics from Prometheus
func (ire *IntelligentRegressionEngine) collectCurrentMetrics(ctx context.Context) (map[string]float64, error) {
	metrics := make(map[string]float64)

	// Define key metrics to collect based on Nephoran performance targets
	metricQueries := map[string]string{
		"intent_processing_latency_p95": `histogram_quantile(0.95, nephoran_networkintent_duration_seconds)`,
		"intent_processing_latency_p99": `histogram_quantile(0.99, nephoran_networkintent_duration_seconds)`,
		"intent_throughput_per_minute":  `rate(nephoran_networkintent_total[1m]) * 60`,
		"llm_processing_latency_p95":    `histogram_quantile(0.95, nephoran_llm_request_duration_seconds)`,
		"rag_cache_hit_rate":            `rate(nephoran_rag_cache_hits_total[5m]) / (rate(nephoran_rag_cache_hits_total[5m]) + rate(nephoran_rag_cache_misses_total[5m])) * 100`,
		"error_rate":                    `rate(nephoran_networkintent_retries_total[5m])`,
		"availability":                  `avg(nephoran_controller_health_status) * 100`,
		"cpu_utilization":               `avg(rate(container_cpu_usage_seconds_total{container="nephoran-operator"}[5m])) * 100`,
		"memory_utilization":            `avg(container_memory_working_set_bytes{container="nephoran-operator"}) / avg(container_spec_memory_limit_bytes{container="nephoran-operator"}) * 100`,
		"concurrent_processing":         `sum(nephoran_worker_queue_depth)`,
	}

	for metricName, query := range metricQueries {
		result, warnings, err := ire.prometheusClient.Query(ctx, query, time.Now())
		if err != nil {
			klog.Warningf("Failed to query metric %s: %v", metricName, err)
			continue
		}

		if len(warnings) > 0 {
			klog.Warningf("Warnings for metric %s: %v", metricName, warnings)
		}

		if result.Type() == model.ValVector {
			vector := result.(model.Vector)
			if len(vector) > 0 {
				metrics[metricName] = float64(vector[0].Value)
			}
		}
	}

	return metrics, nil
}

// analyzeMetricRegression performs detailed regression analysis for a single metric
func (ire *IntelligentRegressionEngine) analyzeMetricRegression(metricName string, currentValue float64, baseline *DynamicBaseline) (*MetricRegression, error) {
	// Calculate basic regression metrics
	absoluteChange := currentValue - baseline.Mean
	var relativeChange float64
	if baseline.Mean != 0 {
		relativeChange = (absoluteChange / baseline.Mean) * 100
	}

	// Determine if regression is significant using confidence bounds
	isSignificant := false
	if currentValue < baseline.ConfidenceBounds.Lower95 || currentValue > baseline.ConfidenceBounds.Upper95 {
		isSignificant = true
	}

	// Calculate effect size (Cohen's d)
	effectSize := 0.0
	if baseline.StandardDeviation > 0 {
		effectSize = absoluteChange / baseline.StandardDeviation
	}

	// Determine severity based on metric type and thresholds
	severity := ire.determineSeverity(metricName, relativeChange, currentValue)

	// Calculate p-value (simplified - would use proper statistical test in production)
	zScore := effectSize
	// For now, use a simplified p-value calculation
	pValue := 2 * (1 - normalCDF(math.Abs(zScore)))

	regression := &MetricRegression{
		MetricName:        metricName,
		BaselineValue:     baseline.Mean,
		CurrentValue:      currentValue,
		AbsoluteChange:    absoluteChange,
		RelativeChangePct: relativeChange,
		IsSignificant:     isSignificant,
		PValue:            pValue,
		EffectSize:        effectSize,
		Severity:          severity,
		Impact:            ire.calculateImpact(metricName, relativeChange),
		TrendDirection:    ire.determineTrendDirection(metricName, relativeChange),
		RegressionType:    ire.determineRegressionType(metricName),
	}

	return regression, nil
}

// determineSeverity determines the severity level of a regression
func (ire *IntelligentRegressionEngine) determineSeverity(metricName string, relativeChange, currentValue float64) string {
	thresholds := ire.config.RegressionThresholds
	targets := ire.config.PerformanceTargets

	switch metricName {
	case "intent_processing_latency_p95", "intent_processing_latency_p99", "llm_processing_latency_p95":
		if currentValue > targets.LatencyP95Ms*1.5 || relativeChange > 50 {
			return "Critical"
		} else if currentValue > targets.LatencyP95Ms || relativeChange > thresholds.LatencyIncreasePct {
			return "High"
		} else if relativeChange > thresholds.LatencyIncreasePct/2 {
			return "Medium"
		}

	case "intent_throughput_per_minute":
		if currentValue < targets.ThroughputRpm*0.5 || relativeChange < -50 {
			return "Critical"
		} else if currentValue < targets.ThroughputRpm || relativeChange < -thresholds.ThroughputDecreasePct {
			return "High"
		} else if relativeChange < -thresholds.ThroughputDecreasePct/2 {
			return "Medium"
		}

	case "availability":
		if currentValue < 99.0 || relativeChange < -1.0 {
			return "Critical"
		} else if currentValue < targets.AvailabilityPct || relativeChange < -thresholds.AvailabilityDecreasePct {
			return "High"
		} else if relativeChange < -thresholds.AvailabilityDecreasePct/2 {
			return "Medium"
		}

	case "error_rate":
		if currentValue > 5.0 || relativeChange > 200 {
			return "Critical"
		} else if currentValue > targets.ErrorRatePct || relativeChange > thresholds.ErrorRateIncreasePct {
			return "High"
		} else if relativeChange > thresholds.ErrorRateIncreasePct/2 {
			return "Medium"
		}

	case "rag_cache_hit_rate":
		if currentValue < targets.CacheHitRatePct*0.8 || relativeChange < -20 {
			return "Critical"
		} else if currentValue < targets.CacheHitRatePct || relativeChange < -thresholds.CacheHitRateDecreasePct {
			return "High"
		} else if relativeChange < -thresholds.CacheHitRateDecreasePct/2 {
			return "Medium"
		}

	default:
		if math.Abs(relativeChange) > 50 {
			return "Critical"
		} else if math.Abs(relativeChange) > 20 {
			return "High"
		} else if math.Abs(relativeChange) > 10 {
			return "Medium"
		}
	}

	return "Low"
}

// calculateOverallConfidence calculates overall confidence in regression detection
func (ire *IntelligentRegressionEngine) calculateOverallConfidence(regressions []MetricRegression, anomalies []*AnomalyEvent, changePoints []*ChangePoint) float64 {
	if len(regressions) == 0 {
		return 0.0
	}

	// Base confidence from regression analysis
	var totalConfidence float64
	criticalCount, highCount := 0, 0

	for _, regression := range regressions {
		switch regression.Severity {
		case "Critical":
			criticalCount++
			totalConfidence += 0.95
		case "High":
			highCount++
			totalConfidence += 0.85
		case "Medium":
			totalConfidence += 0.75
		case "Low":
			totalConfidence += 0.60
		}
	}

	baseConfidence := totalConfidence / float64(len(regressions))

	// Adjust confidence based on supporting evidence
	anomalyWeight := math.Min(float64(len(anomalies))*0.05, 0.15)
	changePointWeight := math.Min(float64(len(changePoints))*0.05, 0.15)

	// Boost confidence if multiple critical or high severity regressions
	severityBoost := 0.0
	if criticalCount > 0 {
		severityBoost += 0.1
	}
	if criticalCount > 1 || (criticalCount > 0 && highCount > 0) {
		severityBoost += 0.1
	}

	finalConfidence := baseConfidence + anomalyWeight + changePointWeight + severityBoost

	// Cap at 0.99 to avoid overconfidence
	return math.Min(finalConfidence, 0.99)
}

// Supporting methods and default configuration

func getDefaultIntelligentRegressionConfig() *IntelligentRegressionConfig {
	return &IntelligentRegressionConfig{
		DetectionInterval:           5 * time.Minute,
		BaselineUpdateInterval:      1 * time.Hour,
		MinDataPoints:               30,
		ConfidenceThreshold:         0.80,
		AnomalyDetectionEnabled:     true,
		ChangePointDetectionEnabled: true,
		SeasonalityDetectionEnabled: true,
		ForecastingEnabled:          true,
		LearningEnabled:             true,
		NWDAFPatternsEnabled:        true,
		TelecomKPIAnalysisEnabled:   true,
		NetworkSliceAwareAnalysis:   true,
		AlertLevels:                 []string{"Critical", "High", "Medium"},
		AlertCooldownPeriod:         15 * time.Minute,
		AlertCorrelationWindow:      5 * time.Minute,
		MaxConcurrentAlerts:         10,
		PrometheusEndpoint:          "http://prometheus:9090",
		GrafanaIntegration:          true,
		ExternalSystemsEnabled:      true,

		PerformanceTargets: &PerformanceTargets{
			LatencyP95Ms:           2000.0,
			LatencyP99Ms:           5000.0,
			ThroughputRpm:          45.0,
			AvailabilityPct:        99.95,
			CacheHitRatePct:        87.0,
			ErrorRatePct:           0.5,
			ResourceUtilizationPct: 80.0,
			ConcurrentIntents:      200,
		},

		RegressionThresholds: &RegressionThresholds{
			LatencyIncreasePct:      15.0,
			ThroughputDecreasePct:   10.0,
			AvailabilityDecreasePct: 0.1,
			ErrorRateIncreasePct:    100.0,
			CacheHitRateDecreasePct: 5.0,
			ResourceIncreaseAbsPct:  10.0,
		},
	}
}

// Placeholder implementations for complex components
func NewBaselineManager() *BaselineManager {
	return &BaselineManager{
		currentBaselines:     make(map[string]*DynamicBaseline),
		adaptationEnabled:    true,
		seasonalAdjustment:   true,
		outlierFilterEnabled: true,
		baselineQualityScore: 0.85,
	}
}

func NewAnomalyDetector(config *IntelligentRegressionConfig) *AnomalyDetector {
	return &AnomalyDetector{
		algorithms:         []AnomalyAlgorithm{},
		ensembleWeights:    make(map[string]float64),
		adaptiveThresholds: make(map[string]float64),
	}
}

func NewChangePointDetector(config *IntelligentRegressionConfig) *ChangePointDetector {
	return &ChangePointDetector{
		algorithms:       []ChangePointAlgorithm{},
		sensitivityLevel: 0.05,
		minimumDistance:  10,
	}
}

func NewForecastingEngine(config *IntelligentRegressionConfig) *ForecastingEngine {
	return &ForecastingEngine{
		models:          make(map[string]ForecastModel),
		ensembleWeights: make(map[string]float64),
		forecastHorizon: 24 * time.Hour,
		updateInterval:  1 * time.Hour,
	}
}

func NewContinuousLearningSystem(config *IntelligentRegressionConfig) *ContinuousLearningSystem {
	return &ContinuousLearningSystem{}
}

func NewCorrelationEngine(config *IntelligentRegressionConfig) *CorrelationEngine {
	return &CorrelationEngine{
		correlationCache: make(map[string]*CorrelationResult),
	}
}

func NewMetricStore(config *IntelligentRegressionConfig) *MetricStore {
	return &MetricStore{
		recentDataCache: make(map[string]*CachedMetricData),
		aggregateCache:  make(map[string]*CachedAggregateData),
	}
}

func NewRegressionHistory() *RegressionHistory {
	return &RegressionHistory{}
}

// Additional supporting methods would be implemented here...

// calculateImpact calculates the business impact of a regression
func (ire *IntelligentRegressionEngine) calculateImpact(metricName string, relativeChange float64) string {
	severity := ire.determineSeverity(metricName, relativeChange, 0)

	switch severity {
	case "Critical":
		return "Severe user impact, immediate action required"
	case "High":
		return "Significant performance degradation affecting users"
	case "Medium":
		return "Moderate impact, monitoring recommended"
	default:
		return "Minor impact, within acceptable variance"
	}
}

// determineTrendDirection determines if the metric trend is improving or degrading
func (ire *IntelligentRegressionEngine) determineTrendDirection(metricName string, relativeChange float64) string {
	// For latency and error rate, increase is degrading
	degradingMetrics := []string{
		"intent_processing_latency_p95", "intent_processing_latency_p99",
		"llm_processing_latency_p95", "error_rate", "cpu_utilization", "memory_utilization",
	}

	for _, metric := range degradingMetrics {
		if metricName == metric {
			if relativeChange > 5 {
				return "Degrading"
			} else if relativeChange < -5 {
				return "Improving"
			}
			return "Stable"
		}
	}

	// For throughput, availability, cache hit rate, increase is improving
	if relativeChange > 5 {
		return "Improving"
	} else if relativeChange < -5 {
		return "Degrading"
	}

	return "Stable"
}

// determineRegressionType categorizes the type of regression
func (ire *IntelligentRegressionEngine) determineRegressionType(metricName string) string {
	performanceMetrics := []string{
		"intent_processing_latency_p95", "intent_processing_latency_p99",
		"llm_processing_latency_p95", "intent_throughput_per_minute",
	}

	for _, metric := range performanceMetrics {
		if metricName == metric {
			return "Performance"
		}
	}

	if metricName == "availability" || metricName == "error_rate" {
		return "Availability"
	}

	if metricName == "cpu_utilization" || metricName == "memory_utilization" {
		return "Resource"
	}

	return "Other"
}

// assessImpactAndGenerateRecommendations evaluates impact and creates actionable recommendations
func (ire *IntelligentRegressionEngine) assessImpactAndGenerateRecommendations(analysis *RegressionAnalysisResult) {
	// Convert to pointer slice for functions that need pointers
	regressionPtrs := make([]*MetricRegression, len(analysis.MetricRegressions))
	for i := range analysis.MetricRegressions {
		regressionPtrs[i] = &analysis.MetricRegressions[i]
	}

	// Assess service impact
	analysis.ServiceImpact = ire.assessServiceImpact(regressionPtrs)

	// Generate automated remediation actions
	analysis.AutomatedRemediation = ire.generateAutomatedActions(regressionPtrs)

	// Generate manual remediation steps
	analysis.ManualRemediation = ire.generateManualActions(regressionPtrs)

	// Generate preventive measures
	analysis.PreventiveMeasures = ire.generatePreventiveMeasures(regressionPtrs)
}

// Placeholder methods for supporting functionality
func (ire *IntelligentRegressionEngine) assessServiceImpact(regressions []*MetricRegression) *ServiceImpactAssessment {
	criticalCount := 0
	highCount := 0

	for _, regression := range regressions {
		switch regression.Severity {
		case "Critical":
			criticalCount++
		case "High":
			highCount++
		}
	}

	var degradationPct float64
	var userImpact int
	var slaRisk float64
	var recoveryTime time.Duration
	var businessCriticality float64

	if criticalCount > 0 {
		degradationPct = 50.0
		userImpact = 1000
		slaRisk = 0.9
		recoveryTime = 30 * time.Minute
		businessCriticality = 0.95
	} else if highCount > 0 {
		degradationPct = 25.0
		userImpact = 500
		slaRisk = 0.6
		recoveryTime = 1 * time.Hour
		businessCriticality = 0.75
	} else {
		degradationPct = 10.0
		userImpact = 100
		slaRisk = 0.2
		recoveryTime = 2 * time.Hour
		businessCriticality = 0.5
	}

	return &ServiceImpactAssessment{
		AffectedServices:         []string{"nephoran-intent-operator", "llm-processor", "rag-service"},
		ServiceDegradationPct:    degradationPct,
		EstimatedUserImpact:      userImpact,
		SLAViolationRisk:         slaRisk,
		RecoveryTimeEstimate:     recoveryTime,
		BusinessCriticalityScore: businessCriticality,
	}
}

func (ire *IntelligentRegressionEngine) generateAutomatedActions(regressions []*MetricRegression) []AutomatedAction {
	actions := make([]AutomatedAction, 0)

	for _, regression := range regressions {
		switch regression.MetricName {
		case "intent_processing_latency_p95", "intent_processing_latency_p99":
			if regression.Severity == "Critical" || regression.Severity == "High" {
				actions = append(actions, AutomatedAction{
					ActionType:       "scaling",
					Description:      "Scale up LLM processor pods",
					Parameters:       map[string]interface{}{"replicas": 3, "resources": "increase"},
					SafetyChecks:     []string{"check_resource_availability", "verify_cluster_capacity"},
					ExecutionTimeout: 5 * time.Minute,
					ApprovalRequired: false,
				})
			}

		case "intent_throughput_per_minute":
			if regression.Severity == "Critical" {
				actions = append(actions, AutomatedAction{
					ActionType:       "circuit_breaker",
					Description:      "Enable circuit breaker for overload protection",
					Parameters:       map[string]interface{}{"failure_threshold": 5, "timeout": "30s"},
					SafetyChecks:     []string{"verify_downstream_health"},
					ExecutionTimeout: 1 * time.Minute,
					ApprovalRequired: false,
				})
			}
		}
	}

	return actions
}

func (ire *IntelligentRegressionEngine) generateManualActions(regressions []*MetricRegression) []ManualAction {
	// Placeholder implementation
	return []ManualAction{
		{
			Description:    "Review recent code deployments and configuration changes",
			Priority:       1,
			EstimatedTime:  30 * time.Minute,
			RequiredSkills: []string{"kubernetes", "performance-analysis"},
		},
		{
			Description:    "Analyze resource utilization and bottlenecks",
			Priority:       2,
			EstimatedTime:  45 * time.Minute,
			RequiredSkills: []string{"monitoring", "troubleshooting"},
		},
	}
}

func (ire *IntelligentRegressionEngine) generatePreventiveMeasures(regressions []*MetricRegression) []PreventiveMeasure {
	// Placeholder implementation
	return []PreventiveMeasure{
		{
			Description:     "Implement automated performance testing in CI/CD pipeline",
			Implementation:  "Add performance tests to GitHub Actions workflows",
			Timeline:        "2 weeks",
			ResponsibleTeam: "DevOps",
		},
		{
			Description:     "Set up proactive resource scaling based on predicted load",
			Implementation:  "Configure HPA with custom metrics and forecasting",
			Timeline:        "1 week",
			ResponsibleTeam: "Platform",
		},
	}
}

// Additional types for method signatures
type ManualAction struct {
	Description    string        `json:"description"`
	Priority       int           `json:"priority"`
	EstimatedTime  time.Duration `json:"estimatedTime"`
	RequiredSkills []string      `json:"requiredSkills"`
}

type PreventiveMeasure struct {
	Description     string `json:"description"`
	Implementation  string `json:"implementation"`
	Timeline        string `json:"timeline"`
	ResponsibleTeam string `json:"responsibleTeam"`
}

type BenchmarkExecution struct {
	ID        string    `json:"id"`
	StartedAt time.Time `json:"startedAt"`
	Status    string    `json:"status"`
}

type RegressionHistory struct {
	analyses []*RegressionAnalysisResult
	mutex    sync.RWMutex
}

func (rh *RegressionHistory) Add(analysis *RegressionAnalysisResult) {
	rh.mutex.Lock()
	defer rh.mutex.Unlock()
	rh.analyses = append(rh.analyses, analysis)
}

type AlertSuppression struct {
	ID        string    `json:"id"`
	Reason    string    `json:"reason"`
	ExpiresAt time.Time `json:"expiresAt"`
}

// convertToAlertManagerConfig converts IntelligentRegressionConfig to AlertManagerConfig
func convertToAlertManagerConfig(config *IntelligentRegressionConfig) *AlertManagerConfig {
	return &AlertManagerConfig{
		MaxConcurrentWorkers:        5,
		AlertProcessingTimeout:      30 * time.Second,
		AlertRetentionPeriod:        30 * 24 * time.Hour,
		CorrelationEnabled:          true,
		CorrelationWindow:           config.AlertCorrelationWindow,
		MaxCorrelatedAlerts:         config.MaxConcurrentAlerts,
		MinCorrelationScore:         0.7,
		RateLimitingEnabled:         true,
		MaxAlertsPerMinute:          10,
		MaxAlertsPerHour:            100,
		BurstAllowance:              5,
		AutoSuppressionEnabled:      true,
		DuplicateSuppressionTime:    15 * time.Minute,
		MaintenanceWindowsEnabled:   true,
		LearningEnabled:             config.LearningEnabled,
		FeedbackProcessingEnabled:   false, // Not available in config
		AdaptiveThresholdsEnabled:   false, // Not available in config,
		DefaultNotificationChannels: []string{"console"},
		EscalationEnabled:           true,
		AcknowledgmentRequired:      false,
		AcknowledgmentTimeout:       4 * time.Hour,
	}
}

// convertToNWDAFConfig converts IntelligentRegressionConfig to NWDAFConfig
func convertToNWDAFConfig(config *IntelligentRegressionConfig) *NWDAFConfig {
	return &NWDAFConfig{
		EnableLoadAnalytics:        true,
		EnablePerformanceAnalytics: true,
		EnableAnomalyAnalytics:     config.AnomalyDetectionEnabled,
		EnableCapacityAnalytics:    true,
		EnableQoEAnalytics:         true,
		EnableSliceAnalytics:       config.NetworkSliceAwareAnalysis,
		AnalyticsInterval:          config.DetectionInterval,
		PredictionHorizon:          24 * time.Hour,
		HistoricalDataRetention:    30 * 24 * time.Hour,
		MinDataPointsForAnalysis:   config.MinDataPoints,
		EventCorrelationEnabled:    true,
		EventCorrelationWindow:     config.AlertCorrelationWindow,
		ExternalDataSources:        []string{},
		NorthboundIntegration:      config.ExternalSystemsEnabled,
	}
}

// Additional placeholder types and interfaces for compilation
type TimeSeriesData struct{}
type PredictionCache struct{}
type AccuracyMetrics struct{}
type AnomalyDetectionConfig struct{}
type ChangePointConfig struct{}
type LearningConfig struct{}
type EscalationPolicy struct{}
type ModelRepository struct{}
type TrainingScheduler struct{}
type LearningPerformanceTracker struct{}
type FeedbackProcessor struct{}
type FeatureExtractor struct{}
type ModelOptimizer struct{}
type KnowledgeBase struct{}
type AlertOutcome struct{}
type HumanAnnotation struct{}
type AnalyticsRepository struct{}
type ServiceAnalyzer struct{}
type MobilityAnalyzer struct{}
type QoSAnalyzer struct{}
type SliceKPIs struct{}
type SLAViolation struct{}
type ResourceAllocation struct{}
type ResourceProfile struct{}
type QoSProfile struct{}
type GeographicScope struct{}
type ExternalDataSource interface{}
type LagAnalyzer struct{}
type CausalityAnalyzer struct{}
type TimeSeriesDatabase interface{}
type MetricAggregator struct{}
type DataCompressor struct{}
type MetricIndexer struct{}
type RetentionPolicy struct{}
type CachedMetricData struct{}
type CachedAggregateData struct{}
type UserExperienceImpact struct{}
type BusinessImpactAssessment struct{}
type RollbackPlan struct{}
type LearningFeedback struct{}
type IsolationTree struct{}
type TrendComponent struct{}
type CyclicalPattern struct{}
type EnvironmentInfo struct{}
type SystemLoadInfo struct{}
type BaselineSnapshot struct{}

// Method implementations for various components would go here...

func (ad *AnomalyDetector) DetectAnomaliesInMetrics(metrics map[string]float64) ([]*AnomalyEvent, error) {
	// Placeholder implementation
	return []*AnomalyEvent{}, nil
}

func (cpd *ChangePointDetector) DetectChangePointsInMetrics(metrics map[string]float64) ([]*ChangePoint, error) {
	// Placeholder implementation
	return []*ChangePoint{}, nil
}

func (fe *ForecastingEngine) GenerateForecast(ctx context.Context, metrics map[string]float64) (*Forecast, error) {
	// Placeholder implementation
	return &Forecast{}, nil
}

func (ce *CorrelationEngine) AnalyzeCorrelations(metrics map[string]float64) (*CorrelationAnalysis, error) {
	// Placeholder implementation
	return &CorrelationAnalysis{}, nil
}

func (cls *ContinuousLearningSystem) ProcessAnalysis(analysis *RegressionAnalysisResult) {
	// Placeholder implementation
}

func (bm *BaselineManager) GetBaseline(metricName string) *DynamicBaseline {
	bm.mutex.RLock()
	defer bm.mutex.RUnlock()
	return bm.currentBaselines[metricName]
}

func (ire *IntelligentRegressionEngine) runBaselineManagement(ctx context.Context) {
	// Placeholder implementation for baseline management
}

func (ire *IntelligentRegressionEngine) runLearningSystem(ctx context.Context) {
	// Placeholder implementation for learning system
}

func (ire *IntelligentRegressionEngine) runNWDAFAnalytics(ctx context.Context) {
	// Placeholder implementation for NWDAF analytics
}

// normalCDF is already defined in cusum_detector.go
