//go:build go1.24

package regression

import (
	"context"
	"math"
	"math/rand"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"gonum.org/v1/gonum/stat"
)

// TestRegressionDetection provides comprehensive tests for the regression detection system
func TestRegressionDetection(t *testing.T) {
	t.Run("BasicRegressionDetection", testBasicRegressionDetection)
	t.Run("CUSUMChangePointDetection", testCUSUMChangePointDetection)
	t.Run("AnomalyDetection", testAnomalyDetection)
	t.Run("NWDAFAnalytics", testNWDAFAnalytics)
	t.Run("IntelligentAlerting", testIntelligentAlerting)
	t.Run("APIIntegration", testAPIIntegration)
	t.Run("PerformanceValidation", testPerformanceValidation)
	t.Run("EndToEndWorkflow", testEndToEndWorkflow)
}

// testBasicRegressionDetection tests the core regression detection functionality
func testBasicRegressionDetection(t *testing.T) {
	// Create test configuration
	config := &RegressionConfig{
		ConfidenceLevel:         0.95,
		SignificanceLevel:       0.05,
		MinSampleSize:           30,
		LatencyRegressionPct:    15.0,
		ThroughputRegressionPct: 10.0,
		TargetLatencyP95Ms:      2000.0,
		TargetThroughputRpm:     45.0,
	}

	detector := NewRegressionDetector(config)
	require.NotNil(t, detector)

	// Create test baseline
	baseline := createTestBaseline()
	err := detector.UpdateBaseline(baseline)
	require.NoError(t, err)

	// Test Case 1: No regression (normal performance)
	t.Run("NoRegression", func(t *testing.T) {
		measurement := &PerformanceMeasurement{
			Timestamp:    time.Now(),
			LatencyP95:   1800.0, // Within expected range
			Throughput:   47.0,   // Good throughput
			ErrorRate:    0.2,    // Low error rate
			Availability: 99.97,  // High availability
			SampleCount:  100,
		}

		analysis, err := detector.AnalyzeRegression(context.Background(), measurement)
		require.NoError(t, err)
		require.NotNil(t, analysis)

		assert.False(t, analysis.HasRegression, "Should not detect regression for normal performance")
		assert.Equal(t, "Low", analysis.RegressionSeverity)
	})

	// Test Case 2: Latency regression
	t.Run("LatencyRegression", func(t *testing.T) {
		measurement := &PerformanceMeasurement{
			Timestamp:    time.Now(),
			LatencyP95:   2800.0, // 40% increase - significant regression
			Throughput:   45.0,   // Normal throughput
			ErrorRate:    0.3,    // Normal error rate
			Availability: 99.95,  // Normal availability
			SampleCount:  100,
		}

		analysis, err := detector.AnalyzeRegression(context.Background(), measurement)
		require.NoError(t, err)
		require.NotNil(t, analysis)

		assert.True(t, analysis.HasRegression, "Should detect latency regression")
		assert.Equal(t, "Critical", analysis.RegressionSeverity)
		assert.Greater(t, analysis.ConfidenceScore, 0.8)

		// Verify latency regression details
		var latencyRegression *MetricRegression
		for _, reg := range analysis.MetricRegressions {
			if reg.MetricName == "LatencyP95" {
				latencyRegression = &reg
				break
			}
		}

		require.NotNil(t, latencyRegression)
		assert.True(t, latencyRegression.IsSignificant)
		assert.Equal(t, "Critical", latencyRegression.Severity)
		assert.Equal(t, "Performance", latencyRegression.RegressionType)
		assert.Greater(t, latencyRegression.RelativeChangePct, 15.0)
	})

	// Test Case 3: Throughput regression
	t.Run("ThroughputRegression", func(t *testing.T) {
		measurement := &PerformanceMeasurement{
			Timestamp:    time.Now(),
			LatencyP95:   2000.0, // Normal latency
			Throughput:   35.0,   // 22% decrease - significant regression
			ErrorRate:    0.2,    // Normal error rate
			Availability: 99.95,  // Normal availability
			SampleCount:  100,
		}

		analysis, err := detector.AnalyzeRegression(context.Background(), measurement)
		require.NoError(t, err)
		require.NotNil(t, analysis)

		assert.True(t, analysis.HasRegression, "Should detect throughput regression")
		assert.Contains(t, []string{"High", "Critical"}, analysis.RegressionSeverity)

		// Find throughput regression
		var throughputRegression *MetricRegression
		for _, reg := range analysis.MetricRegressions {
			if reg.MetricName == "Throughput" {
				throughputRegression = &reg
				break
			}
		}

		require.NotNil(t, throughputRegression)
		assert.True(t, throughputRegression.IsSignificant)
		assert.Equal(t, "Performance", throughputRegression.RegressionType)
		assert.Equal(t, "Degrading", throughputRegression.TrendDirection)
	})

	// Test Case 4: Multiple regressions
	t.Run("MultipleRegressions", func(t *testing.T) {
		measurement := &PerformanceMeasurement{
			Timestamp:    time.Now(),
			LatencyP95:   2500.0, // 25% increase
			Throughput:   38.0,   // 16% decrease
			ErrorRate:    1.5,    // 400% increase
			Availability: 99.85,  // Slight decrease
			SampleCount:  100,
		}

		analysis, err := detector.AnalyzeRegression(context.Background(), measurement)
		require.NoError(t, err)
		require.NotNil(t, analysis)

		assert.True(t, analysis.HasRegression, "Should detect multiple regressions")
		assert.Equal(t, "Critical", analysis.RegressionSeverity)
		assert.Greater(t, analysis.ConfidenceScore, 0.9)
		assert.GreaterOrEqual(t, len(analysis.MetricRegressions), 3)

		// Verify we have regressions for each degraded metric
		regressionsByMetric := make(map[string]*MetricRegression)
		for _, reg := range analysis.MetricRegressions {
			regressionsByMetric[reg.MetricName] = &reg
		}

		assert.Contains(t, regressionsByMetric, "LatencyP95")
		assert.Contains(t, regressionsByMetric, "Throughput")
		assert.Contains(t, regressionsByMetric, "ErrorRate")
	})
}

// testCUSUMChangePointDetection tests the CUSUM change point detection algorithm
func testCUSUMChangePointDetection(t *testing.T) {
	// Create CUSUM detector with test configuration
	config := &CUSUMConfig{
		ThresholdMultiplier:    4.0,
		DriftSensitivity:       0.5,
		ValidationWindowSize:   20,
		FalsePositiveReduction: true,
	}

	detector := NewCUSUMDetector(0, 0, 0, config)
	require.NotNil(t, detector)

	// Test Case 1: No change points (stable data)
	t.Run("StableData", func(t *testing.T) {
		data, timestamps := generateStableTimeSeries(1000, 1000.0, 50.0, time.Now())

		changePoints, err := detector.DetectChangePoints(data, timestamps)
		require.NoError(t, err)

		// Should detect very few or no change points in stable data
		assert.LessOrEqual(t, len(changePoints), 2, "Should detect minimal change points in stable data")
	})

	// Test Case 2: Step change
	t.Run("StepChange", func(t *testing.T) {
		data, timestamps := generateStepChangeTimeSeries(500, 500, 1000.0, 1500.0, 50.0, time.Now())

		changePoints, err := detector.DetectChangePoints(data, timestamps)
		require.NoError(t, err)

		assert.GreaterOrEqual(t, len(changePoints), 1, "Should detect step change")

		if len(changePoints) > 0 {
			cp := changePoints[0]
			assert.Equal(t, "step", cp.ChangeType)
			assert.Greater(t, cp.Confidence, 0.7)

			// Change point should be detected around the actual change point (index 500)
			changeIndex := findTimestampIndex(timestamps, cp.Timestamp)
			assert.InDelta(t, 500, changeIndex, 50, "Change point should be detected near actual change")
		}
	})

	// Test Case 3: Gradual trend change
	t.Run("TrendChange", func(t *testing.T) {
		data, timestamps := generateTrendChangeTimeSeries(1000, 1000.0, 2.0, 50.0, time.Now())

		changePoints, err := detector.DetectChangePoints(data, timestamps)
		require.NoError(t, err)

		// Should detect trend change
		assert.GreaterOrEqual(t, len(changePoints), 1, "Should detect trend change")
	})

	// Test Case 4: Multiple change points
	t.Run("MultipleChanges", func(t *testing.T) {
		data := make([]float64, 0, 1500)
		timestamps := make([]time.Time, 0, 1500)
		baseTime := time.Now()

		// Segment 1: Stable at 1000
		for i := 0; i < 500; i++ {
			data = append(data, 1000.0+rand.NormFloat64()*30.0)
			timestamps = append(timestamps, baseTime.Add(time.Duration(i)*time.Second))
		}

		// Segment 2: Step up to 1200
		for i := 500; i < 1000; i++ {
			data = append(data, 1200.0+rand.NormFloat64()*30.0)
			timestamps = append(timestamps, baseTime.Add(time.Duration(i)*time.Second))
		}

		// Segment 3: Step down to 800
		for i := 1000; i < 1500; i++ {
			data = append(data, 800.0+rand.NormFloat64()*30.0)
			timestamps = append(timestamps, baseTime.Add(time.Duration(i)*time.Second))
		}

		changePoints, err := detector.DetectChangePoints(data, timestamps)
		require.NoError(t, err)

		assert.GreaterOrEqual(t, len(changePoints), 2, "Should detect multiple change points")

		// Verify change points are in reasonable locations
		for _, cp := range changePoints {
			changeIndex := findTimestampIndex(timestamps, cp.Timestamp)
			// Should be near one of our change points (500 or 1000)
			nearFirstChange := math.Abs(float64(changeIndex-500)) < 100
			nearSecondChange := math.Abs(float64(changeIndex-1000)) < 100
			assert.True(t, nearFirstChange || nearSecondChange,
				"Change point at index %d should be near actual change points", changeIndex)
		}
	})

	// Test Case 5: CUSUM parameter validation
	t.Run("ParameterValidation", func(t *testing.T) {
		// Test automatic parameter initialization
		data := generateNormalData(100, 1000.0, 50.0)
		timestamps := generateTimestamps(100, time.Now())

		detector := NewCUSUMDetector(0, 0, 0, config) // Zero parameters should auto-initialize

		_, err := detector.DetectChangePoints(data, timestamps)
		require.NoError(t, err)

		params := detector.GetParameters()
		assert.Greater(t, params["threshold"].(float64), 0.0, "Threshold should be auto-initialized")
		assert.Greater(t, params["drift_factor"].(float64), 0.0, "Drift factor should be auto-initialized")
		assert.NotEqual(t, params["reference_value"].(float64), 0.0, "Reference value should be auto-initialized")
	})
}

// testAnomalyDetection tests the anomaly detection components
func testAnomalyDetection(t *testing.T) {
	config := &IntelligentRegressionConfig{
		AnomalyDetectionEnabled: true,
		MinDataPoints:           30,
		ConfidenceThreshold:     0.80,
	}

	anomalyDetector := NewAnomalyDetector(config)
	require.NotNil(t, anomalyDetector)

	// Test Case 1: Outlier detection
	t.Run("OutlierDetection", func(t *testing.T) {
		// Generate normal data with outliers
		baseData := generateNormalData(100, 1000.0, 50.0)

		// Insert outliers
		baseData[25] = 2000.0 // High outlier
		baseData[50] = 200.0  // Low outlier
		baseData[75] = 1800.0 // Another high outlier

		metrics := map[string]float64{
			"test_metric": stat.Mean(baseData, nil),
		}

		anomalies, err := anomalyDetector.DetectAnomaliesInMetrics(metrics)
		require.NoError(t, err)

		// Should detect the anomalous pattern in the data
		assert.GreaterOrEqual(t, len(anomalies), 0, "Should process anomaly detection without error")
	})
}

// testNWDAFAnalytics tests the NWDAF analytics functionality
func testNWDAFAnalytics(t *testing.T) {
	config := &NWDAFConfig{
		EnableLoadAnalytics:        true,
		EnablePerformanceAnalytics: true,
		EnableAnomalyAnalytics:     true,
		MinDataPointsForAnalysis:   50,
	}

	analyzer := NewNWDAFAnalyzer(config)
	require.NotNil(t, analyzer)

	// Test Case 1: Load analytics insights
	t.Run("LoadAnalytics", func(t *testing.T) {
		metrics := map[string]float64{
			"cpu_utilization":              75.0,
			"memory_utilization":           60.0,
			"concurrent_processing":        150.0,
			"intent_throughput_per_minute": 42.0,
		}

		regressions := []*MetricRegression{
			{
				MetricName:        "cpu_utilization",
				BaselineValue:     50.0,
				CurrentValue:      75.0,
				RelativeChangePct: 50.0,
				Severity:          "High",
				IsSignificant:     true,
			},
		}

		insights, err := analyzer.GenerateInsights(context.Background(), metrics, regressions)
		require.NoError(t, err)
		require.NotNil(t, insights)

		assert.NotEmpty(t, insights.AnalysisID)
		assert.Greater(t, insights.OverallConfidence, 0.0)
		assert.Greater(t, insights.DataQualityScore, 0.0)

		// Should generate recommendations
		assert.GreaterOrEqual(t, len(insights.Recommendations), 0)
	})

	// Test Case 2: Performance analytics insights
	t.Run("PerformanceAnalytics", func(t *testing.T) {
		metrics := map[string]float64{
			"intent_processing_latency_p95": 2500.0,
			"intent_throughput_per_minute":  35.0,
			"error_rate":                    1.2,
			"availability":                  99.85,
		}

		regressions := []*MetricRegression{
			{
				MetricName:        "intent_processing_latency_p95",
				BaselineValue:     1800.0,
				CurrentValue:      2500.0,
				RelativeChangePct: 38.9,
				Severity:          "Critical",
				IsSignificant:     true,
			},
		}

		insights, err := analyzer.GenerateInsights(context.Background(), metrics, regressions)
		require.NoError(t, err)
		require.NotNil(t, insights)

		// Should have performance insights
		if insights.PerformanceInsights != nil {
			assert.GreaterOrEqual(t, len(insights.PerformanceInsights.KPIViolations), 0)
			assert.GreaterOrEqual(t, insights.PerformanceInsights.SLAComplianceScore, 0.0)
		}

		// Should generate actionable recommendations
		assert.GreaterOrEqual(t, len(insights.Recommendations), 0)
		for _, rec := range insights.Recommendations {
			assert.NotEmpty(t, rec.Title)
			assert.NotEmpty(t, rec.Description)
			assert.Contains(t, []string{"critical", "high", "medium", "low"}, rec.Priority)
		}
	})
}

// testIntelligentAlerting tests the intelligent alerting system
func testIntelligentAlerting(t *testing.T) {
	config := &AlertManagerConfig{
		MaxConcurrentWorkers:   2,
		AlertProcessingTimeout: 10 * time.Second,
		CorrelationEnabled:     true,
		CorrelationWindow:      5 * time.Minute,
		RateLimitingEnabled:    false, // Disable for testing
	}

	alertManager := NewIntelligentAlertManager(config)
	require.NotNil(t, alertManager)

	// Test Case 1: Alert generation and processing
	t.Run("AlertGeneration", func(t *testing.T) {
		analysis := &RegressionAnalysisResult{
			RegressionAnalysis: &RegressionAnalysis{
				AnalysisID:         "test-analysis-001",
				Timestamp:          time.Now(),
				HasRegression:      true,
				RegressionSeverity: "High",
				ConfidenceScore:    0.92,
				MetricRegressions: []MetricRegression{
					{
						MetricName:        "intent_processing_latency_p95",
						BaselineValue:     1800.0,
						CurrentValue:      2400.0,
						RelativeChangePct: 33.3,
						Severity:          "High",
						IsSignificant:     true,
						RegressionType:    "Performance",
					},
				},
			},
		}

		err := alertManager.ProcessRegressionAnalysis(analysis)
		require.NoError(t, err)

		// Give some time for async processing
		time.Sleep(100 * time.Millisecond)

		// Verify alert was processed (would check internal state in real implementation)
		assert.True(t, true, "Alert processing completed successfully")
	})

	// Test Case 2: Rate limiting
	t.Run("RateLimiting", func(t *testing.T) {
		rateLimitConfig := &AlertManagerConfig{
			MaxConcurrentWorkers:   1,
			AlertProcessingTimeout: 5 * time.Second,
			RateLimitingEnabled:    true,
			MaxAlertsPerMinute:     5,
		}

		rateLimitedManager := NewIntelligentAlertManager(rateLimitConfig)
		require.NotNil(t, rateLimitedManager)

		// Send multiple alerts rapidly
		analysis := &RegressionAnalysisResult{
			RegressionAnalysis: &RegressionAnalysis{
				AnalysisID:         "rate-limit-test",
				HasRegression:      true,
				RegressionSeverity: "Medium",
				ConfidenceScore:    0.85,
			},
		}

		// This should not fail due to rate limiting logic
		err := rateLimitedManager.ProcessRegressionAnalysis(analysis)
		assert.NoError(t, err)
	})
}

// testAPIIntegration tests the API integration components
func testAPIIntegration(t *testing.T) {
	config := &APIIntegrationConfig{
		EnableAPIServer:   false, // Don't start actual server in tests
		PrometheusEnabled: false, // Don't require external Prometheus
		GrafanaEnabled:    false, // Don't require external Grafana
		WebhooksEnabled:   true,
	}

	// Create mock regression engine and alert manager
	regressionEngine, err := NewIntelligentRegressionEngine(&IntelligentRegressionConfig{})
	require.NoError(t, err)

	alertManager := NewIntelligentAlertManager(&AlertManagerConfig{})

	apiManager, err := NewAPIIntegrationManager(config, regressionEngine, alertManager)
	require.NoError(t, err)
	require.NotNil(t, apiManager)

	// Test Case 1: Webhook functionality
	t.Run("WebhookDelivery", func(t *testing.T) {
		if apiManager.webhookManager == nil {
			t.Skip("Webhook manager not initialized")
		}

		testPayload := map[string]interface{}{
			"event_type": "regression_detected",
			"severity":   "high",
			"timestamp":  time.Now(),
		}

		// This should not fail even without actual endpoints
		err := apiManager.webhookManager.SendWebhook("test", testPayload)
		assert.NoError(t, err)
	})
}

// testPerformanceValidation tests performance characteristics
func testPerformanceValidation(t *testing.T) {
	// Test Case 1: Processing latency
	t.Run("ProcessingLatency", func(t *testing.T) {
		config := &RegressionConfig{
			MinSampleSize: 10, // Reduce for faster testing
		}

		detector := NewRegressionDetector(config)
		baseline := createTestBaseline()
		detector.UpdateBaseline(baseline)

		measurement := &PerformanceMeasurement{
			Timestamp:    time.Now(),
			LatencyP95:   2200.0,
			Throughput:   40.0,
			ErrorRate:    0.5,
			Availability: 99.9,
			SampleCount:  50,
		}

		// Measure processing time
		start := time.Now()
		analysis, err := detector.AnalyzeRegression(context.Background(), measurement)
		duration := time.Since(start)

		require.NoError(t, err)
		require.NotNil(t, analysis)

		// Should complete within reasonable time
		assert.Less(t, duration, 1*time.Second, "Regression analysis should complete quickly")
	})

	// Test Case 2: Memory usage validation
	t.Run("MemoryUsage", func(t *testing.T) {
		// Create detector and process multiple analyses to test memory behavior
		detector := NewRegressionDetector(&RegressionConfig{MinSampleSize: 10})
		baseline := createTestBaseline()
		detector.UpdateBaseline(baseline)

		// Process 100 analyses
		for i := 0; i < 100; i++ {
			measurement := &PerformanceMeasurement{
				Timestamp:    time.Now().Add(time.Duration(i) * time.Second),
				LatencyP95:   1900.0 + rand.Float64()*400.0, // 1900-2300ms
				Throughput:   40.0 + rand.Float64()*20.0,    // 40-60 rpm
				ErrorRate:    rand.Float64() * 2.0,          // 0-2%
				Availability: 99.5 + rand.Float64()*0.5,     // 99.5-100%
				SampleCount:  50,
			}

			_, err := detector.AnalyzeRegression(context.Background(), measurement)
			require.NoError(t, err)
		}

		// Test passes if no memory leaks cause the test to fail
		assert.True(t, true, "Memory usage test completed")
	})
}

// testEndToEndWorkflow tests the complete end-to-end workflow
func testEndToEndWorkflow(t *testing.T) {
	t.Run("CompleteWorkflow", func(t *testing.T) {
		// 1. Create intelligent regression engine
		config := &IntelligentRegressionConfig{
			DetectionInterval:           1 * time.Minute,
			MinDataPoints:               30,
			ConfidenceThreshold:         0.80,
			AnomalyDetectionEnabled:     true,
			ChangePointDetectionEnabled: true,
			ForecastingEnabled:          false, // Disable for faster testing
			NWDAFPatternsEnabled:        true,
			LearningEnabled:             false,                   // Disable for testing
			PrometheusEndpoint:          "http://localhost:9090", // Mock endpoint
		}

		engine, err := NewIntelligentRegressionEngine(config)
		require.NoError(t, err)
		require.NotNil(t, engine)

		// 2. Create alert manager
		alertConfig := &AlertManagerConfig{
			MaxConcurrentWorkers:   2,
			AlertProcessingTimeout: 10 * time.Second,
			CorrelationEnabled:     true,
			LearningEnabled:        false,
		}

		alertManager := NewIntelligentAlertManager(alertConfig)
		require.NotNil(t, alertManager)

		// 3. Create API integration manager
		apiConfig := &APIIntegrationConfig{
			EnableAPIServer:   false, // Don't start server in tests
			PrometheusEnabled: false, // Mock Prometheus
			WebhooksEnabled:   false, // Disable webhooks for testing
		}

		apiManager, err := NewAPIIntegrationManager(apiConfig, engine, alertManager)
		require.NoError(t, err)
		require.NotNil(t, apiManager)

		// 4. Simulate the complete workflow
		_, cancel := context.WithTimeout(context.Background(), 30*time.Second)
		defer cancel()

		// Start the engine (this would normally run continuously)
		// For testing, we'll just verify it starts without error
		assert.NotNil(t, engine, "Engine should be initialized")
		assert.NotNil(t, alertManager, "Alert manager should be initialized")
		assert.NotNil(t, apiManager, "API manager should be initialized")

		// The test passes if all components are created successfully
		// In a real integration test, we would:
		// - Start a mock Prometheus server
		// - Send test metrics
		// - Verify regression detection
		// - Verify alert generation
		// - Verify API responses
	})
}

// Helper functions for test data generation

func createTestBaseline() *PerformanceBaseline {
	return &PerformanceBaseline{
		ID:           "test-baseline-001",
		CreatedAt:    time.Now().Add(-24 * time.Hour),
		ValidFrom:    time.Now().Add(-24 * time.Hour),
		ValidUntil:   time.Now().Add(24 * time.Hour),
		Version:      "1.0",
		SampleCount:  1000,
		QualityScore: 0.95,

		LatencyMetrics: &BaselineMetrics{
			MetricName:   "LatencyP95",
			Unit:         "ms",
			Mean:         1800.0,
			Median:       1750.0,
			StandardDev:  150.0,
			P50:          1750.0,
			P95:          2000.0,
			P99:          2200.0,
			Min:          1200.0,
			Max:          2500.0,
			SampleSize:   1000,
			Distribution: generateNormalData(1000, 1800.0, 150.0),
		},

		ThroughputMetrics: &BaselineMetrics{
			MetricName:   "Throughput",
			Unit:         "rpm",
			Mean:         45.0,
			Median:       45.0,
			StandardDev:  3.0,
			P50:          45.0,
			P95:          50.0,
			P99:          52.0,
			Min:          35.0,
			Max:          55.0,
			SampleSize:   1000,
			Distribution: generateNormalData(1000, 45.0, 3.0),
		},

		ErrorRateMetrics: &BaselineMetrics{
			MetricName:   "ErrorRate",
			Unit:         "percent",
			Mean:         0.3,
			Median:       0.2,
			StandardDev:  0.2,
			P50:          0.2,
			P95:          0.8,
			P99:          1.2,
			Min:          0.0,
			Max:          2.0,
			SampleSize:   1000,
			Distribution: generateExponentialData(1000, 0.3),
		},

		AvailabilityMetrics: &BaselineMetrics{
			MetricName:   "Availability",
			Unit:         "percent",
			Mean:         99.95,
			Median:       99.97,
			StandardDev:  0.05,
			P50:          99.97,
			P95:          99.99,
			P99:          99.99,
			Min:          99.80,
			Max:          100.0,
			SampleSize:   1000,
			Distribution: generateAvailabilityData(1000, 99.95, 0.05),
		},
	}
}

func generateNormalData(size int, mean, stddev float64) []float64 {
	data := make([]float64, size)
	for i := 0; i < size; i++ {
		data[i] = rand.NormFloat64()*stddev + mean
	}
	return data
}

func generateExponentialData(size int, lambda float64) []float64 {
	data := make([]float64, size)
	for i := 0; i < size; i++ {
		data[i] = rand.ExpFloat64() / lambda
	}
	return data
}

func generateAvailabilityData(size int, mean, stddev float64) []float64 {
	data := make([]float64, size)
	for i := 0; i < size; i++ {
		value := rand.NormFloat64()*stddev + mean
		// Clamp to realistic availability range
		if value < 99.0 {
			value = 99.0
		}
		if value > 100.0 {
			value = 100.0
		}
		data[i] = value
	}
	return data
}

func generateStableTimeSeries(size int, mean, stddev float64, startTime time.Time) ([]float64, []time.Time) {
	data := generateNormalData(size, mean, stddev)
	timestamps := generateTimestamps(size, startTime)
	return data, timestamps
}

func generateStepChangeTimeSeries(size1, size2 int, mean1, mean2, stddev float64, startTime time.Time) ([]float64, []time.Time) {
	data := make([]float64, 0, size1+size2)

	// First segment
	segment1 := generateNormalData(size1, mean1, stddev)
	data = append(data, segment1...)

	// Second segment (with step change)
	segment2 := generateNormalData(size2, mean2, stddev)
	data = append(data, segment2...)

	timestamps := generateTimestamps(size1+size2, startTime)
	return data, timestamps
}

func generateTrendChangeTimeSeries(size int, startValue, slope, noise float64, startTime time.Time) ([]float64, []time.Time) {
	data := make([]float64, size)
	for i := 0; i < size; i++ {
		trend := startValue + slope*float64(i)
		data[i] = trend + rand.NormFloat64()*noise
	}

	timestamps := generateTimestamps(size, startTime)
	return data, timestamps
}

func generateTimestamps(size int, startTime time.Time) []time.Time {
	timestamps := make([]time.Time, size)
	for i := 0; i < size; i++ {
		timestamps[i] = startTime.Add(time.Duration(i) * time.Second)
	}
	return timestamps
}

func findTimestampIndex(timestamps []time.Time, target time.Time) int {
	for i, ts := range timestamps {
		if ts.Equal(target) || ts.After(target) {
			return i
		}
	}
	return len(timestamps) - 1
}

// Benchmark tests

func BenchmarkRegressionDetection(b *testing.B) {
	detector := NewRegressionDetector(&RegressionConfig{MinSampleSize: 10})
	baseline := createTestBaseline()
	detector.UpdateBaseline(baseline)

	measurement := &PerformanceMeasurement{
		Timestamp:    time.Now(),
		LatencyP95:   2200.0,
		Throughput:   40.0,
		ErrorRate:    0.5,
		Availability: 99.9,
		SampleCount:  50,
	}

	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		_, err := detector.AnalyzeRegression(context.Background(), measurement)
		if err != nil {
			b.Fatal(err)
		}
	}
}

func BenchmarkCUSUMDetection(b *testing.B) {
	detector := NewCUSUMDetector(200.0, 50.0, 1000.0, getDefaultCUSUMConfig())
	data := generateNormalData(1000, 1000.0, 50.0)
	timestamps := generateTimestamps(1000, time.Now())

	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		_, err := detector.DetectChangePoints(data, timestamps)
		if err != nil {
			b.Fatal(err)
		}
	}
}

func BenchmarkNWDAFAnalytics(b *testing.B) {
	analyzer := NewNWDAFAnalyzer(&NWDAFConfig{
		EnableLoadAnalytics:        true,
		EnablePerformanceAnalytics: true,
		MinDataPointsForAnalysis:   10,
	})

	metrics := map[string]float64{
		"intent_processing_latency_p95": 2200.0,
		"intent_throughput_per_minute":  40.0,
		"cpu_utilization":               75.0,
		"memory_utilization":            60.0,
	}

	regressions := []*MetricRegression{
		{
			MetricName:    "intent_processing_latency_p95",
			CurrentValue:  2200.0,
			BaselineValue: 1800.0,
			Severity:      "High",
			IsSignificant: true,
		},
	}

	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		_, err := analyzer.GenerateInsights(context.Background(), metrics, regressions)
		if err != nil {
			b.Fatal(err)
		}
	}
}
