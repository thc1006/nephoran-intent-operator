/*

Copyright 2025.



Licensed under the Apache License, Version 2.0 (the "License");

you may not use this file except in compliance with the License.

You may obtain a copy of the License at



    http://www.apache.org/licenses/LICENSE-2.0



Unless required by applicable law or agreed to in writing, software

distributed under the License is distributed on an "AS IS" BASIS,

WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

See the License for the specific language governing permissions and

limitations under the License.

*/




package optimization



import (

	"context"

	"fmt"

	"sync"

	"time"



	"github.com/go-logr/logr"



	"github.com/thc1006/nephoran-intent-operator/pkg/controllers/interfaces"

)



// ResourceMonitor monitors system resource usage and performance metrics.

type ResourceMonitor struct {

	logger logr.Logger



	// Metrics collection.

	metricsHistory []SystemMetrics

	currentMetrics *SystemMetrics

	historyMutex   sync.RWMutex



	// Collection settings.

	maxHistorySize int



	// Control.

	started  bool

	stopChan chan bool

	mutex    sync.RWMutex

}



// MetricsAnalyzer analyzes performance metrics and identifies trends.

type MetricsAnalyzer struct {

	logger logr.Logger

}



// ScalingDecisionEngine makes intelligent scaling decisions.

type ScalingDecisionEngine struct {

	logger logr.Logger

	config *PerformanceConfig



	// Decision history.

	decisions     []ScalingDecision

	decisionMutex sync.RWMutex

}



// ConnectionPoolManager manages connection pools across services.

type ConnectionPoolManager struct {

	logger logr.Logger

	config *PerformanceConfig



	// Connection pools.

	httpPools map[string]*HTTPConnectionPool

	dbPools   map[string]*DatabaseConnectionPool



	// Control.

	started bool

	mutex   sync.RWMutex

}



// LoadBalancer handles load balancing across controller instances.

type LoadBalancer struct {

	logger logr.Logger



	// Load balancing state.

	instances map[string]*ControllerInstance

	strategy  LoadBalancingStrategy



	mutex sync.RWMutex

}



// NewResourceMonitor creates a new resource monitor.

func NewResourceMonitor(logger logr.Logger) *ResourceMonitor {

	return &ResourceMonitor{

		logger:         logger.WithName("resource-monitor"),

		metricsHistory: make([]SystemMetrics, 0),

		maxHistorySize: 1000, // Keep last 1000 metrics snapshots

		stopChan:       make(chan bool),

	}

}



// Start starts the resource monitor.

func (rm *ResourceMonitor) Start(ctx context.Context) error {

	rm.mutex.Lock()

	defer rm.mutex.Unlock()



	if rm.started {

		return fmt.Errorf("resource monitor already started")

	}



	// Initialize current metrics.

	rm.currentMetrics = &SystemMetrics{

		Timestamp:         time.Now(),

		QueueDepths:       make(map[interfaces.ProcessingPhase]int),

		ResponseTimes:     make(map[interfaces.ProcessingPhase]time.Duration),

		ThroughputRates:   make(map[interfaces.ProcessingPhase]float64),

		ErrorRates:        make(map[interfaces.ProcessingPhase]float64),

		CacheMetrics:      &CacheMetrics{},

		ConnectionMetrics: &ConnectionMetrics{},

	}



	rm.started = true

	rm.logger.Info("Resource monitor started")



	return nil

}



// Stop stops the resource monitor.

func (rm *ResourceMonitor) Stop(ctx context.Context) error {

	rm.mutex.Lock()

	defer rm.mutex.Unlock()



	if !rm.started {

		return nil

	}



	close(rm.stopChan)

	rm.started = false

	rm.logger.Info("Resource monitor stopped")



	return nil

}



// CollectMetrics collects current system metrics.

func (rm *ResourceMonitor) CollectMetrics(ctx context.Context) {

	rm.mutex.RLock()

	if !rm.started {

		rm.mutex.RUnlock()

		return

	}

	rm.mutex.RUnlock()



	metrics := &SystemMetrics{

		Timestamp: time.Now(),



		// System resource metrics.

		CPUUtilization:    rm.getCPUUtilization(),

		MemoryUtilization: rm.getMemoryUtilization(),

		NetworkIO:         rm.getNetworkIOMetrics(),

		DiskIO:            rm.getDiskIOMetrics(),



		// Application metrics.

		QueueDepths:     rm.getQueueDepthMetrics(),

		ResponseTimes:   rm.getResponseTimeMetrics(),

		ThroughputRates: rm.getThroughputMetrics(),

		ErrorRates:      rm.getErrorRateMetrics(),



		// Cache and connection metrics.

		CacheMetrics:      rm.getCacheMetrics(),

		ConnectionMetrics: rm.getConnectionMetrics(),

	}



	rm.updateMetrics(metrics)

}



// GetCurrentMetrics returns the current system metrics.

func (rm *ResourceMonitor) GetCurrentMetrics() *SystemMetrics {

	rm.historyMutex.RLock()

	defer rm.historyMutex.RUnlock()



	if rm.currentMetrics != nil {

		// Return a copy to avoid race conditions.

		metricsCopy := *rm.currentMetrics

		return &metricsCopy

	}



	return nil

}



// GetMetricsHistory returns historical metrics.

func (rm *ResourceMonitor) GetMetricsHistory(duration time.Duration) []SystemMetrics {

	rm.historyMutex.RLock()

	defer rm.historyMutex.RUnlock()



	cutoff := time.Now().Add(-duration)

	var result []SystemMetrics



	for _, metrics := range rm.metricsHistory {

		if metrics.Timestamp.After(cutoff) {

			result = append(result, metrics)

		}

	}



	return result

}



// Helper methods for metrics collection.



func (rm *ResourceMonitor) getCPUUtilization() float64 {

	// This would implement actual CPU utilization collection.

	// For now, return a placeholder value.

	return 45.0 // 45% CPU utilization

}



func (rm *ResourceMonitor) getMemoryUtilization() float64 {

	// This would implement actual memory utilization collection.

	return 62.0 // 62% memory utilization

}



func (rm *ResourceMonitor) getNetworkIOMetrics() NetworkIOMetrics {

	// This would implement actual network I/O metrics collection.

	return NetworkIOMetrics{

		BytesIn:    1024000,

		BytesOut:   2048000,

		PacketsIn:  5000,

		PacketsOut: 7000,

	}

}



func (rm *ResourceMonitor) getDiskIOMetrics() DiskIOMetrics {

	// This would implement actual disk I/O metrics collection.

	return DiskIOMetrics{

		BytesRead:    10240000,

		BytesWritten: 5120000,

		IOPSRead:     150,

		IOPSWrite:    75,

	}

}



func (rm *ResourceMonitor) getQueueDepthMetrics() map[interfaces.ProcessingPhase]int {

	// This would collect actual queue depth metrics from work queues.

	return map[interfaces.ProcessingPhase]int{

		interfaces.PhaseLLMProcessing:          25,

		interfaces.PhaseResourcePlanning:       15,

		interfaces.PhaseManifestGeneration:     8,

		interfaces.PhaseGitOpsCommit:           12,

		interfaces.PhaseDeploymentVerification: 20,

	}

}



func (rm *ResourceMonitor) getResponseTimeMetrics() map[interfaces.ProcessingPhase]time.Duration {

	// This would collect actual response time metrics.

	return map[interfaces.ProcessingPhase]time.Duration{

		interfaces.PhaseLLMProcessing:          45 * time.Second,

		interfaces.PhaseResourcePlanning:       12 * time.Second,

		interfaces.PhaseManifestGeneration:     5 * time.Second,

		interfaces.PhaseGitOpsCommit:           18 * time.Second,

		interfaces.PhaseDeploymentVerification: 90 * time.Second,

	}

}



func (rm *ResourceMonitor) getThroughputMetrics() map[interfaces.ProcessingPhase]float64 {

	// This would collect actual throughput metrics (operations per second).

	return map[interfaces.ProcessingPhase]float64{

		interfaces.PhaseLLMProcessing:          2.5,

		interfaces.PhaseResourcePlanning:       8.0,

		interfaces.PhaseManifestGeneration:     15.0,

		interfaces.PhaseGitOpsCommit:           4.2,

		interfaces.PhaseDeploymentVerification: 1.8,

	}

}



func (rm *ResourceMonitor) getErrorRateMetrics() map[interfaces.ProcessingPhase]float64 {

	// This would collect actual error rate metrics (percentage).

	return map[interfaces.ProcessingPhase]float64{

		interfaces.PhaseLLMProcessing:          2.1,

		interfaces.PhaseResourcePlanning:       0.8,

		interfaces.PhaseManifestGeneration:     0.3,

		interfaces.PhaseGitOpsCommit:           1.5,

		interfaces.PhaseDeploymentVerification: 3.2,

	}

}



func (rm *ResourceMonitor) getCacheMetrics() *CacheMetrics {

	// This would collect actual cache metrics.

	return &CacheMetrics{

		HitRate:      0.78,

		MissRate:     0.22,

		EvictionRate: 0.05,

		SizeBytes:    1048576000, // 1GB

		ItemCount:    125000,

	}

}



func (rm *ResourceMonitor) getConnectionMetrics() *ConnectionMetrics {

	// This would collect actual connection pool metrics.

	return &ConnectionMetrics{

		TotalConnections:  150,

		ActiveConnections: 95,

		IdleConnections:   55,

		FailedConnections: 3,

	}

}



func (rm *ResourceMonitor) updateMetrics(metrics *SystemMetrics) {

	rm.historyMutex.Lock()

	defer rm.historyMutex.Unlock()



	// Update current metrics.

	rm.currentMetrics = metrics



	// Add to history.

	rm.metricsHistory = append(rm.metricsHistory, *metrics)



	// Trim history if too large.

	if len(rm.metricsHistory) > rm.maxHistorySize {

		// Remove oldest 10% of entries.

		removeCount := rm.maxHistorySize / 10

		rm.metricsHistory = rm.metricsHistory[removeCount:]

	}

}



// NewMetricsAnalyzer creates a new metrics analyzer.

func NewMetricsAnalyzer(logger logr.Logger) *MetricsAnalyzer {

	return &MetricsAnalyzer{

		logger: logger.WithName("metrics-analyzer"),

	}

}



// AnalyzePerformance analyzes system performance metrics.

func (ma *MetricsAnalyzer) AnalyzePerformance(metrics *SystemMetrics) *PerformanceAnalysis {

	if metrics == nil {

		return &PerformanceAnalysis{

			OverallHealth: "unknown",

		}

	}



	analysis := &PerformanceAnalysis{

		CPUUtilization:    metrics.CPUUtilization,

		MemoryUtilization: metrics.MemoryUtilization,

		CacheHitRate:      metrics.CacheMetrics.HitRate,

		Bottlenecks:       make([]PerformanceBottleneck, 0),

		Trends:            make(map[string]PerformanceTrend),

	}



	// Calculate max queue depth.

	maxQueueDepth := 0

	for _, depth := range metrics.QueueDepths {

		if depth > maxQueueDepth {

			maxQueueDepth = depth

		}

	}

	analysis.MaxQueueDepth = maxQueueDepth



	// Calculate average response time.

	totalResponseTime := time.Duration(0)

	phaseCount := len(metrics.ResponseTimes)

	for _, responseTime := range metrics.ResponseTimes {

		totalResponseTime += responseTime

	}

	if phaseCount > 0 {

		analysis.AverageResponseTime = totalResponseTime / time.Duration(phaseCount)

	}



	// Calculate total throughput.

	totalThroughput := 0.0

	for _, throughput := range metrics.ThroughputRates {

		totalThroughput += throughput

	}

	analysis.TotalThroughput = totalThroughput



	// Calculate overall error rate.

	totalErrorRate := 0.0

	for _, errorRate := range metrics.ErrorRates {

		totalErrorRate += errorRate

	}

	if phaseCount > 0 {

		analysis.OverallErrorRate = totalErrorRate / float64(phaseCount)

	}



	// Determine overall health.

	analysis.OverallHealth = ma.determineOverallHealth(analysis)



	// Identify bottlenecks.

	analysis.Bottlenecks = ma.identifyBottlenecks(metrics)



	return analysis

}



func (ma *MetricsAnalyzer) determineOverallHealth(analysis *PerformanceAnalysis) string {

	healthScore := 100.0



	// Deduct points for high resource utilization.

	if analysis.CPUUtilization > 80 {

		healthScore -= 20

	} else if analysis.CPUUtilization > 60 {

		healthScore -= 10

	}



	if analysis.MemoryUtilization > 85 {

		healthScore -= 20

	} else if analysis.MemoryUtilization > 70 {

		healthScore -= 10

	}



	// Deduct points for high error rates.

	if analysis.OverallErrorRate > 5 {

		healthScore -= 30

	} else if analysis.OverallErrorRate > 2 {

		healthScore -= 15

	}



	// Deduct points for low cache hit rates.

	if analysis.CacheHitRate < 0.5 {

		healthScore -= 15

	} else if analysis.CacheHitRate < 0.7 {

		healthScore -= 5

	}



	// Deduct points for high queue depths.

	if analysis.MaxQueueDepth > 100 {

		healthScore -= 20

	} else if analysis.MaxQueueDepth > 50 {

		healthScore -= 10

	}



	// Determine health category.

	if healthScore >= 80 {

		return "healthy"

	} else if healthScore >= 60 {

		return "degraded"

	} else {

		return "unhealthy"

	}

}



func (ma *MetricsAnalyzer) identifyBottlenecks(metrics *SystemMetrics) []PerformanceBottleneck {

	bottlenecks := make([]PerformanceBottleneck, 0)



	// Check for CPU bottlenecks.

	if metrics.CPUUtilization > 80 {

		bottlenecks = append(bottlenecks, PerformanceBottleneck{

			Type:        "cpu",

			Severity:    "high",

			Description: fmt.Sprintf("High CPU utilization: %.1f%%", metrics.CPUUtilization),

			Impact:      metrics.CPUUtilization / 100.0,

		})

	}



	// Check for memory bottlenecks.

	if metrics.MemoryUtilization > 85 {

		bottlenecks = append(bottlenecks, PerformanceBottleneck{

			Type:        "memory",

			Severity:    "high",

			Description: fmt.Sprintf("High memory utilization: %.1f%%", metrics.MemoryUtilization),

			Impact:      metrics.MemoryUtilization / 100.0,

		})

	}



	// Check for queue bottlenecks.

	for phase, depth := range metrics.QueueDepths {

		if depth > 50 {

			severity := "medium"

			if depth > 100 {

				severity = "high"

			}



			bottlenecks = append(bottlenecks, PerformanceBottleneck{

				Phase:       phase,

				Type:        "queue",

				Severity:    severity,

				Description: fmt.Sprintf("High queue depth for %s: %d", phase, depth),

				Impact:      float64(depth) / 200.0, // Normalize to 0-1 scale

			})

		}

	}



	// Check for response time bottlenecks.

	for phase, responseTime := range metrics.ResponseTimes {

		if responseTime > 60*time.Second {

			severity := "medium"

			if responseTime > 120*time.Second {

				severity = "high"

			}



			bottlenecks = append(bottlenecks, PerformanceBottleneck{

				Phase:       phase,

				Type:        "response_time",

				Severity:    severity,

				Description: fmt.Sprintf("High response time for %s: %v", phase, responseTime),

				Impact:      float64(responseTime) / float64(300*time.Second), // Normalize to 5-minute scale

			})

		}

	}



	return bottlenecks

}



// NewScalingDecisionEngine creates a new scaling decision engine.

func NewScalingDecisionEngine(config *PerformanceConfig, logger logr.Logger) *ScalingDecisionEngine {

	return &ScalingDecisionEngine{

		logger:    logger.WithName("scaling-engine"),

		config:    config,

		decisions: make([]ScalingDecision, 0),

	}

}



// EvaluateScaling evaluates whether scaling actions are needed.

func (sde *ScalingDecisionEngine) EvaluateScaling(ctx context.Context, metrics *SystemMetrics) []ScalingDecision {

	decisions := make([]ScalingDecision, 0)



	if metrics == nil {

		return decisions

	}



	// Check CPU-based scaling.

	if metrics.CPUUtilization > sde.config.CPUScaleUpThreshold {

		decision := ScalingDecision{

			Action:          "scale_up",

			CurrentReplicas: 3, // This would be retrieved from actual deployment

			TargetReplicas:  4, // Scale up by 1

			Reason:          fmt.Sprintf("CPU utilization %.1f%% exceeds threshold %.1f%%", metrics.CPUUtilization, sde.config.CPUScaleUpThreshold),

			Confidence:      0.8,

			Timestamp:       time.Now(),

		}

		decisions = append(decisions, decision)

	} else if metrics.CPUUtilization < sde.config.CPUScaleDownThreshold {

		decision := ScalingDecision{

			Action:          "scale_down",

			CurrentReplicas: 3,

			TargetReplicas:  2, // Scale down by 1

			Reason:          fmt.Sprintf("CPU utilization %.1f%% below threshold %.1f%%", metrics.CPUUtilization, sde.config.CPUScaleDownThreshold),

			Confidence:      0.7,

			Timestamp:       time.Now(),

		}

		decisions = append(decisions, decision)

	}



	// Check memory-based scaling.

	if metrics.MemoryUtilization > sde.config.MemoryScaleUpThreshold {

		decision := ScalingDecision{

			Action:          "scale_up",

			CurrentReplicas: 3,

			TargetReplicas:  4,

			Reason:          fmt.Sprintf("Memory utilization %.1f%% exceeds threshold %.1f%%", metrics.MemoryUtilization, sde.config.MemoryScaleUpThreshold),

			Confidence:      0.75,

			Timestamp:       time.Now(),

		}

		decisions = append(decisions, decision)

	}



	// Check queue-based scaling.

	for phase, depth := range metrics.QueueDepths {

		if depth > sde.config.QueueDepthThreshold {

			decision := ScalingDecision{

				Action:          "scale_up",

				Phase:           phase,

				CurrentReplicas: 2, // This would be retrieved from actual phase worker count

				TargetReplicas:  3,

				Reason:          fmt.Sprintf("Queue depth %d for phase %s exceeds threshold %d", depth, phase, sde.config.QueueDepthThreshold),

				Confidence:      0.85,

				Timestamp:       time.Now(),

			}

			decisions = append(decisions, decision)

		}

	}



	// Store decisions for history.

	sde.storeDecisions(decisions)



	return decisions

}



// ApplyScalingProfile applies a scaling profile to the decision engine.

func (sde *ScalingDecisionEngine) ApplyScalingProfile(profile ScalingProfile) {

	sde.logger.Info("Applying scaling profile", "profile", profile)

	// This would update internal scaling parameters based on the profile.

}



func (sde *ScalingDecisionEngine) storeDecisions(decisions []ScalingDecision) {

	sde.decisionMutex.Lock()

	defer sde.decisionMutex.Unlock()



	sde.decisions = append(sde.decisions, decisions...)



	// Keep only recent decisions (last 1000).

	if len(sde.decisions) > 1000 {

		sde.decisions = sde.decisions[len(sde.decisions)-1000:]

	}

}



// NewConnectionPoolManager creates a new connection pool manager.

func NewConnectionPoolManager(config *PerformanceConfig, logger logr.Logger) *ConnectionPoolManager {

	return &ConnectionPoolManager{

		logger:    logger.WithName("connection-pool-manager"),

		config:    config,

		httpPools: make(map[string]*HTTPConnectionPool),

		dbPools:   make(map[string]*DatabaseConnectionPool),

	}

}



// Start starts the connection pool manager.

func (cpm *ConnectionPoolManager) Start(ctx context.Context) error {

	cpm.mutex.Lock()

	defer cpm.mutex.Unlock()



	if cpm.started {

		return fmt.Errorf("connection pool manager already started")

	}



	// Initialize default pools.

	cpm.httpPools["default"] = NewHTTPConnectionPool("default", cpm.config.MaxConnectionsPerPool, cpm.logger)

	cpm.httpPools["llm"] = NewHTTPConnectionPool("llm", cpm.config.MaxConnectionsPerPool/2, cpm.logger) // Smaller pool for LLM APIs

	cpm.httpPools["git"] = NewHTTPConnectionPool("git", cpm.config.MaxConnectionsPerPool/4, cpm.logger) // Even smaller for git



	cpm.started = true

	cpm.logger.Info("Connection pool manager started")



	return nil

}



// Stop stops the connection pool manager.

func (cpm *ConnectionPoolManager) Stop(ctx context.Context) error {

	cpm.mutex.Lock()

	defer cpm.mutex.Unlock()



	if !cpm.started {

		return nil

	}



	// Close all pools.

	for name, pool := range cpm.httpPools {

		if err := pool.Close(); err != nil {

			cpm.logger.Error(err, "Error closing HTTP pool", "pool", name)

		}

	}



	for name, pool := range cpm.dbPools {

		if err := pool.Close(); err != nil {

			cpm.logger.Error(err, "Error closing DB pool", "pool", name)

		}

	}



	cpm.started = false

	cpm.logger.Info("Connection pool manager stopped")



	return nil

}



// ApplyResourceProfile applies a resource profile to connection pools.

func (cpm *ConnectionPoolManager) ApplyResourceProfile(profile ResourceProfile) error {

	cpm.logger.Info("Applying resource profile to connection pools", "profile", profile)



	// Adjust pool sizes based on profile.

	for _, pool := range cpm.httpPools {

		pool.Resize(profile.MaxWorkers * 2) // 2 connections per worker

	}



	return nil

}



// NewLoadBalancer creates a new load balancer.

func NewLoadBalancer(logger logr.Logger) *LoadBalancer {

	return &LoadBalancer{

		logger:    logger.WithName("load-balancer"),

		instances: make(map[string]*ControllerInstance),

		strategy:  LoadBalancingStrategyRoundRobin,

	}

}



// Supporting data structures and placeholder implementations.



// LoadBalancingStrategy defines load balancing strategies.

type LoadBalancingStrategy string



const (

	// LoadBalancingStrategyRoundRobin holds loadbalancingstrategyroundrobin value.

	LoadBalancingStrategyRoundRobin LoadBalancingStrategy = "round_robin"

	// LoadBalancingStrategyLeastConnections holds loadbalancingstrategyleastconnections value.

	LoadBalancingStrategyLeastConnections LoadBalancingStrategy = "least_connections"

	// LoadBalancingStrategyResourceBased holds loadbalancingstrategyresourcebased value.

	LoadBalancingStrategyResourceBased LoadBalancingStrategy = "resource_based"

)



// ControllerInstance represents a controller instance for load balancing.

type ControllerInstance struct {

	ID               string                       `json:"id"`

	Address          string                       `json:"address"`

	Health           string                       `json:"health"` // healthy, degraded, unhealthy

	CurrentLoad      int                          `json:"currentLoad"`

	MaxCapacity      int                          `json:"maxCapacity"`

	ProcessingPhases []interfaces.ProcessingPhase `json:"processingPhases"`

	AvgResponseTime  time.Duration                `json:"avgResponseTime"`

	ErrorRate        float64                      `json:"errorRate"`

	ThroughputRate   float64                      `json:"throughputRate"`

}



// HTTPConnectionPool manages HTTP connections.

type HTTPConnectionPool struct {

	name     string

	maxConns int

	logger   logr.Logger

	// Implementation would include actual connection pool logic.

}



// NewHTTPConnectionPool performs newhttpconnectionpool operation.

func NewHTTPConnectionPool(name string, maxConns int, logger logr.Logger) *HTTPConnectionPool {

	return &HTTPConnectionPool{

		name:     name,

		maxConns: maxConns,

		logger:   logger.WithName("http-pool").WithValues("pool", name),

	}

}



// Close performs close operation.

func (p *HTTPConnectionPool) Close() error {

	p.logger.Info("Closing HTTP connection pool")

	return nil

}



// Resize performs resize operation.

func (p *HTTPConnectionPool) Resize(newSize int) {

	p.logger.Info("Resizing HTTP connection pool", "oldSize", p.maxConns, "newSize", newSize)

	p.maxConns = newSize

}



// DatabaseConnectionPool manages database connections.

type DatabaseConnectionPool struct {

	name     string

	maxConns int

	logger   logr.Logger

	// Implementation would include actual database connection pool logic.

}



// NewDatabaseConnectionPool performs newdatabaseconnectionpool operation.

func NewDatabaseConnectionPool(name string, maxConns int, logger logr.Logger) *DatabaseConnectionPool {

	return &DatabaseConnectionPool{

		name:     name,

		maxConns: maxConns,

		logger:   logger.WithName("db-pool").WithValues("pool", name),

	}

}



// Close performs close operation.

func (p *DatabaseConnectionPool) Close() error {

	p.logger.Info("Closing database connection pool")

	return nil

}

