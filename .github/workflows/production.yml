# Comprehensive 9-Stage Production Pipeline for Nephoran Intent Operator
# This pipeline implements zero-manual-intervention deployment with comprehensive quality gates
# Stages: Code Quality → Testing → Performance → Security → Build → Staging → Production → Release → Monitoring

name: Production Deployment Pipeline

on:
  push:
    branches: [main]
    tags: ['v*']
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'production'
        type: choice
        options:
          - production
          - staging
      force_deploy:
        description: 'Force deployment (bypass some quality gates)'
        required: false
        default: false
        type: boolean

env:
  # Container registry configuration
  REGISTRY: us-central1-docker.pkg.dev/poised-elf-466913-q2/nephoran
  # Quality gate thresholds
  MIN_COVERAGE: "90"
  MAX_SECURITY_SCORE: "7.0"  # CVSS threshold
  MAX_PERFORMANCE_REGRESSION: "10"  # percentage
  # Kubernetes configuration
  STAGING_CLUSTER: "nephoran-staging"
  PRODUCTION_CLUSTER: "nephoran-production"
  # Monitoring configuration
  MONITORING_DURATION: "24h"

jobs:
  # ===================================
  # STAGE 1: CODE QUALITY
  # ===================================
  code-quality:
    name: "Stage 1: Code Quality & SAST"
    runs-on: ubuntu-22.04
    outputs:
      version: ${{ steps.version.outputs.version }}
      should_deploy: ${{ steps.quality-gate.outputs.should_deploy }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for SonarCloud
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.24'
          cache: true

      - name: Generate version
        id: version
        run: |
          if [[ "${{ github.ref }}" == refs/tags/* ]]; then
            VERSION=${GITHUB_REF#refs/tags/}
          else
            VERSION=$(git describe --tags --always --dirty 2>/dev/null || echo "v0.0.0-$(git rev-parse --short HEAD)")
          fi
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "VERSION=$VERSION" >> $GITHUB_ENV

      - name: Code formatting check
        run: |
          unformatted=$(gofmt -l .)
          if [ -n "$unformatted" ]; then
            echo "❌ Code formatting issues found:"
            echo "$unformatted"
            exit 1
          fi
          echo "✅ Code formatting passed"

      - name: Install linting tools
        run: |
          go install github.com/golangci/golangci-lint/cmd/golangci-lint@v1.61.0
          go install honnef.co/go/tools/cmd/staticcheck@latest
          go install github.com/securego/gosec/v2/cmd/gosec@latest

      - name: Run golangci-lint
        run: |
          golangci-lint run --timeout=10m --out-format=github-actions ./...

      - name: Run staticcheck
        run: |
          staticcheck -f stylish ./...

      - name: SAST Analysis with gosec
        run: |
          mkdir -p security-reports/gosec
          gosec -fmt sarif -out security-reports/gosec/gosec.sarif -severity high ./... || true
          # Ensure SARIF file exists even if no issues found
          if [ ! -f "security-reports/gosec/gosec.sarif" ]; then
            echo '{"version":"2.1.0","$schema":"https://json.schemastore.org/sarif-2.1.0.json","runs":[{"tool":{"driver":{"name":"gosec","version":"2.18.2"}},"results":[]}]}' > security-reports/gosec/gosec.sarif
          fi
          gosec -fmt json -out security-reports/gosec/gosec.json ./... || true

      - name: Upload SAST results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always() && hashFiles('security-reports/gosec/gosec.sarif') != ''
        with:
          sarif_file: security-reports/gosec/gosec.sarif

      - name: SonarCloud Scan
        uses: SonarSource/sonarcloud-github-action@master
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        with:
          args: >
            -Dsonar.projectKey=nephoran-intent-operator
            -Dsonar.organization=${{ secrets.SONAR_ORGANIZATION }}
            -Dsonar.go.coverage.reportPaths=coverage.out
            -Dsonar.go.golangci-lint.reportPaths=golangci-lint-report.xml

      - name: Quality Gate check
        id: quality-gate
        run: |
          # Check SonarCloud quality gate
          QUALITY_GATE_STATUS=$(curl -s -u "${{ secrets.SONAR_TOKEN }}:" \
            "https://sonarcloud.io/api/qualitygates/project_status?projectKey=nephoran-intent-operator" | \
            jq -r '.projectStatus.status')
          
          if [[ "$QUALITY_GATE_STATUS" != "OK" && "${{ inputs.force_deploy }}" != "true" ]]; then
            echo "❌ SonarCloud Quality Gate failed: $QUALITY_GATE_STATUS"
            echo "should_deploy=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          echo "✅ Code quality checks passed"
          echo "should_deploy=true" >> $GITHUB_OUTPUT

      - name: Store quality reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: quality-reports
          path: security-reports/
          retention-days: 30

  # ===================================
  # STAGE 2: COMPREHENSIVE TESTING
  # ===================================
  comprehensive-testing:
    name: "Stage 2: Comprehensive Testing"
    runs-on: ubuntu-22.04
    needs: [code-quality]
    if: needs.code-quality.outputs.should_deploy == 'true'
    strategy:
      matrix:
        test-type: [unit, integration, e2e]
        go-version: ['1.24']
    outputs:
      coverage_percentage: ${{ steps.coverage.outputs.coverage }}
      test_results: ${{ steps.test-summary.outputs.results }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go ${{ matrix.go-version }}
        uses: actions/setup-go@v5
        with:
          go-version: ${{ matrix.go-version }}
          cache: true

      - name: Install test dependencies
        run: |
          go install github.com/onsi/ginkgo/v2/ginkgo@latest
          go install github.com/onsi/gomega@latest
          go mod download

      - name: Set up test infrastructure
        run: |
          # Install kind for Kubernetes testing
          curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
          chmod +x ./kind && sudo mv ./kind /usr/local/bin/kind
          
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/kubectl

      - name: Run ${{ matrix.test-type }} tests
        run: |
          mkdir -p test-results
          case "${{ matrix.test-type }}" in
            "unit")
              ./scripts/run-comprehensive-tests.sh --type unit
              ;;
            "integration")
              ./scripts/run-comprehensive-tests.sh --type integration
              ;;
            "e2e")
              # Create kind cluster for E2E tests
              kind create cluster --config=tests/e2e/kind-config.yaml --wait=300s
              kubectl cluster-info
              ./scripts/run-comprehensive-tests.sh --type e2e
              ;;
          esac

      - name: Calculate coverage
        id: coverage
        if: matrix.test-type == 'unit'
        run: |
          if [ -f test-results/coverage/coverage.out ]; then
            coverage=$(go tool cover -func=test-results/coverage/coverage.out | grep total: | awk '{print $3}' | sed 's/%//')
            echo "coverage=$coverage" >> $GITHUB_OUTPUT
            echo "Coverage: $coverage%"
            
            if (( $(echo "$coverage < $MIN_COVERAGE" | bc -l) )); then
              echo "❌ Coverage $coverage% below threshold $MIN_COVERAGE%"
              exit 1
            fi
            echo "✅ Coverage $coverage% meets threshold $MIN_COVERAGE%"
          fi

      - name: Test summary
        id: test-summary
        run: |
          # Generate test summary
          results_file="test-results/test-summary-${{ matrix.test-type }}.json"
          echo '{"type":"${{ matrix.test-type }}","status":"passed","timestamp":"'$(date -Iseconds)'"}' > "$results_file"
          echo "results=$results_file" >> $GITHUB_OUTPUT

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.test-type }}
          path: test-results/
          retention-days: 30

  # ===================================
  # STAGE 3: PERFORMANCE TESTING
  # ===================================
  performance-testing:
    name: "Stage 3: Performance & Load Testing"
    runs-on: ubuntu-22.04
    needs: [code-quality, comprehensive-testing]
    if: needs.code-quality.outputs.should_deploy == 'true'
    outputs:
      performance_score: ${{ steps.perf-analysis.outputs.score }}
      regression_detected: ${{ steps.regression-check.outputs.detected }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.24'
          cache: true

      - name: Install performance testing tools
        run: |
          go install github.com/rakyll/hey@latest
          go install github.com/tsenart/vegeta@latest
          sudo apt-get update && sudo apt-get install -y bc

      - name: Create kind cluster for performance testing
        run: |
          curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
          chmod +x ./kind && sudo mv ./kind /usr/local/bin/kind
          kind create cluster --config=tests/performance/kind-config.yaml --wait=300s

      - name: Deploy application for performance testing
        run: |
          # Build and load images into kind
          make docker-build VERSION=${{ needs.code-quality.outputs.version }}
          kind load docker-image $REGISTRY/llm-processor:${{ needs.code-quality.outputs.version }}
          kind load docker-image $REGISTRY/nephio-bridge:${{ needs.code-quality.outputs.version }}
          kind load docker-image $REGISTRY/oran-adaptor:${{ needs.code-quality.outputs.version }}
          kind load docker-image $REGISTRY/rag-api:${{ needs.code-quality.outputs.version }}
          
          # Deploy to kind cluster
          kubectl apply -f deployments/kustomize/overlays/dev/

      - name: Wait for deployment readiness
        run: |
          kubectl wait --for=condition=available --timeout=600s deployment/llm-processor
          kubectl wait --for=condition=available --timeout=600s deployment/nephio-bridge
          kubectl wait --for=condition=available --timeout=600s deployment/oran-adaptor

      - name: Run performance tests
        run: |
          mkdir -p test-results/performance
          ./scripts/execute-production-load-test.sh --duration=10m --output=test-results/performance/

      - name: Performance analysis
        id: perf-analysis
        run: |
          # Analyze performance results
          python3 << 'EOF'
          import json
          import sys
          
          # Load performance data
          try:
              with open('test-results/performance/results.json', 'r') as f:
                  data = json.load(f)
              
              # Calculate performance score (0-100, higher is better)
              latency_score = max(0, 100 - (data.get('avg_latency_ms', 1000) / 10))
              throughput_score = min(100, data.get('requests_per_sec', 0) / 10)
              error_score = max(0, 100 - (data.get('error_rate_percent', 50) * 2))
              
              overall_score = (latency_score + throughput_score + error_score) / 3
              
              print(f"Performance Score: {overall_score:.2f}/100")
              print(f"::set-output name=score::{overall_score:.2f}")
              
              # Fail if performance is too poor
              if overall_score < 70:
                  print("❌ Performance score below threshold (70)")
                  sys.exit(1)
              
              print("✅ Performance tests passed")
              
          except FileNotFoundError:
              print("❌ Performance results file not found")
              sys.exit(1)
          EOF

      - name: Performance regression check
        id: regression-check
        run: |
          # Download previous performance baseline
          BASELINE_FILE="performance-baseline.json"
          
          # Try to download from previous successful run
          curl -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
               -H "Accept: application/vnd.github.v3.raw" \
               -o "$BASELINE_FILE" \
               "https://api.github.com/repos/${{ github.repository }}/contents/$BASELINE_FILE" || echo "No baseline found"
          
          if [ -f "$BASELINE_FILE" ] && [ -f "test-results/performance/results.json" ]; then
            python3 << 'EOF'
          import json
          import sys
          
          with open('performance-baseline.json', 'r') as f:
              baseline = json.load(f)
          
          with open('test-results/performance/results.json', 'r') as f:
              current = json.load(f)
          
          # Check for regression
          baseline_latency = baseline.get('avg_latency_ms', 1000)
          current_latency = current.get('avg_latency_ms', 1000)
          
          regression_percent = ((current_latency - baseline_latency) / baseline_latency) * 100
          
          print(f"Latency regression: {regression_percent:.2f}%")
          
          if regression_percent > int('${{ env.MAX_PERFORMANCE_REGRESSION }}'):
              print(f"❌ Performance regression detected: {regression_percent:.2f}%")
              print(f"::set-output name=detected::true")
              if '${{ inputs.force_deploy }}' != 'true':
                  sys.exit(1)
          else:
              print("✅ No significant performance regression")
              print(f"::set-output name=detected::false")
          EOF
          else
            echo "No baseline for comparison - establishing new baseline"
            echo "detected=false" >> $GITHUB_OUTPUT
          fi

      - name: Store performance results
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: test-results/performance/
          retention-days: 90

  # ===================================
  # STAGE 4: SECURITY SCANNING
  # ===================================
  security-scanning:
    name: "Stage 4: Advanced Security Scanning"
    runs-on: ubuntu-22.04
    needs: [code-quality]
    if: needs.code-quality.outputs.should_deploy == 'true'
    outputs:
      security_score: ${{ steps.security-analysis.outputs.score }}
      critical_vulns: ${{ steps.vuln-check.outputs.critical_count }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.24'
          cache: true

      - name: Install security tools
        run: |
          # Install Trivy
          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
          
          # Install Cosign
          curl -O -L "https://github.com/sigstore/cosign/releases/latest/download/cosign-linux-amd64"
          sudo mv cosign-linux-amd64 /usr/local/bin/cosign
          sudo chmod +x /usr/local/bin/cosign
          
          # Install Syft for SBOM generation
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
          
          # Install additional scanners
          go install github.com/sonatypecommunity/nancy@latest
          go install golang.org/x/vuln/cmd/govulncheck@latest

      - name: Dependency vulnerability scan
        run: |
          mkdir -p security-reports
          
          # Nancy scan
          go list -json -m all | nancy sleuth --output-format=json > security-reports/nancy-report.json || true
          
          # govulncheck
          govulncheck -json ./... > security-reports/govulncheck-report.json || true

      - name: Build container images for scanning
        run: |
          make docker-build VERSION=${{ needs.code-quality.outputs.version }}

      - name: Container security scanning
        run: |
          services=("llm-processor" "nephio-bridge" "oran-adaptor" "rag-api")
          
          for service in "${services[@]}"; do
            echo "Scanning $service container..."
            
            # Trivy scan
            trivy image --format json --output security-reports/trivy-$service.json \
              $REGISTRY/$service:${{ needs.code-quality.outputs.version }}
            
            # Generate SBOM
            syft $REGISTRY/$service:${{ needs.code-quality.outputs.version }} \
              -o spdx-json > security-reports/sbom-$service.json
          done

      - name: Kubernetes manifest security scan
        run: |
          # Scan Kubernetes manifests with Trivy
          trivy config --format json --output security-reports/k8s-manifests.json deployments/
          
          # Additional manifest validation with kube-score
          curl -L https://github.com/zegl/kube-score/releases/latest/download/kube-score_linux_amd64 -o kube-score
          chmod +x kube-score && sudo mv kube-score /usr/local/bin/
          
          find deployments/ -name "*.yaml" -exec kube-score score {} \; > security-reports/kube-score-report.txt || true

      - name: Secrets scanning
        run: |
          # Install and run gitleaks
          curl -L https://github.com/gitleaks/gitleaks/releases/latest/download/gitleaks-linux-amd64 -o gitleaks
          chmod +x gitleaks && sudo mv gitleaks /usr/local/bin/
          
          gitleaks detect --source . --report-format json --report-path security-reports/gitleaks-report.json || true

      - name: Security vulnerability analysis
        id: vuln-check
        run: |
          python3 << 'EOF'
          import json
          import glob
          import sys
          
          critical_vulns = 0
          high_vulns = 0
          total_vulns = 0
          
          # Analyze Trivy reports
          for report_file in glob.glob('security-reports/trivy-*.json'):
              try:
                  with open(report_file, 'r') as f:
                      data = json.load(f)
                  
                  results = data.get('Results', [])
                  for result in results:
                      vulns = result.get('Vulnerabilities', [])
                      for vuln in vulns:
                          severity = vuln.get('Severity', '').upper()
                          total_vulns += 1
                          if severity == 'CRITICAL':
                              critical_vulns += 1
                          elif severity == 'HIGH':
                              high_vulns += 1
              except Exception as e:
                  print(f"Error processing {report_file}: {e}")
          
          print(f"Security scan results:")
          print(f"- Critical vulnerabilities: {critical_vulns}")
          print(f"- High vulnerabilities: {high_vulns}")
          print(f"- Total vulnerabilities: {total_vulns}")
          
          print(f"::set-output name=critical_count::{critical_vulns}")
          print(f"::set-output name=high_count::{high_vulns}")
          print(f"::set-output name=total_count::{total_vulns}")
          
          # Calculate security score (0-100, higher is better)
          max_allowed_critical = 0
          max_allowed_high = 5
          
          if critical_vulns > max_allowed_critical:
              print(f"❌ {critical_vulns} critical vulnerabilities found (max: {max_allowed_critical})")
              if '${{ inputs.force_deploy }}' != 'true':
                  sys.exit(1)
          
          if high_vulns > max_allowed_high:
              print(f"⚠️ {high_vulns} high vulnerabilities found (max: {max_allowed_high})")
          
          # Calculate security score
          security_score = max(0, 100 - (critical_vulns * 50) - (high_vulns * 10))
          print(f"Security score: {security_score}/100")
          print(f"::set-output name=score::{security_score}")
          
          if security_score >= 80:
              print("✅ Security scan passed")
          else:
              print("❌ Security score below threshold (80)")
              if '${{ inputs.force_deploy }}' != 'true':
                  sys.exit(1)
          EOF

      - name: Upload security reports to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: security-reports/
          category: "security-scan"

      - name: Store security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: security-reports/
          retention-days: 90

  # ===================================
  # STAGE 5: BUILD AND PUSH
  # ===================================
  build-and-push:
    name: "Stage 5: Multi-arch Build & Push"
    runs-on: ubuntu-22.04
    needs: [code-quality, comprehensive-testing, performance-testing, security-scanning]
    if: needs.code-quality.outputs.should_deploy == 'true'
    outputs:
      images_digest: ${{ steps.build.outputs.digest }}
      sbom_attestation: ${{ steps.attestation.outputs.sbom }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          platforms: linux/amd64,linux/arm64

      - name: Configure Google Cloud authentication
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Configure Docker for GCP
        run: |
          gcloud auth configure-docker us-central1-docker.pkg.dev --quiet

      - name: Install cosign and syft
        run: |
          curl -O -L "https://github.com/sigstore/cosign/releases/latest/download/cosign-linux-amd64"
          sudo mv cosign-linux-amd64 /usr/local/bin/cosign
          sudo chmod +x /usr/local/bin/cosign
          
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin

      - name: Build and push multi-arch images
        id: build
        run: |
          VERSION=${{ needs.code-quality.outputs.version }}
          SERVICES=("llm-processor" "nephio-bridge" "oran-adaptor" "rag-api")
          
          # Build all images with multi-arch support
          for service in "${SERVICES[@]}"; do
            echo "Building $service..."
            
            if [ "$service" = "rag-api" ]; then
              docker buildx build \
                --platform linux/amd64,linux/arm64 \
                --push \
                -f rag-python/Dockerfile \
                -t $REGISTRY/$service:$VERSION \
                -t $REGISTRY/$service:latest \
                --label "org.opencontainers.image.source=${{ github.server_url }}/${{ github.repository }}" \
                --label "org.opencontainers.image.version=$VERSION" \
                --label "org.opencontainers.image.created=$(date -Iseconds)" \
                --label "org.opencontainers.image.revision=${{ github.sha }}" \
                --provenance=true \
                --sbom=true \
                ./rag-python
            else
              docker buildx build \
                --platform linux/amd64,linux/arm64 \
                --push \
                --target $service \
                -t $REGISTRY/$service:$VERSION \
                -t $REGISTRY/$service:latest \
                --label "org.opencontainers.image.source=${{ github.server_url }}/${{ github.repository }}" \
                --label "org.opencontainers.image.version=$VERSION" \
                --label "org.opencontainers.image.created=$(date -Iseconds)" \
                --label "org.opencontainers.image.revision=${{ github.sha }}" \
                --provenance=true \
                --sbom=true \
                .
            fi
          done
          
          echo "Images built and pushed successfully"

      - name: Sign container images
        run: |
          SERVICES=("llm-processor" "nephio-bridge" "oran-adaptor" "rag-api")
          VERSION=${{ needs.code-quality.outputs.version }}
          
          for service in "${SERVICES[@]}"; do
            echo "Signing $service..."
            cosign sign --yes $REGISTRY/$service:$VERSION
            cosign sign --yes $REGISTRY/$service:latest
          done

      - name: Generate and attest SBOM
        id: attestation
        run: |
          SERVICES=("llm-processor" "nephio-bridge" "oran-adaptor" "rag-api")
          VERSION=${{ needs.code-quality.outputs.version }}
          
          mkdir -p sbom-reports
          
          for service in "${SERVICES[@]}"; do
            echo "Generating SBOM for $service..."
            syft $REGISTRY/$service:$VERSION -o spdx-json > sbom-reports/$service-sbom.spdx.json
            
            # Attest SBOM
            cosign attest --yes --predicate sbom-reports/$service-sbom.spdx.json \
              --type spdxjson $REGISTRY/$service:$VERSION
          done
          
          echo "sbom=sbom-reports/" >> $GITHUB_OUTPUT

      - name: Store build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-artifacts
          path: |
            sbom-reports/
          retention-days: 90

  # ===================================
  # STAGE 6: STAGING DEPLOYMENT
  # ===================================
  staging-deployment:
    name: "Stage 6: Staging Deployment & Smoke Tests"
    runs-on: ubuntu-22.04
    needs: [code-quality, build-and-push]
    if: needs.code-quality.outputs.should_deploy == 'true'
    environment: staging
    outputs:
      deployment_url: ${{ steps.deploy.outputs.url }}
      smoke_test_results: ${{ steps.smoke-tests.outputs.results }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure Google Cloud authentication
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Get GKE credentials
        run: |
          gcloud container clusters get-credentials $STAGING_CLUSTER \
            --zone us-central1-a --project poised-elf-466913-q2

      - name: Deploy to staging
        id: deploy
        run: |
          VERSION=${{ needs.code-quality.outputs.version }}
          
          # Update image tags in staging overlay
          cd deployments/kustomize/overlays/staging
          
          kustomize edit set image \
            llm-processor=$REGISTRY/llm-processor:$VERSION \
            nephio-bridge=$REGISTRY/nephio-bridge:$VERSION \
            oran-adaptor=$REGISTRY/oran-adaptor:$VERSION \
            rag-api=$REGISTRY/rag-api:$VERSION
          
          # Apply staging deployment
          kustomize build . | kubectl apply -f -
          
          # Wait for deployment readiness
          kubectl rollout status deployment/llm-processor --timeout=600s
          kubectl rollout status deployment/nephio-bridge --timeout=600s
          kubectl rollout status deployment/oran-adaptor --timeout=600s
          
          # Get staging URL
          STAGING_URL=$(kubectl get service llm-processor -o jsonpath='{.status.loadBalancer.ingress[0].ip}' || echo "cluster-internal")
          echo "url=http://$STAGING_URL" >> $GITHUB_OUTPUT

      - name: Run smoke tests
        id: smoke-tests
        run: |
          # Install testing dependencies
          sudo apt-get update && sudo apt-get install -y curl jq
          
          # Wait for services to be fully ready
          sleep 60
          
          # Smoke test results
          mkdir -p test-results/smoke
          
          # Test 1: Health checks
          echo "Running health checks..."
          SERVICES=("llm-processor" "nephio-bridge" "oran-adaptor")
          
          for service in "${SERVICES[@]}"; do
            kubectl get pod -l app=$service -o jsonpath='{.items[*].status.phase}' | grep -q "Running" || {
              echo "❌ $service pods not running"
              exit 1
            }
            echo "✅ $service pods are running"
          done
          
          # Test 2: API endpoints
          echo "Testing API endpoints..."
          LLM_PROCESSOR_POD=$(kubectl get pod -l app=llm-processor -o jsonpath='{.items[0].metadata.name}')
          
          kubectl port-forward $LLM_PROCESSOR_POD 8080:8080 &
          PORT_FORWARD_PID=$!
          sleep 10
          
          # Test health endpoint
          curl -f http://localhost:8080/health || {
            echo "❌ Health endpoint failed"
            kill $PORT_FORWARD_PID
            exit 1
          }
          echo "✅ Health endpoint accessible"
          
          # Test metrics endpoint
          curl -f http://localhost:8080/metrics || {
            echo "❌ Metrics endpoint failed"
            kill $PORT_FORWARD_PID
            exit 1
          }
          echo "✅ Metrics endpoint accessible"
          
          kill $PORT_FORWARD_PID
          
          # Generate smoke test results
          echo '{"status":"passed","timestamp":"'$(date -Iseconds)'","tests_run":5}' > test-results/smoke/results.json
          echo "results=test-results/smoke/results.json" >> $GITHUB_OUTPUT

      - name: Staging deployment summary
        run: |
          echo "✅ Staging deployment successful"
          echo "Deployment URL: ${{ steps.deploy.outputs.url }}"
          kubectl get pods -o wide
          kubectl get services

      - name: Store smoke test results
        uses: actions/upload-artifact@v4
        with:
          name: smoke-test-results
          path: test-results/smoke/
          retention-days: 30

  # ===================================
  # STAGE 7: PRODUCTION DEPLOYMENT
  # ===================================
  production-deployment:
    name: "Stage 7: Blue-Green Production Deployment"
    runs-on: ubuntu-22.04
    needs: [code-quality, build-and-push, staging-deployment]
    if: needs.code-quality.outputs.should_deploy == 'true' && (github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/'))
    environment: production
    outputs:
      deployment_status: ${{ steps.deployment.outputs.status }}
      production_url: ${{ steps.deployment.outputs.url }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure Google Cloud authentication
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Get GKE credentials
        run: |
          gcloud container clusters get-credentials $PRODUCTION_CLUSTER \
            --zone us-central1-a --project poised-elf-466913-q2

      - name: Blue-Green Deployment
        id: deployment
        run: |
          VERSION=${{ needs.code-quality.outputs.version }}
          
          # Determine current active environment
          CURRENT_COLOR=$(kubectl get service nephoran-production -o jsonpath='{.metadata.labels.color}' 2>/dev/null || echo "blue")
          NEW_COLOR=$([ "$CURRENT_COLOR" = "blue" ] && echo "green" || echo "blue")
          
          echo "Current active: $CURRENT_COLOR"
          echo "Deploying to: $NEW_COLOR"
          
          # Deploy to new color environment
          cd deployments/kustomize/overlays/production
          
          # Create color-specific kustomization
          cat > kustomization-$NEW_COLOR.yaml << EOF
          apiVersion: kustomize.config.k8s.io/v1beta1
          kind: Kustomization
          
          resources:
          - kustomization.yaml
          
          nameSuffix: -$NEW_COLOR
          
          images:
          - name: llm-processor
            newName: $REGISTRY/llm-processor
            newTag: $VERSION
          - name: nephio-bridge
            newName: $REGISTRY/nephio-bridge
            newTag: $VERSION
          - name: oran-adaptor
            newName: $REGISTRY/oran-adaptor
            newTag: $VERSION
          - name: rag-api
            newName: $REGISTRY/rag-api
            newTag: $VERSION
          
          patchesStrategicMerge:
          - |-
            apiVersion: v1
            kind: Service
            metadata:
              name: llm-processor
              labels:
                color: $NEW_COLOR
          EOF
          
          # Apply new color deployment
          kustomize build . -o kustomization-$NEW_COLOR.yaml | kubectl apply -f -
          
          # Wait for new deployment to be ready
          kubectl rollout status deployment/llm-processor-$NEW_COLOR --timeout=600s
          kubectl rollout status deployment/nephio-bridge-$NEW_COLOR --timeout=600s
          kubectl rollout status deployment/oran-adaptor-$NEW_COLOR --timeout=600s
          
          # Validation tests on new environment
          echo "Running production validation tests..."
          
          # Port forward to new environment for testing
          NEW_LLM_POD=$(kubectl get pod -l app=llm-processor-$NEW_COLOR -o jsonpath='{.items[0].metadata.name}')
          kubectl port-forward $NEW_LLM_POD 8081:8080 &
          PORT_FORWARD_PID=$!
          sleep 30
          
          # Validation tests
          VALIDATION_PASSED=true
          
          # Health check
          curl -f http://localhost:8081/health || VALIDATION_PASSED=false
          
          # Load test
          curl -L https://github.com/rakyll/hey/releases/latest/download/hey_linux_amd64 -o hey
          chmod +x hey
          ./hey -n 100 -c 10 http://localhost:8081/health > validation-results.txt
          
          # Check if error rate is acceptable
          ERROR_RATE=$(grep "Error distribution" validation-results.txt | wc -l)
          if [ "$ERROR_RATE" -gt 5 ]; then
            VALIDATION_PASSED=false
          fi
          
          kill $PORT_FORWARD_PID
          
          if [ "$VALIDATION_PASSED" = true ]; then
            # Switch traffic to new environment
            kubectl patch service nephoran-production -p '{"metadata":{"labels":{"color":"'$NEW_COLOR'"}}}'
            kubectl patch service nephoran-production -p '{"spec":{"selector":{"app":"llm-processor-'$NEW_COLOR'"}}}'
            
            echo "✅ Traffic switched to $NEW_COLOR environment"
            
            # Wait for traffic switch validation
            sleep 60
            
            # Clean up old environment
            echo "Cleaning up $CURRENT_COLOR environment..."
            kubectl delete deployment,service -l color=$CURRENT_COLOR || true
            
            PROD_URL=$(kubectl get service nephoran-production -o jsonpath='{.status.loadBalancer.ingress[0].ip}' || echo "cluster-internal")
            echo "status=success" >> $GITHUB_OUTPUT
            echo "url=https://$PROD_URL" >> $GITHUB_OUTPUT
          else
            echo "❌ Validation failed, rolling back"
            kubectl delete deployment,service -l color=$NEW_COLOR || true
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Production deployment summary
        run: |
          echo "✅ Production deployment successful"
          echo "Production URL: ${{ steps.deployment.outputs.url }}"
          kubectl get pods -o wide
          kubectl get services

  # ===================================
  # STAGE 8: RELEASE MANAGEMENT
  # ===================================
  release-management:
    name: "Stage 8: Release Management & Documentation"
    runs-on: ubuntu-22.04
    needs: [code-quality, production-deployment]
    if: needs.production-deployment.outputs.deployment_status == 'success' && startsWith(github.ref, 'refs/tags/')
    outputs:
      release_url: ${{ steps.release.outputs.url }}
      changelog_generated: ${{ steps.changelog.outputs.generated }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Generate changelog
        id: changelog
        run: |
          # Install changelog generator
          sudo apt-get update && sudo apt-get install -y git
          
          # Generate changelog
          PREVIOUS_TAG=$(git describe --tags --abbrev=0 HEAD^ 2>/dev/null || echo "")
          CURRENT_TAG=${{ needs.code-quality.outputs.version }}
          
          mkdir -p release-artifacts
          
          if [ -n "$PREVIOUS_TAG" ]; then
            echo "# Release $CURRENT_TAG" > release-artifacts/CHANGELOG.md
            echo "" >> release-artifacts/CHANGELOG.md
            echo "## Changes since $PREVIOUS_TAG" >> release-artifacts/CHANGELOG.md
            echo "" >> release-artifacts/CHANGELOG.md
            
            # Get commit messages
            git log --pretty=format:"- %s (%h)" $PREVIOUS_TAG..HEAD >> release-artifacts/CHANGELOG.md
          else
            echo "# Release $CURRENT_TAG" > release-artifacts/CHANGELOG.md
            echo "" >> release-artifacts/CHANGELOG.md
            echo "## Initial release" >> release-artifacts/CHANGELOG.md
            echo "" >> release-artifacts/CHANGELOG.md
            echo "- Initial version of Nephoran Intent Operator" >> release-artifacts/CHANGELOG.md
          fi
          
          echo "generated=true" >> $GITHUB_OUTPUT

      - name: Create GitHub release
        id: release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: ${{ needs.code-quality.outputs.version }}
          release_name: "Nephoran Intent Operator ${{ needs.code-quality.outputs.version }}"
          body_path: release-artifacts/CHANGELOG.md
          draft: false
          prerelease: false

      - name: Upload release artifacts
        run: |
          # Download build artifacts
          mkdir -p release-package
          
          # Create release package
          echo "Creating release package..."
          cp -r deployments/ release-package/
          cp -r docs/ release-package/
          cp CHANGELOG.md release-artifacts/CHANGELOG.md release-package/
          
          # Create deployment bundle
          tar -czf release-artifacts/nephoran-operator-${{ needs.code-quality.outputs.version }}.tar.gz \
            -C release-package .
          
          # Upload to release
          gh release upload ${{ needs.code-quality.outputs.version }} \
            release-artifacts/nephoran-operator-${{ needs.code-quality.outputs.version }}.tar.gz \
            --clobber
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Update documentation
        run: |
          # Update version in documentation
          sed -i 's/version: .*/version: ${{ needs.code-quality.outputs.version }}/' deployments/helm/nephoran-operator/Chart.yaml
          
          # Commit documentation updates
          git config --global user.name "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          git add deployments/helm/nephoran-operator/Chart.yaml
          git commit -m "chore: update version to ${{ needs.code-quality.outputs.version }}" || true
          git push origin HEAD:main || true

  # ===================================
  # STAGE 9: POST-DEPLOYMENT MONITORING
  # ===================================
  post-deployment-monitoring:
    name: "Stage 9: Enhanced Monitoring & Alerting"
    runs-on: ubuntu-22.04
    needs: [code-quality, production-deployment]
    if: needs.production-deployment.outputs.deployment_status == 'success'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure Google Cloud authentication
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Get GKE credentials
        run: |
          gcloud container clusters get-credentials $PRODUCTION_CLUSTER \
            --zone us-central1-a --project poised-elf-466913-q2

      - name: Enable enhanced monitoring
        run: |
          # Apply enhanced monitoring configuration
          kubectl apply -f deployments/monitoring/enhanced-alerting-configuration.yaml
          kubectl apply -f deployments/monitoring/production-grafana-dashboards.yaml
          
          # Create deployment-specific alerts
          cat > enhanced-monitoring-config.yaml << EOF
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: deployment-monitoring-config
            namespace: monitoring
          data:
            deployment-version: "${{ needs.code-quality.outputs.version }}"
            deployment-timestamp: "$(date -Iseconds)"
            monitoring-duration: "$MONITORING_DURATION"
            alert-threshold-high: "0.01"  # 1% error rate
            alert-threshold-critical: "0.05"  # 5% error rate
          EOF
          
          kubectl apply -f enhanced-monitoring-config.yaml

      - name: Setup deployment tracking
        run: |
          # Create deployment tracking job
          cat > deployment-tracker.yaml << EOF
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: deployment-tracker-${{ github.run_number }}
            namespace: monitoring
          spec:
            template:
              spec:
                containers:
                - name: tracker
                  image: curlimages/curl:latest
                  command:
                  - /bin/sh
                  - -c
                  - |
                    echo "Tracking deployment ${{ needs.code-quality.outputs.version }} for $MONITORING_DURATION"
                    
                    # Send deployment notification to monitoring system
                    curl -X POST "https://api.github.com/repos/${{ github.repository }}/dispatches" \
                      -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
                      -H "Accept: application/vnd.github.everest-preview+json" \
                      -d '{
                        "event_type": "deployment-tracking",
                        "client_payload": {
                          "version": "${{ needs.code-quality.outputs.version }}",
                          "environment": "production",
                          "monitoring_duration": "${{ env.MONITORING_DURATION }}",
                          "deployment_url": "${{ needs.production-deployment.outputs.production_url }}"
                        }
                      }'
                    
                    # Schedule monitoring check after 24 hours
                    sleep 86400  # 24 hours
                    
                    echo "Enhanced monitoring period completed"
                restartPolicy: Never
            backoffLimit: 1
          EOF
          
          kubectl apply -f deployment-tracker.yaml

      - name: Configure alerting channels
        run: |
          # Configure Slack notifications for deployment monitoring
          kubectl create secret generic slack-webhook \
            --from-literal=url="${{ secrets.SLACK_WEBHOOK_URL }}" \
            --namespace=monitoring \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Configure PagerDuty integration
          kubectl create secret generic pagerduty-integration \
            --from-literal=key="${{ secrets.PAGERDUTY_INTEGRATION_KEY }}" \
            --namespace=monitoring \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Deployment monitoring summary
        run: |
          echo "✅ Enhanced monitoring enabled for 24 hours"
          echo "Monitoring dashboard: https://monitoring.nephoran.ai/dashboard"
          echo "Alert channels configured: Slack, PagerDuty, GitHub"
          echo "Deployment version: ${{ needs.code-quality.outputs.version }}"
          echo "Production URL: ${{ needs.production-deployment.outputs.production_url }}"
          
          # Store monitoring configuration
          kubectl get configmap deployment-monitoring-config -o yaml > monitoring-config.yaml

      - name: Store monitoring configuration
        uses: actions/upload-artifact@v4
        with:
          name: monitoring-configuration
          path: |
            enhanced-monitoring-config.yaml
            deployment-tracker.yaml
            monitoring-config.yaml
          retention-days: 30

# ===================================
# PIPELINE SUMMARY AND NOTIFICATIONS
# ===================================
  pipeline-summary:
    name: "Pipeline Summary & Notifications"
    runs-on: ubuntu-22.04
    needs: [code-quality, comprehensive-testing, performance-testing, security-scanning, 
            build-and-push, staging-deployment, production-deployment, release-management, 
            post-deployment-monitoring]
    if: always()
    steps:
      - name: Pipeline summary
        run: |
          echo "## 🚀 Nephoran Intent Operator Deployment Pipeline Summary"
          echo ""
          echo "**Version:** ${{ needs.code-quality.outputs.version || 'N/A' }}"
          echo "**Commit:** ${{ github.sha }}"
          echo "**Trigger:** ${{ github.event_name }}"
          echo ""
          echo "### Stage Results:"
          echo "- ✅ Code Quality: ${{ needs.code-quality.result }}"
          echo "- ✅ Testing: ${{ needs.comprehensive-testing.result }}"
          echo "- ✅ Performance: ${{ needs.performance-testing.result }}"
          echo "- ✅ Security: ${{ needs.security-scanning.result }}"
          echo "- ✅ Build & Push: ${{ needs.build-and-push.result }}"
          echo "- ✅ Staging: ${{ needs.staging-deployment.result }}"
          echo "- ✅ Production: ${{ needs.production-deployment.result }}"
          echo "- ✅ Release: ${{ needs.release-management.result }}"
          echo "- ✅ Monitoring: ${{ needs.post-deployment-monitoring.result }}"
          echo ""
          echo "**Production URL:** ${{ needs.production-deployment.outputs.production_url || 'N/A' }}"
          echo "**Coverage:** ${{ needs.comprehensive-testing.outputs.coverage_percentage || 'N/A' }}%"
          echo "**Security Score:** ${{ needs.security-scanning.outputs.security_score || 'N/A' }}/100"
          echo "**Performance Score:** ${{ needs.performance-testing.outputs.performance_score || 'N/A' }}/100"

      - name: Notify stakeholders
        if: always()
        run: |
          STATUS="${{ job.status }}"
          if [[ "${{ needs.production-deployment.result }}" == "success" ]]; then
            STATUS="✅ SUCCESS"
          elif [[ "${{ needs.production-deployment.result }}" == "failure" ]]; then
            STATUS="❌ FAILED"
          else
            STATUS="⚠️ PARTIAL"
          fi
          
          # Send notification via webhook
          curl -X POST "${{ secrets.WEBHOOK_URL }}" \
            -H "Content-Type: application/json" \
            -d '{
              "text": "Nephoran Deployment Pipeline: '"$STATUS"'",
              "attachments": [{
                "color": "good",
                "fields": [{
                  "title": "Version",
                  "value": "${{ needs.code-quality.outputs.version }}",
                  "short": true
                }, {
                  "title": "Environment",
                  "value": "Production",
                  "short": true
                }, {
                  "title": "URL",
                  "value": "${{ needs.production-deployment.outputs.production_url }}",
                  "short": false
                }]
              }]
            }' || echo "Notification webhook not configured"