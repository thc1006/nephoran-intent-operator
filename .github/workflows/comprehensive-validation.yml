name: Comprehensive Validation Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run comprehensive validation daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Test scope to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - functional
          - performance
          - security
          - production
      target_score:
        description: 'Target score (minimum points to pass)'
        required: false
        default: '90'
        type: string
      enable_load_testing:
        description: 'Enable load testing'
        required: false
        default: true
        type: boolean
      enable_chaos_testing:
        description: 'Enable chaos testing'
        required: false
        default: false
        type: boolean

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/nephoran-intent-operator
  GO_VERSION: "1.24"
  KUBEBUILDER_VERSION: "3.12.0"
  KUBERNETES_VERSION: "1.28.0"

jobs:
  # Pre-validation checks
  pre-validation:
    name: Pre-Validation Checks
    runs-on: ubuntu-latest
    outputs:
      should-run-full-suite: ${{ steps.check-changes.outputs.should-run }}
      test-scope: ${{ steps.set-scope.outputs.scope }}
      target-score: ${{ steps.set-target.outputs.score }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check for significant changes
        id: check-changes
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] || [ "${{ github.event_name }}" = "schedule" ]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" = "push" ] && [ "${{ github.ref }}" = "refs/heads/main" ]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" = "pull_request" ]; then
            # Check if critical files changed
            if git diff --name-only HEAD^ HEAD | grep -E '\.(go|yaml|yml)$' | grep -E '(pkg/|cmd/|api/|controllers/)'; then
              echo "should-run=true" >> $GITHUB_OUTPUT
            else
              echo "should-run=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "should-run=false" >> $GITHUB_OUTPUT
          fi

      - name: Set test scope
        id: set-scope
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "scope=${{ github.event.inputs.test_scope }}" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" = "schedule" ]; then
            echo "scope=all" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" = "push" ]; then
            echo "scope=functional" >> $GITHUB_OUTPUT
          else
            echo "scope=functional" >> $GITHUB_OUTPUT
          fi

      - name: Set target score
        id: set-target
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "score=${{ github.event.inputs.target_score }}" >> $GITHUB_OUTPUT
          else
            echo "score=90" >> $GITHUB_OUTPUT
          fi

  # Build and prepare test environment
  build-and-prepare:
    name: Build and Prepare Test Environment
    runs-on: ubuntu-latest
    needs: pre-validation
    if: needs.pre-validation.outputs.should-run-full-suite == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: Cache Go modules
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/go-build
            ~/go/pkg/mod
          key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-go-

      - name: Install dependencies
        run: |
          go mod download
          go install github.com/onsi/ginkgo/v2/ginkgo@latest
          go install sigs.k8s.io/controller-runtime/tools/setup-envtest@latest

      - name: Setup test environment
        run: |
          setup-envtest use ${{ env.KUBERNETES_VERSION }}
          echo "KUBEBUILDER_ASSETS=$(setup-envtest use -p path ${{ env.KUBERNETES_VERSION }})" >> $GITHUB_ENV

      - name: Build operator
        run: |
          make build

      - name: Build test images
        run: |
          make docker-build IMG=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:test-${{ github.sha }}

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-artifacts
          path: |
            bin/
            Dockerfile*
          retention-days: 1

  # Functional Completeness Testing (50 points)
  functional-validation:
    name: Functional Completeness Validation
    runs-on: ubuntu-latest
    needs: [pre-validation, build-and-prepare]
    if: |
      needs.pre-validation.outputs.should-run-full-suite == 'true' && 
      (needs.pre-validation.outputs.test-scope == 'all' || 
       needs.pre-validation.outputs.test-scope == 'functional')
    strategy:
      matrix:
        test-category: [
          "intent-processing",
          "llm-rag-integration", 
          "porch-integration",
          "multi-cluster",
          "oran-compliance"
        ]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: build-artifacts

      - name: Setup test environment
        run: |
          setup-envtest use ${{ env.KUBERNETES_VERSION }}
          echo "KUBEBUILDER_ASSETS=$(setup-envtest use -p path ${{ env.KUBERNETES_VERSION }})" >> $GITHUB_ENV

      - name: Run functional tests
        run: |
          ginkgo run \
            --race \
            --trace \
            --cover \
            --coverprofile=functional-${{ matrix.test-category }}-coverage.out \
            --output-dir=test-results \
            --junit-report=functional-${{ matrix.test-category }}-junit.xml \
            --json-report=functional-${{ matrix.test-category }}-results.json \
            --focus="${{ matrix.test-category }}" \
            ./tests/validation/...

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: functional-test-results-${{ matrix.test-category }}
          path: |
            test-results/
            functional-*.out
            functional-*.xml
            functional-*.json
          retention-days: 30

  # Performance Benchmarking (25 points)
  performance-validation:
    name: Performance Benchmarking
    runs-on: ubuntu-latest
    needs: [pre-validation, build-and-prepare]
    if: |
      needs.pre-validation.outputs.should-run-full-suite == 'true' && 
      (needs.pre-validation.outputs.test-scope == 'all' || 
       needs.pre-validation.outputs.test-scope == 'performance')
    strategy:
      matrix:
        test-type: [
          "latency-benchmarks",
          "throughput-benchmarks", 
          "scalability-tests",
          "resource-efficiency"
        ]
        include:
          - test-type: "latency-benchmarks"
            concurrency: 10
            duration: "2m"
          - test-type: "throughput-benchmarks"
            concurrency: 50
            duration: "5m"
          - test-type: "scalability-tests"
            concurrency: 200
            duration: "3m"
          - test-type: "resource-efficiency"
            concurrency: 25
            duration: "2m"
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: Setup test environment
        run: |
          setup-envtest use ${{ env.KUBERNETES_VERSION }}
          echo "KUBEBUILDER_ASSETS=$(setup-envtest use -p path ${{ env.KUBERNETES_VERSION }})" >> $GITHUB_ENV

      - name: Run performance benchmarks
        env:
          LOAD_TEST_ENABLED: ${{ github.event.inputs.enable_load_testing || 'true' }}
          MAX_CONCURRENCY: ${{ matrix.concurrency }}
          TEST_DURATION: ${{ matrix.duration }}
        run: |
          ginkgo run \
            --race \
            --timeout=30m \
            --slow-spec-threshold=60s \
            --output-dir=test-results \
            --junit-report=performance-${{ matrix.test-type }}-junit.xml \
            --json-report=performance-${{ matrix.test-type }}-results.json \
            --focus="${{ matrix.test-type }}" \
            ./tests/validation/...

      - name: Generate performance report
        run: |
          go run ./tests/scripts/generate-performance-report.go \
            -input=test-results/performance-${{ matrix.test-type }}-results.json \
            -output=test-results/performance-${{ matrix.test-type }}-report.html \
            -type=${{ matrix.test-type }}

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results-${{ matrix.test-type }}
          path: |
            test-results/
          retention-days: 30

  # Security Compliance Testing (15 points)  
  security-validation:
    name: Security Compliance Validation
    runs-on: ubuntu-latest
    needs: [pre-validation, build-and-prepare]
    if: |
      needs.pre-validation.outputs.should-run-full-suite == 'true' && 
      (needs.pre-validation.outputs.test-scope == 'all' || 
       needs.pre-validation.outputs.test-scope == 'security')
    strategy:
      matrix:
        security-category: [
          "authentication-authorization",
          "data-encryption",
          "network-security", 
          "vulnerability-scanning"
        ]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: Run container security scan
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:test-${{ github.sha }}
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Setup test environment
        run: |
          setup-envtest use ${{ env.KUBERNETES_VERSION }}
          echo "KUBEBUILDER_ASSETS=$(setup-envtest use -p path ${{ env.KUBERNETES_VERSION }})" >> $GITHUB_ENV

      - name: Run security validation tests
        run: |
          ginkgo run \
            --race \
            --output-dir=test-results \
            --junit-report=security-${{ matrix.security-category }}-junit.xml \
            --json-report=security-${{ matrix.security-category }}-results.json \
            --focus="${{ matrix.security-category }}" \
            ./tests/validation/...

      - name: Upload security test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-test-results-${{ matrix.security-category }}
          path: |
            test-results/
            trivy-results.sarif
          retention-days: 30

  # Production Readiness Testing (10 points)
  production-validation:
    name: Production Readiness Validation  
    runs-on: ubuntu-latest
    needs: [pre-validation, build-and-prepare]
    if: |
      needs.pre-validation.outputs.should-run-full-suite == 'true' && 
      (needs.pre-validation.outputs.test-scope == 'all' || 
       needs.pre-validation.outputs.test-scope == 'production')
    strategy:
      matrix:
        readiness-category: [
          "high-availability",
          "fault-tolerance",
          "monitoring-observability",
          "disaster-recovery"
        ]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go  
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: Setup Kubernetes cluster
        uses: helm/kind-action@v1.4.0
        with:
          version: v0.20.0
          kubernetes_version: v${{ env.KUBERNETES_VERSION }}
          cluster_name: validation-cluster

      - name: Deploy operator to test cluster
        run: |
          make deploy IMG=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:test-${{ github.sha }}
          kubectl wait --for=condition=available --timeout=300s deployment/nephoran-intent-operator-controller-manager -n nephoran-system

      - name: Run production readiness tests
        env:
          CHAOS_TEST_ENABLED: ${{ github.event.inputs.enable_chaos_testing || 'false' }}
        run: |
          ginkgo run \
            --race \
            --timeout=20m \
            --output-dir=test-results \
            --junit-report=production-${{ matrix.readiness-category }}-junit.xml \
            --json-report=production-${{ matrix.readiness-category }}-results.json \
            --focus="${{ matrix.readiness-category }}" \
            ./tests/validation/...

      - name: Collect cluster logs
        if: always()
        run: |
          kubectl logs -n nephoran-system deployment/nephoran-intent-operator-controller-manager > operator-logs.txt
          kubectl describe pods -n nephoran-system > pod-descriptions.txt
          kubectl get events --all-namespaces --sort-by='.lastTimestamp' > cluster-events.txt

      - name: Upload production test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: production-test-results-${{ matrix.readiness-category }}
          path: |
            test-results/
            operator-logs.txt
            pod-descriptions.txt
            cluster-events.txt
          retention-days: 30

  # Chaos Engineering Tests (Optional)
  chaos-validation:
    name: Chaos Engineering Validation
    runs-on: ubuntu-latest
    needs: [pre-validation, build-and-prepare]
    if: |
      needs.pre-validation.outputs.should-run-full-suite == 'true' && 
      github.event.inputs.enable_chaos_testing == 'true' &&
      (needs.pre-validation.outputs.test-scope == 'all' || 
       needs.pre-validation.outputs.test-scope == 'production')
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Kubernetes cluster with Chaos Mesh
        run: |
          # Setup kind cluster with chaos mesh
          ./scripts/setup-chaos-testing.sh

      - name: Run chaos engineering tests
        run: |
          ginkgo run \
            --timeout=30m \
            --output-dir=test-results \
            --junit-report=chaos-junit.xml \
            --json-report=chaos-results.json \
            ./tests/chaos/...

      - name: Upload chaos test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: chaos-test-results
          path: test-results/
          retention-days: 30

  # Aggregate Results and Generate Final Report
  aggregate-results:
    name: Aggregate Results and Score Validation
    runs-on: ubuntu-latest
    needs: [
      pre-validation,
      functional-validation,
      performance-validation, 
      security-validation,
      production-validation
    ]
    if: always() && needs.pre-validation.outputs.should-run-full-suite == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          path: all-test-results/

      - name: Aggregate and calculate final score
        id: calculate-score
        run: |
          go run ./tests/scripts/aggregate-results.go \
            -input-dir=all-test-results \
            -output=final-validation-report.json \
            -target-score=${{ needs.pre-validation.outputs.target-score }}
          
          # Extract final score for job outputs
          FINAL_SCORE=$(jq -r '.total_score' final-validation-report.json)
          echo "final-score=${FINAL_SCORE}" >> $GITHUB_OUTPUT
          
          TARGET_SCORE=${{ needs.pre-validation.outputs.target-score }}
          echo "target-score=${TARGET_SCORE}" >> $GITHUB_OUTPUT

      - name: Generate comprehensive HTML report
        run: |
          go run ./tests/scripts/generate-html-report.go \
            -input=final-validation-report.json \
            -output=comprehensive-validation-report.html \
            -template=./tests/templates/validation-report.html

      - name: Upload final validation report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-validation-report
          path: |
            final-validation-report.json
            comprehensive-validation-report.html
          retention-days: 90

      - name: Post validation summary
        uses: actions/github-script@v6
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            const report = JSON.parse(fs.readFileSync('final-validation-report.json', 'utf8'));
            
            const summary = `
            ## üöÄ Nephoran Intent Operator Validation Results
            
            **Final Score: ${report.total_score}/${report.max_possible_score} points**
            **Target Score: ${{ steps.calculate-score.outputs.target-score }} points**
            **Status: ${report.total_score >= ${{ steps.calculate-score.outputs.target-score }} ? '‚úÖ PASSED' : '‚ùå FAILED'}**
            
            ### Category Breakdown:
            - üéØ **Functional Completeness**: ${report.functional_score}/50 points (Target: 45)
            - ‚ö° **Performance Benchmarks**: ${report.performance_score}/25 points (Target: 23)
            - üîí **Security Compliance**: ${report.security_score}/15 points (Target: 14)
            - üè≠ **Production Readiness**: ${report.production_score}/10 points (Target: 8)
            
            ### Performance Metrics:
            - **P95 Latency**: ${report.p95_latency}
            - **Throughput**: ${report.throughput_achieved} intents/min
            - **Availability**: ${report.availability_achieved}%
            
            ### Execution Summary:
            - **Execution Time**: ${report.execution_time}
            - **Tests Executed**: ${report.tests_executed}
            - **Tests Passed**: ${report.tests_passed}
            - **Tests Failed**: ${report.tests_failed}
            
            [üìä View Detailed Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

      - name: Validate final score
        run: |
          FINAL_SCORE=${{ steps.calculate-score.outputs.final-score }}
          TARGET_SCORE=${{ steps.calculate-score.outputs.target-score }}
          
          echo "Final Score: ${FINAL_SCORE}/${TARGET_SCORE}"
          
          if [ ${FINAL_SCORE} -lt ${TARGET_SCORE} ]; then
            echo "‚ùå Validation FAILED: Score ${FINAL_SCORE} is below target ${TARGET_SCORE}"
            exit 1
          else
            echo "‚úÖ Validation PASSED: Score ${FINAL_SCORE} meets target ${TARGET_SCORE}"
          fi

  # Notification and cleanup
  notify-completion:
    name: Notify Completion
    runs-on: ubuntu-latest
    needs: [aggregate-results]
    if: always()
    steps:
      - name: Notify on success
        if: needs.aggregate-results.result == 'success'
        run: |
          echo "üéâ Comprehensive validation completed successfully!"
          echo "Score: ${{ needs.aggregate-results.outputs.final-score }}/100"

      - name: Notify on failure  
        if: needs.aggregate-results.result == 'failure'
        run: |
          echo "‚ùå Comprehensive validation failed!"
          echo "Score: ${{ needs.aggregate-results.outputs.final-score }}/100"
          echo "Target: ${{ needs.aggregate-results.outputs.target-score }}/100"

      # Add Slack/email notifications here if needed

# Quality gates for branch protection
# This workflow must pass for PRs to be merged to main branch