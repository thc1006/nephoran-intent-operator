name: "Performance Benchmarking & Regression Detection"

on:
  pull_request:
    branches: [main]
    paths:
      - 'pkg/**'
      - 'cmd/**'
      - 'deployments/**'
      - 'go.mod'
      - 'go.sum'
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      benchmark_duration:
        description: 'Benchmark duration (minutes)'
        required: false
        default: '5'
        type: string
      load_test_users:
        description: 'Load test concurrent users'
        required: false
        default: '200'
        type: string
      enable_regression_check:
        description: 'Enable regression detection'
        required: false
        default: true
        type: boolean

env:
  BENCHMARK_DURATION: ${{ github.event.inputs.benchmark_duration || '5' }}
  LOAD_TEST_USERS: ${{ github.event.inputs.load_test_users || '200' }}
  ENABLE_REGRESSION: ${{ github.event.inputs.enable_regression_check || 'true' }}
  PERFORMANCE_THRESHOLD_P95: "2.0"  # seconds
  THROUGHPUT_THRESHOLD: "45"        # intents per minute
  AVAILABILITY_THRESHOLD: "99.95"   # percentage
  RAG_LATENCY_THRESHOLD: "200"      # milliseconds
  CACHE_HIT_THRESHOLD: "87"         # percentage

jobs:
  performance-validation:
    name: "Performance Claims Validation"
    runs-on: ubuntu-latest
    timeout-minutes: 30
    outputs:
      performance-score: ${{ steps.benchmark.outputs.performance-score }}
      regression-detected: ${{ steps.regression.outputs.regression-detected }}
      benchmark-results: ${{ steps.benchmark.outputs.results-json }}
    
    steps:
      - name: "Checkout Repository"
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for regression analysis
          
      - name: "Setup Go"
        uses: actions/setup-go@v5
          id: setup-go
        with:
          go-version-file: go.mod
          cache: true
          

      - name: Print Go version
        run: |
              echo "Go version: $(go version)"
              echo "Go env:"
              go env

      - name: "Setup Performance Test Environment"
        id: setup
        run: |
          echo "Setting up performance testing infrastructure..."
          
          # Install dependencies
          sudo apt-get update
          sudo apt-get install -y jq curl wrk redis-server
          
          # Start Redis for caching tests
          sudo systemctl start redis-server
          
          # Install hey for load testing
          curl -L https://hey-release.s3.us-east-2.amazonaws.com/hey_linux_amd64 -o hey
          chmod +x hey
          sudo mv hey /usr/local/bin/
          
          # Setup test data directory
          mkdir -p test-results/performance
          mkdir -p test-results/benchmarks
          
          echo "setup-complete=true" >> $GITHUB_OUTPUT
          
      - name: "Build Performance Test Suite"
        run: |
          echo "Building performance test components..."
          
          # Build main application with optimizations
          CGO_ENABLED=0 GOOS=linux go build -ldflags="-s -w" -o bin/nephoran-operator ./cmd/nephoran-operator
          
          # Build performance test runners
          go build -ldflags="-s -w" -o bin/performance-test-runner ./test/performance/cmd/runner
          go build -ldflags="-s -w" -o bin/load-test-orchestrator ./test/performance/cmd/load-tester
          go build -ldflags="-s -w" -o bin/statistical-validator ./test/performance/cmd/validator
          go build -ldflags="-s -w" -o bin/regression-detector ./test/performance/cmd/regression
          
          # Verify builds
          ./bin/nephoran-operator --version || echo "Version check not implemented"
          ./bin/performance-test-runner --help || echo "Help not implemented"
          
      - name: "Start Application Services"
        id: services
        run: |
          echo "Starting Nephoran Intent Operator services..."
          
          # Start main application in background
          export NEPHORAN_CONFIG_PATH=./test/performance/config/test-config.yaml
          export PROMETHEUS_METRICS_PORT=8080
          export PPROF_ENABLED=true
          export LOG_LEVEL=info
          
          ./bin/nephoran-operator &
          APP_PID=$!
          echo "app-pid=$APP_PID" >> $GITHUB_OUTPUT
          
          # Start mock services for testing
          go run ./test/performance/mocks/llm-service --port 8091 &
          go run ./test/performance/mocks/rag-service --port 8092 &
          go run ./test/performance/mocks/weaviate --port 8093 &
          
          # Wait for services to be ready
          timeout 60s bash -c 'until curl -s http://localhost:8080/health; do sleep 2; done'
          timeout 30s bash -c 'until curl -s http://localhost:8091/health; do sleep 2; done'
          timeout 30s bash -c 'until curl -s http://localhost:8092/health; do sleep 2; done'
          timeout 30s bash -c 'until curl -s http://localhost:8093/health; do sleep 2; done'
          
          echo "services-ready=true" >> $GITHUB_OUTPUT
          
      - name: "Execute Performance Benchmarks"
        id: benchmark
        run: |
          echo "Executing comprehensive performance benchmarks..."
          
          # Run intent processing latency benchmarks
          ./bin/performance-test-runner \
            --test-type="intent-latency" \
            --duration="${BENCHMARK_DURATION}m" \
            --concurrent-users="50" \
            --output-format="json" \
            --results-file="test-results/benchmarks/intent-latency.json"
          
          # Run throughput benchmarks
          ./bin/performance-test-runner \
            --test-type="throughput" \
            --duration="${BENCHMARK_DURATION}m" \
            --target-rate="${THROUGHPUT_THRESHOLD}" \
            --results-file="test-results/benchmarks/throughput.json"
          
          # Run concurrent user capacity test
          ./bin/load-test-orchestrator \
            --max-users="${LOAD_TEST_USERS}" \
            --ramp-duration="2m" \
            --hold-duration="3m" \
            --results-file="test-results/benchmarks/load-test.json"
          
          # Run RAG performance benchmarks
          ./bin/performance-test-runner \
            --test-type="rag-latency" \
            --duration="2m" \
            --concurrent-users="20" \
            --results-file="test-results/benchmarks/rag-performance.json"
          
          # Run cache efficiency tests
          ./bin/performance-test-runner \
            --test-type="cache-efficiency" \
            --duration="2m" \
            --cache-warmup=true \
            --results-file="test-results/benchmarks/cache-performance.json"
          
          # Aggregate results and calculate performance score
          RESULTS=$(./bin/statistical-validator \
            --benchmark-dir="test-results/benchmarks" \
            --confidence-level="95" \
            --output-format="json")
          
          echo "Benchmark Results:"
          echo "$RESULTS" | jq '.'
          
          # Extract performance metrics
          P95_LATENCY=$(echo "$RESULTS" | jq -r '.metrics.intent_processing_p95_latency')
          THROUGHPUT=$(echo "$RESULTS" | jq -r '.metrics.throughput_per_minute')
          AVAILABILITY=$(echo "$RESULTS" | jq -r '.metrics.availability_percentage')
          RAG_LATENCY=$(echo "$RESULTS" | jq -r '.metrics.rag_p95_latency')
          CACHE_HIT_RATE=$(echo "$RESULTS" | jq -r '.metrics.cache_hit_rate_percentage')
          CONCURRENT_CAPACITY=$(echo "$RESULTS" | jq -r '.metrics.concurrent_user_capacity')
          
          # Calculate overall performance score (0-100%)
          SCORE=0
          
          # CLAIM 1: Intent Processing P95 Latency ‚â§ 2s
          if (( $(echo "$P95_LATENCY <= $PERFORMANCE_THRESHOLD_P95" | bc -l) )); then
            SCORE=$((SCORE + 17))  # 100/6 ‚âà 17%
          fi
          
          # CLAIM 2: Concurrent User Capacity ‚â• 200
          if (( $(echo "$CONCURRENT_CAPACITY >= $LOAD_TEST_USERS" | bc -l) )); then
            SCORE=$((SCORE + 17))
          fi
          
          # CLAIM 3: Throughput ‚â• 45 intents/min
          if (( $(echo "$THROUGHPUT >= $THROUGHPUT_THRESHOLD" | bc -l) )); then
            SCORE=$((SCORE + 17))
          fi
          
          # CLAIM 4: Service Availability ‚â• 99.95%
          if (( $(echo "$AVAILABILITY >= $AVAILABILITY_THRESHOLD" | bc -l) )); then
            SCORE=$((SCORE + 17))
          fi
          
          # CLAIM 5: RAG Retrieval P95 Latency ‚â§ 200ms
          if (( $(echo "$RAG_LATENCY <= $RAG_LATENCY_THRESHOLD" | bc -l) )); then
            SCORE=$((SCORE + 16))
          fi
          
          # CLAIM 6: Cache Hit Rate ‚â• 87%
          if (( $(echo "$CACHE_HIT_RATE >= $CACHE_HIT_THRESHOLD" | bc -l) )); then
            SCORE=$((SCORE + 16))
          fi
          
          echo "Performance Score: $SCORE%"
          echo "performance-score=$SCORE" >> $GITHUB_OUTPUT
          echo "results-json=$RESULTS" >> $GITHUB_OUTPUT
          
          # Create summary for PR comment
          cat > test-results/performance-summary.md << EOF
          ## üìä Performance Benchmarking Results
          
          **Overall Performance Score: ${SCORE}/100** $([ $SCORE -ge 90 ] && echo "‚úÖ" || echo "‚ùå")
          
          ### Performance Claims Validation
          
          | Claim | Target | Actual | Status |
          |-------|--------|--------|--------|
          | Intent Processing P95 Latency | ‚â§${PERFORMANCE_THRESHOLD_P95}s | ${P95_LATENCY}s | $([ $(echo "$P95_LATENCY <= $PERFORMANCE_THRESHOLD_P95" | bc -l) -eq 1 ] && echo "‚úÖ PASS" || echo "‚ùå FAIL") |
          | Concurrent User Capacity | ‚â•${LOAD_TEST_USERS} | ${CONCURRENT_CAPACITY} | $([ $(echo "$CONCURRENT_CAPACITY >= $LOAD_TEST_USERS" | bc -l) -eq 1 ] && echo "‚úÖ PASS" || echo "‚ùå FAIL") |
          | Throughput | ‚â•${THROUGHPUT_THRESHOLD}/min | ${THROUGHPUT}/min | $([ $(echo "$THROUGHPUT >= $THROUGHPUT_THRESHOLD" | bc -l) -eq 1 ] && echo "‚úÖ PASS" || echo "‚ùå FAIL") |
          | Service Availability | ‚â•${AVAILABILITY_THRESHOLD}% | ${AVAILABILITY}% | $([ $(echo "$AVAILABILITY >= $AVAILABILITY_THRESHOLD" | bc -l) -eq 1 ] && echo "‚úÖ PASS" || echo "‚ùå FAIL") |
          | RAG Retrieval P95 Latency | ‚â§${RAG_LATENCY_THRESHOLD}ms | ${RAG_LATENCY}ms | $([ $(echo "$RAG_LATENCY <= $RAG_LATENCY_THRESHOLD" | bc -l) -eq 1 ] && echo "‚úÖ PASS" || echo "‚ùå FAIL") |
          | Cache Hit Rate | ‚â•${CACHE_HIT_THRESHOLD}% | ${CACHE_HIT_RATE}% | $([ $(echo "$CACHE_HIT_RATE >= $CACHE_HIT_THRESHOLD" | bc -l) -eq 1 ] && echo "‚úÖ PASS" || echo "‚ùå FAIL") |
          
          ### Statistical Validation
          - **Confidence Level**: $(echo "$RESULTS" | jq -r '.statistical_analysis.confidence_level')%
          - **Sample Size**: $(echo "$RESULTS" | jq -r '.statistical_analysis.sample_size')
          - **P-Value**: $(echo "$RESULTS" | jq -r '.statistical_analysis.p_value')
          - **Effect Size (Cohen's d)**: $(echo "$RESULTS" | jq -r '.statistical_analysis.effect_size')
          
          EOF
          
      - name: "Perform Regression Analysis"
        id: regression
        if: ${{ github.event_name == 'pull_request' && env.ENABLE_REGRESSION == 'true' }}
        run: |
          echo "Performing performance regression analysis..."
          
          # Get baseline performance metrics from main branch
          git checkout origin/main -- test-results/ 2>/dev/null || echo "No baseline found"
          
          # Run regression detector
          REGRESSION_RESULT=$(./bin/regression-detector \
            --baseline-dir="test-results/benchmarks" \
            --current-results="${{ steps.benchmark.outputs.results-json }}" \
            --sensitivity="moderate" \
            --output-format="json")
          
          echo "Regression Analysis:"
          echo "$REGRESSION_RESULT" | jq '.'
          
          REGRESSION_DETECTED=$(echo "$REGRESSION_RESULT" | jq -r '.regression_detected')
          PERFORMANCE_CHANGE=$(echo "$REGRESSION_RESULT" | jq -r '.performance_change_percent')
          SIGNIFICANCE=$(echo "$REGRESSION_RESULT" | jq -r '.statistical_significance')
          
          echo "regression-detected=$REGRESSION_DETECTED" >> $GITHUB_OUTPUT
          echo "performance-change=$PERFORMANCE_CHANGE" >> $GITHUB_OUTPUT
          
          # Add regression analysis to summary
          cat >> test-results/performance-summary.md << EOF
          
          ### üîç Regression Analysis
          - **Regression Detected**: $([ "$REGRESSION_DETECTED" = "true" ] && echo "‚ùå YES" || echo "‚úÖ NO")
          - **Performance Change**: ${PERFORMANCE_CHANGE}%
          - **Statistical Significance**: ${SIGNIFICANCE}
          EOF
          
      - name: "Generate Performance Report"
        run: |
          echo "Generating comprehensive performance report..."
          
          # Create detailed HTML report
          ./bin/performance-test-runner \
            --generate-report \
            --results-dir="test-results/benchmarks" \
            --output-file="test-results/performance-report.html" \
            --include-graphs=true
          
          # Create artifacts summary
          cat > test-results/artifacts-summary.txt << EOF
          Performance Test Artifacts Generated:
          - performance-summary.md: Executive summary for PR comments
          - performance-report.html: Detailed performance report with charts
          - benchmarks/: Raw benchmark data in JSON format
          - metrics.json: Aggregated performance metrics
          EOF
          
      - name: "Upload Performance Artifacts"
        uses: actions/upload-artifact@v4
        with:
          name: performance-test-results-${{ github.run_number }}
          path: |
            test-results/
            !test-results/**/*.log
          retention-days: 30
          
      - name: "Stop Services"
        if: always()
        run: |
          echo "Stopping services..."
          if [ -n "${{ steps.services.outputs.app-pid }}" ]; then
            kill ${{ steps.services.outputs.app-pid }} || true
          fi
          pkill -f "performance/mocks" || true
          
  deployment-gate:
    name: "Performance-Based Deployment Gate"
    runs-on: ubuntu-latest
    needs: performance-validation
    if: always()
    
    steps:
      - name: "Evaluate Performance Gate"
        id: gate
        run: |
          PERFORMANCE_SCORE="${{ needs.performance-validation.outputs.performance-score }}"
          REGRESSION_DETECTED="${{ needs.performance-validation.outputs.regression-detected }}"
          
          echo "Performance Score: $PERFORMANCE_SCORE"
          echo "Regression Detected: $REGRESSION_DETECTED"
          
          # Deployment gate criteria
          MIN_PERFORMANCE_SCORE=90
          ALLOW_REGRESSION=false
          
          GATE_PASSED=true
          
          # Check minimum performance score
          if [ "$PERFORMANCE_SCORE" -lt "$MIN_PERFORMANCE_SCORE" ]; then
            echo "‚ùå Performance gate FAILED: Score $PERFORMANCE_SCORE < $MIN_PERFORMANCE_SCORE"
            GATE_PASSED=false
          fi
          
          # Check for regressions (only on PRs)
          if [ "${{ github.event_name }}" = "pull_request" ] && [ "$REGRESSION_DETECTED" = "true" ] && [ "$ALLOW_REGRESSION" = "false" ]; then
            echo "‚ùå Performance gate FAILED: Regression detected"
            GATE_PASSED=false
          fi
          
          if [ "$GATE_PASSED" = "true" ]; then
            echo "‚úÖ Performance gate PASSED"
            echo "gate-status=passed" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Performance gate FAILED"
            echo "gate-status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi
          
  pr-comment:
    name: "Update PR with Performance Results"
    runs-on: ubuntu-latest
    needs: [performance-validation, deployment-gate]
    if: ${{ github.event_name == 'pull_request' && always() }}
    
    steps:
      - name: "Download Performance Results"
        uses: actions/download-artifact@v4
        with:
          name: performance-test-results-${{ github.run_number }}
          
      - name: "Comment on PR"
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              const performanceSummary = fs.readFileSync('test-results/performance-summary.md', 'utf8');
              const gateStatus = '${{ needs.deployment-gate.outputs.gate-status || "failed" }}';
              
              const comment = `
              ${performanceSummary}
              
              ### üö¶ Deployment Gate Status: ${gateStatus === 'passed' ? '‚úÖ PASSED' : '‚ùå FAILED'}
              
              ${gateStatus === 'passed' 
                ? '**Ready for deployment** - All performance requirements met' 
                : '**Deployment blocked** - Performance requirements not met'}
              
              ---
              üìä [View detailed performance report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
              `;
              
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.error('Error reading performance summary:', error);
            }
            
  publish-metrics:
    name: "Publish Performance Metrics"
    runs-on: ubuntu-latest
    needs: performance-validation
    if: ${{ github.ref == 'refs/heads/main' }}
    
    steps:
      - name: "Download Performance Results"
        uses: actions/download-artifact@v4
        with:
          name: performance-test-results-${{ github.run_number }}
          
      - name: "Publish to Performance Database"
        run: |
          echo "Publishing performance metrics to monitoring system..."
          
          # This would typically send metrics to your monitoring system
          # Example: Prometheus Pushgateway, InfluxDB, etc.
          
          PERFORMANCE_SCORE="${{ needs.performance-validation.outputs.performance-score }}"
          TIMESTAMP=$(date -u +%s)
          
          # Create metrics in Prometheus format
          cat > performance-metrics.txt << EOF
          # HELP nephoran_benchmark_performance_score Overall performance score (0-100)
          # TYPE nephoran_benchmark_performance_score gauge
          nephoran_benchmark_performance_score{branch="${{ github.ref_name }}",commit="${{ github.sha }}"} ${PERFORMANCE_SCORE} ${TIMESTAMP}000
          
          # HELP nephoran_benchmark_execution_timestamp Benchmark execution timestamp
          # TYPE nephoran_benchmark_execution_timestamp gauge
          nephoran_benchmark_execution_timestamp{branch="${{ github.ref_name }}",commit="${{ github.sha }}"} ${TIMESTAMP} ${TIMESTAMP}000
          EOF
          
          # Push to Prometheus Pushgateway (if configured)
          if [ -n "${{ secrets.PUSHGATEWAY_URL }}" ]; then
            curl -X POST "${{ secrets.PUSHGATEWAY_URL }}/metrics/job/nephoran-ci-benchmarks/instance/${{ github.run_id }}" \
              --data-binary @performance-metrics.txt
          fi
          
          echo "Metrics published successfully"

  cleanup:
    name: "Cleanup Test Environment"
    runs-on: ubuntu-latest
    needs: [performance-validation, deployment-gate, pr-comment, publish-metrics]
    if: always()
    
    steps:
      - name: "Cleanup Resources"
        run: |
          echo "Cleaning up test environment resources..."
          # Stop any remaining processes
          sudo pkill -f "nephoran" || true
          sudo pkill -f "performance-test" || true
          
          # Clean up temporary files
          sudo rm -rf /tmp/nephoran-test-* || true
          
          echo "Cleanup completed"
