// Package validation provides performance benchmarking for O-RAN interfaces.

// This module implements comprehensive performance testing and benchmarking for A1, E2, O1, and O2 interfaces.

package test_validation

import (
	"context"
	"fmt"
	"sync"
	"sync/atomic"
	"time"

	"github.com/onsi/ginkgo/v2"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// ORANPerformanceBenchmarker provides comprehensive performance testing for O-RAN interfaces.

type ORANPerformanceBenchmarker struct {
	oranValidator *ORANInterfaceValidator

	testFactory *ORANTestFactory

	k8sClient client.Client

	// Performance metrics.

	benchmarkResults map[string]*ORANBenchmarkResult

	concurrencyLimits map[string]int

	// Test configuration.

	testDuration time.Duration

	warmupDuration time.Duration

	maxConcurrency int
}

// ORANBenchmarkResult contains performance benchmark results for an O-RAN interface.

// This type is specific to O-RAN interface performance testing and extends the base BenchmarkResult.

type ORANBenchmarkResult struct {
	*BenchmarkResult // Embed the base BenchmarkResult from comprehensive_validation_suite.go

	// O-RAN specific fields.

	InterfaceName string `json:"interfaceName"`

	TotalRequests int64 `json:"totalRequests"`

	SuccessfulRequests int64 `json:"successfulRequests"`

	FailedRequests int64 `json:"failedRequests"`

	AverageLatency time.Duration `json:"averageLatency"`

	MedianLatency time.Duration `json:"medianLatency"`

	P95Latency time.Duration `json:"p95Latency"`

	P99Latency time.Duration `json:"p99Latency"`

	MinLatency time.Duration `json:"minLatency"`

	MaxLatency time.Duration `json:"maxLatency"`

	ThroughputRPS float64 `json:"throughputRps"`

	ErrorRate float64 `json:"errorRate"`

	// Latency distribution.

	LatencyDistribution []time.Duration `json:"latencyDistribution"`

	// Resource utilization.

	MemoryUsageMB float64 `json:"memoryUsageMb"`

	CPUUsagePercent float64 `json:"cpuUsagePercent"`

	// Test metadata.

	TestDuration time.Duration `json:"testDuration"`

	ConcurrencyLevel int `json:"concurrencyLevel"`

	StartTime time.Time `json:"startTime"`

	EndTime time.Time `json:"endTime"`
}

// NewORANPerformanceBenchmarker creates a new performance benchmarker.

func NewORANPerformanceBenchmarker(validator *ORANInterfaceValidator, factory *ORANTestFactory) *ORANPerformanceBenchmarker {

	return &ORANPerformanceBenchmarker{

		oranValidator: validator,

		testFactory: factory,

		benchmarkResults: make(map[string]*ORANBenchmarkResult),

		concurrencyLimits: map[string]int{

			"A1": 50, // A1 interface can handle 50 concurrent operations

			"E2": 100, // E2 interface can handle 100 concurrent operations

			"O1": 25, // O1 interface more limited due to NETCONF overhead

			"O2": 10, // O2 interface limited due to cloud provisioning overhead

		},

		testDuration: 2 * time.Minute,

		warmupDuration: 30 * time.Second,

		maxConcurrency: 100,
	}

}

// SetK8sClient sets the Kubernetes client for benchmarking.

func (opb *ORANPerformanceBenchmarker) SetK8sClient(client client.Client) {

	opb.k8sClient = client

}

// RunComprehensivePerformanceBenchmarks runs benchmarks for all O-RAN interfaces.

func (opb *ORANPerformanceBenchmarker) RunComprehensivePerformanceBenchmarks(ctx context.Context) map[string]*ORANBenchmarkResult {

	ginkgo.By("Running Comprehensive O-RAN Performance Benchmarks")

	// Run benchmarks for each interface.

	interfaces := []string{"A1", "E2", "O1", "O2"}

	for _, interfaceName := range interfaces {

		ginkgo.By(fmt.Sprintf("Benchmarking %s Interface Performance", interfaceName))

		result := opb.runInterfaceBenchmark(ctx, interfaceName)

		opb.benchmarkResults[interfaceName] = result

		ginkgo.By(fmt.Sprintf("%s Interface Results: %.2f RPS, %.2fms avg latency, %.1f%% error rate",

			interfaceName, result.ThroughputRPS, float64(result.AverageLatency.Nanoseconds())/1e6, result.ErrorRate))

	}

	// Run load testing scenarios.

	opb.runLoadTestingScenarios(ctx)

	// Run concurrency testing.

	opb.runConcurrencyTesting(ctx)

	// Generate performance report.

	opb.generatePerformanceReport()

	return opb.benchmarkResults

}

// runInterfaceBenchmark runs performance benchmark for a specific interface.

func (opb *ORANPerformanceBenchmarker) runInterfaceBenchmark(ctx context.Context, interfaceName string) *ORANBenchmarkResult {

	result := &ORANBenchmarkResult{

		BenchmarkResult: &BenchmarkResult{

			Name: interfaceName,

			MetricValue: 0,

			MetricUnit: "rps",

			Threshold: 0,

			Passed: false,

			Score: 0,

			MaxScore: 100,
		},

		InterfaceName: interfaceName,

		StartTime: time.Now(),

		ConcurrencyLevel: opb.concurrencyLimits[interfaceName],

		LatencyDistribution: make([]time.Duration, 0, 10000),
	}

	// Warmup period.

	ginkgo.By(fmt.Sprintf("Warming up %s interface", interfaceName))

	opb.runWarmup(ctx, interfaceName)

	// Main benchmark.

	var totalRequests, successfulRequests, failedRequests int64

	var latencies []time.Duration

	var latencyMutex sync.Mutex

	concurrency := opb.concurrencyLimits[interfaceName]

	sem := make(chan struct{}, concurrency)

	var wg sync.WaitGroup

	benchmarkStart := time.Now()

	deadline := benchmarkStart.Add(opb.testDuration)

	// Worker goroutines.

	for range concurrency {

		wg.Add(1)

		go func() {

			defer wg.Done()

			for time.Now().Before(deadline) {

				sem <- struct{}{}

				// Execute interface operation.

				startTime := time.Now()

				success := opb.executeInterfaceOperation(ctx, interfaceName)

				latency := time.Since(startTime)

				// Record metrics.

				atomic.AddInt64(&totalRequests, 1)

				if success {

					atomic.AddInt64(&successfulRequests, 1)

				} else {

					atomic.AddInt64(&failedRequests, 1)

				}

				// Record latency.

				latencyMutex.Lock()

				latencies = append(latencies, latency)

				latencyMutex.Unlock()

				<-sem

				// Small delay to prevent overwhelming.

				time.Sleep(1 * time.Millisecond)

			}

		}()

	}

	wg.Wait()

	result.EndTime = time.Now()

	actualDuration := result.EndTime.Sub(result.StartTime)

	// Calculate metrics.

	result.TotalRequests = totalRequests

	result.SuccessfulRequests = successfulRequests

	result.FailedRequests = failedRequests

	result.TestDuration = actualDuration

	result.ThroughputRPS = float64(totalRequests) / actualDuration.Seconds()

	result.ErrorRate = float64(failedRequests) / float64(totalRequests) * 100

	// Calculate latency statistics.

	if len(latencies) > 0 {

		result.LatencyDistribution = latencies

		result.MinLatency, result.MaxLatency, result.AverageLatency = opb.calculateLatencyStats(latencies)

		result.MedianLatency = opb.calculatePercentile(latencies, 0.5)

		result.P95Latency = opb.calculatePercentile(latencies, 0.95)

		result.P99Latency = opb.calculatePercentile(latencies, 0.99)

	}

	// Update benchmark result based on performance.

	if result.ErrorRate > 5.0 {

		result.BenchmarkResult.Passed = false

	} else {

		result.BenchmarkResult.Passed = true

	}

	result.BenchmarkResult.MetricValue = result.ThroughputRPS

	result.BenchmarkResult.Score = int(100 - result.ErrorRate)

	return result

}

// runWarmup performs warmup operations for consistent benchmarking.

func (opb *ORANPerformanceBenchmarker) runWarmup(ctx context.Context, interfaceName string) {

	warmupDeadline := time.Now().Add(opb.warmupDuration)

	for time.Now().Before(warmupDeadline) {

		opb.executeInterfaceOperation(ctx, interfaceName)

		time.Sleep(10 * time.Millisecond)

	}

}

// executeInterfaceOperation executes a single operation for the specified interface.

func (opb *ORANPerformanceBenchmarker) executeInterfaceOperation(ctx context.Context, interfaceName string) bool {

	switch interfaceName {

	case "A1":

		return opb.executeA1Operation(ctx)

	case "E2":

		return opb.executeE2Operation(ctx)

	case "O1":

		return opb.executeO1Operation(ctx)

	case "O2":

		return opb.executeO2Operation(ctx)

	default:

		return false

	}

}

// executeA1Operation executes an A1 interface operation.

func (opb *ORANPerformanceBenchmarker) executeA1Operation(ctx context.Context) bool {

	// Create and manage A1 policy.

	policy := opb.testFactory.CreateA1Policy("traffic-steering")

	// Create policy.

	if err := opb.oranValidator.ricMockService.CreatePolicy(policy); err != nil {

		return false

	}

	// Read policy.

	_, err := opb.oranValidator.ricMockService.GetPolicy(policy.PolicyID)

	if err != nil {

		opb.oranValidator.ricMockService.DeletePolicy(policy.PolicyID)

		return false

	}

	// Update policy.

	policy.PolicyData["primaryPathWeight"] = 0.8

	if err := opb.oranValidator.ricMockService.UpdatePolicy(policy); err != nil {

		opb.oranValidator.ricMockService.DeletePolicy(policy.PolicyID)

		return false

	}

	// Delete policy.

	if err := opb.oranValidator.ricMockService.DeletePolicy(policy.PolicyID); err != nil {

		return false

	}

	return true

}

// executeE2Operation executes an E2 interface operation.

func (opb *ORANPerformanceBenchmarker) executeE2Operation(ctx context.Context) bool {

	// Register E2 node and create subscription.

	node := opb.testFactory.CreateE2Node("gnodeb")

	// Register node.

	if err := opb.oranValidator.e2MockService.RegisterNode(node); err != nil {

		return false

	}

	// Create subscription.

	subscription := opb.testFactory.CreateE2Subscription("KPM", node.NodeID)

	if err := opb.oranValidator.e2MockService.CreateSubscription(subscription); err != nil {

		opb.oranValidator.e2MockService.UnregisterNode(node.NodeID)

		return false

	}

	// Update subscription.

	subscription.EventTrigger["reportingPeriod"] = 2000

	if err := opb.oranValidator.e2MockService.UpdateSubscription(subscription); err != nil {

		opb.oranValidator.e2MockService.DeleteSubscription(subscription.SubscriptionID)

		opb.oranValidator.e2MockService.UnregisterNode(node.NodeID)

		return false

	}

	// Send heartbeat.

	if err := opb.oranValidator.e2MockService.SendHeartbeat(node.NodeID); err != nil {

		opb.oranValidator.e2MockService.DeleteSubscription(subscription.SubscriptionID)

		opb.oranValidator.e2MockService.UnregisterNode(node.NodeID)

		return false

	}

	// Cleanup.

	opb.oranValidator.e2MockService.DeleteSubscription(subscription.SubscriptionID)

	opb.oranValidator.e2MockService.UnregisterNode(node.NodeID)

	return true

}

// executeO1Operation executes an O1 interface operation.

func (opb *ORANPerformanceBenchmarker) executeO1Operation(ctx context.Context) bool {

	// Manage O1 configuration.

	element := opb.testFactory.CreateManagedElement("AMF")

	// Add managed element.

	if err := opb.oranValidator.smoMockService.AddManagedElement(element); err != nil {

		return false

	}

	// Apply configuration.

	config := opb.testFactory.CreateO1Configuration("FCAPS", element.ElementID)

	if err := opb.oranValidator.smoMockService.ApplyConfiguration(config); err != nil {

		opb.oranValidator.smoMockService.RemoveManagedElement(element.ElementID)

		return false

	}

	// Get configuration.

	_, err := opb.oranValidator.smoMockService.GetConfiguration(config.ConfigID)

	if err != nil {

		opb.oranValidator.smoMockService.RemoveManagedElement(element.ElementID)

		return false

	}

	// Get managed element.

	_, err = opb.oranValidator.smoMockService.GetManagedElement(element.ElementID)

	if err != nil {

		opb.oranValidator.smoMockService.RemoveManagedElement(element.ElementID)

		return false

	}

	// Cleanup.

	opb.oranValidator.smoMockService.RemoveManagedElement(element.ElementID)

	return true

}

// executeO2Operation executes an O2 interface operation.

func (opb *ORANPerformanceBenchmarker) executeO2Operation(ctx context.Context) bool {

	// Simulate cloud infrastructure operations.

	// Validate Terraform template.

	terraformTemplate := map[string]interface{}{

		"terraform": map[string]interface{}{

			"required_providers": map[string]interface{}{

				"kubernetes": map[string]interface{}{

					"source": "hashicorp/kubernetes",

					"version": "~> 2.0",
				},
			},
		},

		"resource": map[string]interface{}{

			"kubernetes_deployment": map[string]interface{}{

				"test_deployment": map[string]interface{}{

					"metadata": map[string]interface{}{

						"name": "test-upf",
					},
				},
			},
		},
	}

	if !opb.oranValidator.ValidateTerraformTemplate(terraformTemplate) {

		return false

	}

	// Validate cloud provider config.

	cloudConfig := map[string]interface{}{

		"provider": "aws",

		"region": "us-west-2",

		"resources": map[string]interface{}{

			"ec2_instances": 2,

			"s3_buckets": 1,
		},
	}

	if !opb.oranValidator.ValidateCloudProviderConfig(cloudConfig) {

		return false

	}

	// Simulate resource lifecycle.

	time.Sleep(5 * time.Millisecond) // Simulate cloud API call

	return true

}

// runLoadTestingScenarios runs various load testing scenarios.

func (opb *ORANPerformanceBenchmarker) runLoadTestingScenarios(ctx context.Context) {

	ginkgo.By("Running Load Testing Scenarios")

	scenarios := []struct {
		name string

		concurrency int

		duration time.Duration

		interface_ string
	}{

		{"Low Load A1", 10, 1 * time.Minute, "A1"},

		{"Medium Load A1", 25, 1 * time.Minute, "A1"},

		{"High Load A1", 50, 1 * time.Minute, "A1"},

		{"Low Load E2", 20, 1 * time.Minute, "E2"},

		{"Medium Load E2", 50, 1 * time.Minute, "E2"},

		{"High Load E2", 100, 1 * time.Minute, "E2"},

		{"Steady State O1", 15, 2 * time.Minute, "O1"},

		{"Peak Load O2", 5, 30 * time.Second, "O2"},
	}

	for _, scenario := range scenarios {

		ginkgo.By(fmt.Sprintf("Running %s scenario", scenario.name))

		// Adjust concurrency for scenario.

		originalLimit := opb.concurrencyLimits[scenario.interface_]

		opb.concurrencyLimits[scenario.interface_] = scenario.concurrency

		originalDuration := opb.testDuration

		opb.testDuration = scenario.duration

		result := opb.runInterfaceBenchmark(ctx, scenario.interface_)

		opb.benchmarkResults[scenario.name] = result

		// Restore original settings.

		opb.concurrencyLimits[scenario.interface_] = originalLimit

		opb.testDuration = originalDuration

		ginkgo.By(fmt.Sprintf("%s completed: %.2f RPS, %.2fms latency",

			scenario.name, result.ThroughputRPS, float64(result.AverageLatency.Nanoseconds())/1e6))

	}

}

// runConcurrencyTesting tests different concurrency levels.

func (opb *ORANPerformanceBenchmarker) runConcurrencyTesting(ctx context.Context) {

	ginkgo.By("Running Concurrency Testing")

	testInterface := "E2" // Use E2 as it has good concurrency characteristics

	originalLimit := opb.concurrencyLimits[testInterface]

	originalDuration := opb.testDuration

	opb.testDuration = 30 * time.Second

	concurrencyLevels := []int{1, 5, 10, 25, 50, 75, 100}

	for _, concurrency := range concurrencyLevels {

		ginkgo.By(fmt.Sprintf("Testing concurrency level: %d", concurrency))

		opb.concurrencyLimits[testInterface] = concurrency

		result := opb.runInterfaceBenchmark(ctx, testInterface)

		scenarioName := fmt.Sprintf("Concurrency-%d", concurrency)

		opb.benchmarkResults[scenarioName] = result

		ginkgo.By(fmt.Sprintf("Concurrency %d: %.2f RPS, %.2fms latency",

			concurrency, result.ThroughputRPS, float64(result.AverageLatency.Nanoseconds())/1e6))

	}

	// Restore original settings.

	opb.concurrencyLimits[testInterface] = originalLimit

	opb.testDuration = originalDuration

}

// calculateLatencyStats calculates basic latency statistics.

func (opb *ORANPerformanceBenchmarker) calculateLatencyStats(latencies []time.Duration) (minLatency, maxLatency, avg time.Duration) {

	if len(latencies) == 0 {

		return

	}

	minLatency = latencies[0]

	maxLatency = latencies[0]

	var total int64

	for _, latency := range latencies {

		if latency < minLatency {

			minLatency = latency

		}

		if latency > maxLatency {

			maxLatency = latency

		}

		total += latency.Nanoseconds()

	}

	avg = time.Duration(total / int64(len(latencies)))

	return

}

// calculatePercentile calculates the specified percentile from latency data.

func (opb *ORANPerformanceBenchmarker) calculatePercentile(latencies []time.Duration, percentile float64) time.Duration {

	if len(latencies) == 0 {

		return 0

	}

	// Simple percentile calculation (would use sort in production).

	index := int(float64(len(latencies)) * percentile)

	if index >= len(latencies) {

		index = len(latencies) - 1

	}

	return latencies[index]

}

// generatePerformanceReport generates a comprehensive performance report.

func (opb *ORANPerformanceBenchmarker) generatePerformanceReport() {

	ginkgo.By("Generating Performance Report")

	ginkgo.By("=== O-RAN Interface Performance Benchmark Results ===")

	// Interface summary.

	interfaces := []string{"A1", "E2", "O1", "O2"}

	for _, interfaceName := range interfaces {

		if result, exists := opb.benchmarkResults[interfaceName]; exists {

			ginkgo.By(fmt.Sprintf("%s Interface Performance:", interfaceName))

			ginkgo.By(fmt.Sprintf("  Throughput: %.2f RPS", result.ThroughputRPS))

			ginkgo.By(fmt.Sprintf("  Average Latency: %.2fms", float64(result.AverageLatency.Nanoseconds())/1e6))

			ginkgo.By(fmt.Sprintf("  P95 Latency: %.2fms", float64(result.P95Latency.Nanoseconds())/1e6))

			ginkgo.By(fmt.Sprintf("  Error Rate: %.1f%%", result.ErrorRate))

			ginkgo.By(fmt.Sprintf("  Total Requests: %d", result.TotalRequests))

			ginkgo.By("")

		}

	}

	// Performance targets validation.

	opb.validatePerformanceTargets()

}

// validatePerformanceTargets validates that interfaces meet performance targets.

func (opb *ORANPerformanceBenchmarker) validatePerformanceTargets() {

	ginkgo.By("Validating Performance Targets")

	targets := map[string]struct {
		minThroughput float64

		maxLatency time.Duration

		maxErrorRate float64
	}{

		"A1": {minThroughput: 20.0, maxLatency: 100 * time.Millisecond, maxErrorRate: 2.0},

		"E2": {minThroughput: 50.0, maxLatency: 50 * time.Millisecond, maxErrorRate: 1.5},

		"O1": {minThroughput: 10.0, maxLatency: 200 * time.Millisecond, maxErrorRate: 1.0},

		"O2": {minThroughput: 2.0, maxLatency: 5 * time.Second, maxErrorRate: 3.0},
	}

	allTargetsMet := true

	for interfaceName, target := range targets {

		if result, exists := opb.benchmarkResults[interfaceName]; exists {

			targetsMet := true

			if result.ThroughputRPS < target.minThroughput {

				ginkgo.By(fmt.Sprintf("âŒ %s Throughput: %.2f RPS < %.2f RPS (target)",

					interfaceName, result.ThroughputRPS, target.minThroughput))

				targetsMet = false

			} else {

				ginkgo.By(fmt.Sprintf("âœ… %s Throughput: %.2f RPS >= %.2f RPS (target)",

					interfaceName, result.ThroughputRPS, target.minThroughput))

			}

			if result.AverageLatency > target.maxLatency {

				ginkgo.By(fmt.Sprintf("âŒ %s Latency: %.2fms > %.2fms (target)",

					interfaceName, float64(result.AverageLatency.Nanoseconds())/1e6,

					float64(target.maxLatency.Nanoseconds())/1e6))

				targetsMet = false

			} else {

				ginkgo.By(fmt.Sprintf("âœ… %s Latency: %.2fms <= %.2fms (target)",

					interfaceName, float64(result.AverageLatency.Nanoseconds())/1e6,

					float64(target.maxLatency.Nanoseconds())/1e6))

			}

			if result.ErrorRate > target.maxErrorRate {

				ginkgo.By(fmt.Sprintf("âŒ %s Error Rate: %.1f%% > %.1f%% (target)",

					interfaceName, result.ErrorRate, target.maxErrorRate))

				targetsMet = false

			} else {

				ginkgo.By(fmt.Sprintf("âœ… %s Error Rate: %.1f%% <= %.1f%% (target)",

					interfaceName, result.ErrorRate, target.maxErrorRate))

			}

			if !targetsMet {

				allTargetsMet = false

			}

		}

	}

	if allTargetsMet {

		ginkgo.By("ðŸŽ¯ All Performance Targets Met!")

	} else {

		ginkgo.By("âš ï¸  Some Performance Targets Not Met")

	}

}

// GetBenchmarkResults returns all benchmark results.

func (opb *ORANPerformanceBenchmarker) GetBenchmarkResults() map[string]*BenchmarkResult {

	results := make(map[string]*BenchmarkResult)

	for k, v := range opb.benchmarkResults {

		if v != nil && v.BenchmarkResult != nil {

			results[k] = v.BenchmarkResult

		}

	}

	return results

}

// GetInterfaceBenchmark returns benchmark result for a specific interface.

func (opb *ORANPerformanceBenchmarker) GetInterfaceBenchmark(interfaceName string) *BenchmarkResult {

	if result, ok := opb.benchmarkResults[interfaceName]; ok && result != nil {

		return result.BenchmarkResult

	}

	return nil

}
