# ğŸš€ Ollama å¿«é€Ÿå•Ÿå‹•æŒ‡å—

æœ¬åœ° LLM éƒ¨ç½² - 5 åˆ†é˜å¿«é€Ÿä¸Šæ‰‹

## æ–¹æ³• 1: è‡ªå‹•åŒ–è¨­å®šï¼ˆæœ€ç°¡å–®ï¼‰â­

```bash
# é‹è¡Œè‡ªå‹•åŒ–è¨­å®šè…³æœ¬
./scripts/setup-ollama.sh

# æŒ‰ç…§æç¤ºé¸æ“‡ï¼š
# 1. å®‰è£ Ollama (å¦‚æœæœªå®‰è£)
# 2. é¸æ“‡æ¨¡å‹ (æ¨è–¦: llama2:7b)
# 3. æ¸¬è©¦æ¨¡å‹
# 4. ç”Ÿæˆé…ç½®æª”æ¡ˆ
```

è…³æœ¬æœƒè‡ªå‹•ï¼š
- âœ… å®‰è£ Ollama
- âœ… ä¸‹è¼‰æ‚¨é¸æ“‡çš„æ¨¡å‹
- âœ… æ¸¬è©¦æ¨¡å‹åŠŸèƒ½
- âœ… å‰µå»º `.env` é…ç½®æª”æ¡ˆ

---

## æ–¹æ³• 2: Docker Composeï¼ˆæ¨è–¦ç”Ÿç”¢ç’°å¢ƒï¼‰

```bash
# 1. å•Ÿå‹•æ‰€æœ‰æœå‹™ï¼ˆOllama + Weaviate + RAGï¼‰
docker-compose -f docker-compose.ollama.yml up -d

# 2. ä¸‹è¼‰æ¨¡å‹
docker exec -it nephoran-ollama ollama pull llama2:7b

# 3. é©—è­‰æœå‹™
docker-compose -f docker-compose.ollama.yml ps
curl http://localhost:8000/health
curl http://localhost:8000/stats | jq '.config'

# 4. æ¸¬è©¦æ„åœ–è™•ç†
curl -X POST http://localhost:8000/process \
  -H "Content-Type: application/json" \
  -d '{"intent": "Deploy AMF with 3 replicas in namespace 5g-core"}'
```

### æŸ¥çœ‹æ—¥èªŒ
```bash
# RAG æœå‹™æ—¥èªŒ
docker-compose -f docker-compose.ollama.yml logs -f rag-service

# Ollama æ—¥èªŒ
docker-compose -f docker-compose.ollama.yml logs -f ollama
```

### åœæ­¢æœå‹™
```bash
docker-compose -f docker-compose.ollama.yml down
```

---

## æ–¹æ³• 3: æ‰‹å‹•æœ¬åœ°é‹è¡Œ

### Step 1: å®‰è£ Ollama

```bash
# Linux / macOS
curl -fsSL https://ollama.com/install.sh | sh

# é©—è­‰å®‰è£
ollama --version
```

### Step 2: ä¸‹è¼‰æ¨¡å‹

```bash
# æ¨è–¦æ¨¡å‹ï¼ˆé¸ä¸€å€‹ï¼‰
ollama pull llama2:7b    # å¿«é€Ÿï¼Œ4GB
ollama pull mistral:7b   # é«˜å“è³ªï¼Œ4GB
ollama pull llama2:13b   # ç”Ÿç”¢ç´šï¼Œ8GB

# é©—è­‰æ¨¡å‹
ollama list
```

### Step 3: å•Ÿå‹• Ollama æœå‹™

```bash
# åœ¨å¾Œå°é‹è¡Œ
ollama serve &

# é©—è­‰æœå‹™
curl http://localhost:11434/api/tags
```

### Step 4: é…ç½® RAG æœå‹™

```bash
# è¤‡è£½ç¯„ä¾‹é…ç½®
cp .env.ollama.example .env

# ç·¨è¼¯ .envï¼ˆæˆ–ç›´æ¥è¨­å®šç’°å¢ƒè®Šæ•¸ï¼‰
export LLM_PROVIDER=ollama
export LLM_MODEL=llama2:7b
export OLLAMA_BASE_URL=http://localhost:11434
export WEAVIATE_URL=http://localhost:8080  # éœ€è¦å…ˆå•Ÿå‹• Weaviate
```

### Step 5: å•Ÿå‹• Weaviateï¼ˆå¦‚æœå°šæœªé‹è¡Œï¼‰

```bash
docker run -d \
  --name weaviate \
  -p 8080:8080 \
  -p 50051:50051 \
  -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true \
  -e PERSISTENCE_DATA_PATH=/var/lib/weaviate \
  -e DEFAULT_VECTORIZER_MODULE=none \
  semitechnologies/weaviate:1.24.5
```

### Step 6: å•Ÿå‹• RAG æœå‹™

```bash
cd rag-python

# å®‰è£ä¾è³´ï¼ˆå¦‚æœå°šæœªå®‰è£ï¼‰
pip install -r requirements.txt

# å•Ÿå‹•æœå‹™
uvicorn api:app --reload --port 8000
```

### Step 7: æ¸¬è©¦

```bash
# å¥åº·æª¢æŸ¥
curl http://localhost:8000/health

# æŸ¥çœ‹é…ç½®
curl http://localhost:8000/stats | jq '.config'
# æ‡‰è©²çœ‹åˆ°: "llm_provider": "ollama"

# Swagger UI
open http://localhost:8000/docs

# æ¸¬è©¦æ„åœ–è™•ç†
curl -X POST http://localhost:8000/process \
  -H "Content-Type: application/json" \
  -d '{
    "intent": "Deploy AMF with 3 replicas in namespace 5g-core"
  }' | jq
```

---

## ğŸ¯ å¿«é€Ÿæ¸¬è©¦å‘½ä»¤

### æ¸¬è©¦ä¸åŒçš„æ„åœ–

```bash
# 1. éƒ¨ç½²æ„åœ–
curl -X POST http://localhost:8000/process \
  -H "Content-Type: application/json" \
  -d '{"intent": "Deploy AMF with 3 replicas"}' | jq

# 2. æ“´å±•æ„åœ–
curl -X POST http://localhost:8000/process \
  -H "Content-Type: application/json" \
  -d '{"intent": "Scale UPF to 5 replicas"}' | jq

# 3. è¤‡é›œæ„åœ–
curl -X POST http://localhost:8000/process \
  -H "Content-Type: application/json" \
  -d '{"intent": "Deploy SMF with high availability and 4 replicas in namespace 5g-core"}' | jq
```

---

## ğŸ”§ åˆ‡æ›æ¨¡å‹

### å‹•æ…‹åˆ‡æ›ï¼ˆä¸éœ€é‡å•Ÿï¼‰

```bash
# æ–¹æ³• 1: ç’°å¢ƒè®Šæ•¸ï¼ˆæ¨è–¦ï¼‰
export LLM_MODEL=mistral:7b
# é‡å•Ÿ RAG æœå‹™

# æ–¹æ³• 2: Docker
docker-compose -f docker-compose.ollama.yml down
# ç·¨è¼¯ docker-compose.ollama.yml ä¸­çš„ OLLAMA_MODEL
docker-compose -f docker-compose.ollama.yml up -d
```

### æ¨¡å‹å°æ¯”

| æ¨¡å‹ | é€Ÿåº¦ | å“è³ª | è¨˜æ†¶é«” | æ¨è–¦å ´æ™¯ |
|------|------|------|--------|---------|
| llama2:7b | â­â­â­â­â­ | â­â­â­ | 4GB | é–‹ç™¼/æ¸¬è©¦ |
| mistral:7b | â­â­â­â­ | â­â­â­â­â­ | 4GB | ç”Ÿç”¢ç’°å¢ƒ |
| llama2:13b | â­â­â­ | â­â­â­â­â­ | 8GB | é«˜å“è³ªéœ€æ±‚ |

---

## ğŸ“Š é©—è­‰æ•´åˆ

### 1. æª¢æŸ¥ Ollama ç‹€æ…‹

```bash
# æŸ¥çœ‹é‹è¡Œçš„æ¨¡å‹
ollama ps

# æŸ¥çœ‹å·²ä¸‹è¼‰çš„æ¨¡å‹
ollama list

# æ¸¬è©¦æ¨¡å‹å›æ‡‰
ollama run llama2:7b "Deploy AMF with 3 replicas. Output JSON only."
```

### 2. æª¢æŸ¥ RAG æœå‹™é…ç½®

```bash
# æŸ¥çœ‹é…ç½®ï¼ˆæ‡‰è©²é¡¯ç¤º ollama providerï¼‰
curl http://localhost:8000/stats | jq '.config'

# é æœŸè¼¸å‡º:
# {
#   "llm_provider": "ollama",
#   "llm_model": "llama2:7b",
#   "ollama_base_url": "http://localhost:11434",
#   ...
# }
```

### 3. ç«¯åˆ°ç«¯æ¸¬è©¦

```bash
# æ¸¬è©¦å®Œæ•´æµç¨‹
time curl -X POST http://localhost:8000/process \
  -H "Content-Type: application/json" \
  -d '{"intent": "Deploy AMF with 3 replicas in namespace 5g-core"}' | jq

# æª¢æŸ¥å›æ‡‰æ™‚é–“ï¼ˆæ‡‰è©²åœ¨ 3-5 ç§’å…§ï¼‰
# æª¢æŸ¥ JSON è¼¸å‡ºæ ¼å¼æ˜¯å¦æ­£ç¢º
```

---

## âš ï¸ å¸¸è¦‹å•é¡Œ

### Q1: Ollama é€£æ¥å¤±æ•—
```bash
# æª¢æŸ¥ Ollama æ˜¯å¦é‹è¡Œ
ps aux | grep ollama
curl http://localhost:11434/api/tags

# å¦‚æœæ²’é‹è¡Œï¼Œå•Ÿå‹•å®ƒ
ollama serve &
```

### Q2: æ¨¡å‹æœªæ‰¾åˆ°
```bash
# ä¸‹è¼‰ç¼ºå°‘çš„æ¨¡å‹
ollama pull llama2:7b

# é©—è­‰
ollama list
```

### Q3: JSON è¼¸å‡ºæ ¼å¼éŒ¯èª¤
```bash
# åˆ‡æ›åˆ°æ›´å¯é çš„æ¨¡å‹
export LLM_MODEL=mistral:7b
ollama pull mistral:7b
# é‡å•Ÿ RAG æœå‹™
```

### Q4: è¨˜æ†¶é«”ä¸è¶³
```bash
# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
export LLM_MODEL=llama2:7b  # è€Œé 13b
```

### Q5: å›æ‡‰é€Ÿåº¦æ…¢
```bash
# æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ GPU
OLLAMA_NUM_GPU=1 ollama serve

# æˆ–æ¸›å°‘ä¸¦ç™¼è«‹æ±‚
# åœ¨ api.py ä¸­è¨­å®š workers=1
```

---

## ğŸ“š é€²éšé…ç½®

è©³ç´°æ–‡æª”è«‹åƒè€ƒï¼š
- **å®Œæ•´æŒ‡å—**: `docs/OLLAMA_INTEGRATION.md`
- **Docker Compose**: `docker-compose.ollama.yml`
- **ç’°å¢ƒè®Šæ•¸**: `.env.ollama.example`
- **è¨­å®šè…³æœ¬**: `scripts/setup-ollama.sh`

---

## ğŸ¯ ä¸‹ä¸€æ­¥

1. **è‡ªå®šç¾©æ¨¡å‹**: å‰µå»ºé›»ä¿¡é ˜åŸŸå„ªåŒ–çš„ Modelfile
2. **ç”Ÿç”¢éƒ¨ç½²**: ä½¿ç”¨ Kubernetes éƒ¨ç½²ï¼ˆè¦‹æ–‡æª”ï¼‰
3. **æ•ˆèƒ½èª¿å„ª**: GPU åŠ é€Ÿå’Œä¸¦ç™¼é…ç½®
4. **ç›£æ§**: æ·»åŠ  Prometheus metrics

---

**éœ€è¦å¹«åŠ©ï¼Ÿ**
- æŸ¥çœ‹å®Œæ•´æ–‡æª”: `docs/OLLAMA_INTEGRATION.md`
- GitHub Issues: https://github.com/thc1006/nephoran-intent-operator/issues
- PR #344: https://github.com/thc1006/nephoran-intent-operator/pull/344

**æœ€å¾Œæ›´æ–°**: 2026-02-14
