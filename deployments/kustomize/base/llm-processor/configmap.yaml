apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-processor-config
  labels:
    app: llm-processor
    component: ai-processing
    version: v2.0.0
    app.kubernetes.io/name: llm-processor
    app.kubernetes.io/component: ai-processing
    app.kubernetes.io/part-of: nephoran-intent-operator
  annotations:
    nephoran.com/config-version: "v2.0.0"
    nephoran.com/last-updated: "2025-01-27"
data:
  # Enhanced API Configuration
  mistral_api_url: "http://mistral-inference.ai-models.svc.cluster.local:8080/v1/chat/completions"
  openai_api_url: "https://api.openai.com/v1/chat/completions"
  rag_api_url: "http://rag-api.default.svc.cluster.local:5001/process_intent"
  
  # Model Configuration
  models.yaml: |
    models:
      primary:
        name: "mistral-8x22b"
        endpoint: "http://mistral-inference.ai-models.svc.cluster.local:8080/v1/chat/completions"
        timeout_seconds: 60
        max_tokens: 2048
        temperature: 0.0
        
      fallback:
        name: "gpt-4o-mini"
        endpoint: "https://api.openai.com/v1/chat/completions"
        timeout_seconds: 30
        max_tokens: 2048
        temperature: 0.0
        
    retry_policy:
      max_retries: 3
      backoff_strategy: "exponential"
      base_delay_ms: 1000
      max_delay_ms: 30000
      
    circuit_breaker:
      failure_threshold: 5
      recovery_timeout: 60
      half_open_max_calls: 3

  # Telecom Knowledge Base Context
  telecom_context.yaml: |
    network_functions:
      5g_core:
        - name: "AMF"
          description: "Access and Mobility Management Function"
          interfaces: ["N1", "N2", "N8", "N11", "N12", "N14", "N15", "N22"]
          resources:
            cpu_min: "500m"
            cpu_max: "2000m"
            memory_min: "1Gi"
            memory_max: "4Gi"
        
        - name: "SMF"
          description: "Session Management Function"
          interfaces: ["N4", "N7", "N10", "N11", "N16"]
          resources:
            cpu_min: "500m"
            cpu_max: "2000m"
            memory_min: "1Gi"
            memory_max: "4Gi"
        
        - name: "UPF"
          description: "User Plane Function"
          interfaces: ["N3", "N4", "N6", "N9"]
          resources:
            cpu_min: "2000m"
            cpu_max: "8000m"
            memory_min: "4Gi"
            memory_max: "16Gi"
      
      oran:
        - name: "Near-RT-RIC"
          description: "Near Real-Time RAN Intelligent Controller"
          interfaces: ["A1", "E2", "O1"]
          resources:
            cpu_min: "1000m"
            cpu_max: "4000m"
            memory_min: "2Gi"
            memory_max: "8Gi"
        
        - name: "O-DU"
          description: "O-RAN Distributed Unit"
          interfaces: ["E2", "F1-U", "F1-C"]
          resources:
            cpu_min: "2000m"
            cpu_max: "8000m"
            memory_min: "4Gi"
            memory_max: "16Gi"

  # Prompt Templates
  prompt_templates.yaml: |
    system_prompts:
      deployment: |
        You are an expert telecommunications network engineer with deep knowledge of:
        - 5G Core Network Functions (AMF, SMF, UPF, NSSF, PCF, UDM, UDR, AUSF, NRF)
        - O-RAN architecture (O-DU, O-CU, Near-RT RIC, Non-RT RIC)
        - Network slicing and QoS management
        - Edge computing and MEC
        - Kubernetes and cloud-native network functions
        
        Translate natural language requests into NetworkFunctionDeployment JSON.
        
      scaling: |
        You are an expert in telecom network function scaling.
        Translate scaling requests into NetworkFunctionScale JSON.
        Consider horizontal vs vertical scaling patterns for different NF types.

  # Logging Configuration
  logging.yaml: |
    level: "info"
    format: "json"
    fields:
      service: "llm-processor"
      version: "v1.0.0"
    outputs:
      - type: "stdout"
      - type: "file"
        path: "/var/log/llm-processor.log"
        max_size: "100MB"
        max_backups: 3