# Ollama Integration Guide for Nephoran Intent Operator

æœ¬æ–‡æª”ä»‹ç´¹å¦‚ä½•å°‡ Ollama ä½œç‚ºæœ¬åœ° LLM provider æ•´åˆåˆ° Nephoran Intent Operator çš„ RAG ç®¡ç·šä¸­ã€‚

## ğŸ“‹ ç›®éŒ„

- [ç‚ºä»€éº¼ä½¿ç”¨ Ollamaï¼Ÿ](#ç‚ºä»€éº¼ä½¿ç”¨-ollama)
- [ç³»çµ±éœ€æ±‚](#ç³»çµ±éœ€æ±‚)
- [å®‰è£ Ollama](#å®‰è£-ollama)
- [é…ç½® RAG æœå‹™](#é…ç½®-rag-æœå‹™)
- [æ¸¬è©¦æ•´åˆ](#æ¸¬è©¦æ•´åˆ)
- [ç”Ÿç”¢éƒ¨ç½²](#ç”Ÿç”¢éƒ¨ç½²)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ç‚ºä»€éº¼ä½¿ç”¨ Ollamaï¼Ÿ

### å„ªå‹¢
- âœ… **å®Œå…¨æœ¬åœ°éƒ¨ç½²** - ç„¡éœ€ä¾è³´å¤–éƒ¨ API
- âœ… **æ•¸æ“šéš±ç§** - æ‰€æœ‰æ•¸æ“šä¿ç•™åœ¨æœ¬åœ°
- âœ… **ç„¡ API æˆæœ¬** - å…è²»ä½¿ç”¨
- âœ… **é›¢ç·šå·¥ä½œ** - ä¸éœ€è¦ç¶²è·¯é€£æ¥
- âœ… **ä½å»¶é²** - æœ¬åœ°æ¨ç†é€Ÿåº¦å¿«
- âœ… **å¤šæ¨¡å‹æ”¯æ´** - æ”¯æ´ Llama 2, Mistral, CodeLlama ç­‰

### é©ç”¨å ´æ™¯
- é–‹ç™¼å’Œæ¸¬è©¦ç’°å¢ƒ
- æ•¸æ“šæ•æ„Ÿçš„ç”Ÿç”¢ç’°å¢ƒ
- éœ€è¦é›¢ç·šé‹è¡Œçš„é‚Šç·£éƒ¨ç½²
- æˆæœ¬å„ªåŒ–çš„å ´æ™¯

---

## ç³»çµ±éœ€æ±‚

### ç¡¬é«”éœ€æ±‚

| çµ„ä»¶ | æœ€ä½é…ç½® | æ¨è–¦é…ç½® |
|------|---------|---------|
| CPU | 4 æ ¸å¿ƒ | 8+ æ ¸å¿ƒ |
| RAM | 8 GB | 16+ GB |
| ç£ç¢Ÿ | 10 GB | 20+ GB (å¤šæ¨¡å‹) |
| GPU | ç„¡ (å¯é¸) | NVIDIA GPU (åŠ é€Ÿæ¨ç†) |

### è»Ÿé«”éœ€æ±‚
- **ä½œæ¥­ç³»çµ±**: Linux (Ubuntu 20.04+), macOS, Windows (WSL2)
- **Docker**: 20.10+ (å¦‚æœä½¿ç”¨å®¹å™¨åŒ–éƒ¨ç½²)
- **Python**: 3.11+ (å·²åœ¨ requirements.txt ä¸­)

### æ¨¡å‹å¤§å°åƒè€ƒ

| æ¨¡å‹ | åƒæ•¸é‡ | è¨˜æ†¶é«”éœ€æ±‚ | æ¨ç†é€Ÿåº¦ | é©ç”¨å ´æ™¯ |
|------|--------|-----------|---------|---------|
| llama2:7b | 7B | ~4 GB | å¿« | é–‹ç™¼/æ¸¬è©¦ |
| llama2:13b | 13B | ~8 GB | ä¸­ç­‰ | ä¸€èˆ¬ç”Ÿç”¢ |
| mistral:7b | 7B | ~4 GB | å¿« | é«˜å“è³ªè¼¸å‡º |
| codellama:7b | 7B | ~4 GB | å¿« | ç¨‹å¼ç¢¼ç”Ÿæˆ |

---

## å®‰è£ Ollama

### æ–¹æ³• 1: Linux/macOS ä¸€éµå®‰è£ï¼ˆæ¨è–¦ï¼‰

```bash
# ä¸‹è¼‰ä¸¦å®‰è£ Ollama
curl -fsSL https://ollama.com/install.sh | sh

# é©—è­‰å®‰è£
ollama --version
```

### æ–¹æ³• 2: Docker å®‰è£

```bash
# æ‹‰å– Ollama Docker æ˜ åƒ
docker pull ollama/ollama

# é‹è¡Œ Ollama å®¹å™¨
docker run -d \
  --name ollama \
  -p 11434:11434 \
  -v ollama-data:/root/.ollama \
  ollama/ollama

# é©—è­‰é‹è¡Œ
docker logs ollama
```

### æ–¹æ³• 3: Windows (WSL2)

```powershell
# åœ¨ WSL2 Ubuntu ä¸­åŸ·è¡Œ
wsl

# ç„¶å¾Œé‹è¡Œ Linux å®‰è£è…³æœ¬
curl -fsSL https://ollama.com/install.sh | sh
```

---

## ä¸‹è¼‰å’Œé…ç½®æ¨¡å‹

### ä¸‹è¼‰æ¨è–¦æ¨¡å‹

```bash
# Llama 2 7B (æ¨è–¦å…¥é–€ä½¿ç”¨)
ollama pull llama2:7b

# Mistral 7B (æ›´å¥½çš„å“è³ª)
ollama pull mistral:7b

# CodeLlama (ç¨‹å¼ç¢¼å°ˆç”¨)
ollama pull codellama:7b

# æŸ¥çœ‹å·²ä¸‹è¼‰çš„æ¨¡å‹
ollama list
```

### æ¸¬è©¦æ¨¡å‹

```bash
# æ¸¬è©¦ Llama 2
ollama run llama2:7b "Deploy AMF with 3 replicas in namespace 5g-core. Output as JSON."

# æ¸¬è©¦ Mistral
ollama run mistral:7b "Generate a NetworkIntent JSON for scaling UPF to 5 replicas"
```

### è‡ªå®šç¾©æ¨¡å‹é…ç½®ï¼ˆå¯é¸ï¼‰

å‰µå»º `Modelfile` å„ªåŒ–æ¨¡å‹è¼¸å‡ºï¼š

```dockerfile
# custom-telecom-model.Modelfile
FROM llama2:7b

# è¨­å®šç³»çµ±æç¤ºè©
SYSTEM """
You are an expert telecommunications network engineer.
You translate natural language commands into structured JSON for O-RAN network functions.
Always output valid JSON without explanations.
"""

# åƒæ•¸å„ªåŒ–
PARAMETER temperature 0
PARAMETER top_p 0.9
PARAMETER num_predict 2048

# ç¯„ä¾‹å°è©±
MESSAGE user Deploy AMF with 3 replicas
MESSAGE assistant {"type": "NetworkFunctionDeployment", "name": "amf", "namespace": "5g-core", "spec": {"replicas": 3}}
```

å‰µå»ºè‡ªå®šç¾©æ¨¡å‹ï¼š

```bash
ollama create telecom-assistant -f custom-telecom-model.Modelfile
ollama run telecom-assistant "Scale UPF to 5 replicas"
```

---

## é…ç½® RAG æœå‹™

### é¸é … A: ç’°å¢ƒè®Šæ•¸é…ç½®ï¼ˆæ¨è–¦ï¼‰

å‰µå»º `.env` æª”æ¡ˆï¼š

```bash
# .env
# LLM Provider é…ç½®
LLM_PROVIDER=ollama
LLM_MODEL=llama2:7b  # æˆ– mistral:7b, codellama:7b, telecom-assistant
OLLAMA_BASE_URL=http://localhost:11434

# Weaviate é…ç½®
WEAVIATE_URL=http://localhost:8080

# å¿«å–é…ç½®
CACHE_MAX_SIZE=1000
CACHE_TTL_SECONDS=3600

# çŸ¥è­˜åº«è·¯å¾‘
KNOWLEDGE_BASE_PATH=/app/knowledge_base
```

### é¸é … B: Docker Compose é…ç½®

å‰µå»º `docker-compose.ollama.yml`ï¼š

```yaml
version: '3.8'

services:
  # Ollama æœå‹™
  ollama:
    image: ollama/ollama:latest
    container_name: nephoran-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    # å¦‚æœæœ‰ GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Weaviate å‘é‡è³‡æ–™åº«
  weaviate:
    image: semitechnologies/weaviate:1.24.5
    container_name: nephoran-weaviate
    ports:
      - "8080:8080"
      - "50051:50051"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: /var/lib/weaviate
      DEFAULT_VECTORIZER_MODULE: none
      ENABLE_MODULES: text2vec-openai
      CLUSTER_HOSTNAME: weaviate
    volumes:
      - weaviate-data:/var/lib/weaviate
    restart: unless-stopped

  # RAG æœå‹™
  rag-service:
    build:
      context: .
      dockerfile: rag-python/Dockerfile
    container_name: nephoran-rag-service
    ports:
      - "8000:8000"
    environment:
      LLM_PROVIDER: ollama
      LLM_MODEL: llama2:7b
      OLLAMA_BASE_URL: http://ollama:11434
      WEAVIATE_URL: http://weaviate:8080
      LOG_LEVEL: INFO
    volumes:
      - ./knowledge_base:/app/knowledge_base:ro
    depends_on:
      - ollama
      - weaviate
    restart: unless-stopped

volumes:
  ollama-data:
  weaviate-data:
```

å•Ÿå‹•æœå‹™ï¼š

```bash
# å•Ÿå‹•æ‰€æœ‰æœå‹™
docker-compose -f docker-compose.ollama.yml up -d

# åœ¨ Ollama å®¹å™¨ä¸­ä¸‹è¼‰æ¨¡å‹
docker exec -it nephoran-ollama ollama pull llama2:7b

# æŸ¥çœ‹æ—¥èªŒ
docker-compose -f docker-compose.ollama.yml logs -f rag-service
```

### é¸é … C: Kubernetes éƒ¨ç½²

æ›´æ–° `deployments/rag-service.yaml` çš„ ConfigMapï¼š

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: rag-config
  namespace: nephoran-rag
data:
  llm_provider: "ollama"
  llm_model: "llama2:7b"
  ollama_base_url: "http://ollama-service:11434"
  chunk_size: "1000"
  chunk_overlap: "200"
  embedding_model: "text-embedding-3-large"
  log_level: "INFO"
---
# Ollama Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: nephoran-rag
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      containers:
      - name: ollama
        image: ollama/ollama:latest
        ports:
        - containerPort: 11434
          name: http
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        volumeMounts:
        - name: ollama-data
          mountPath: /root/.ollama
      volumes:
      - name: ollama-data
        persistentVolumeClaim:
          claimName: ollama-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: ollama-service
  namespace: nephoran-rag
spec:
  selector:
    app: ollama
  ports:
  - protocol: TCP
    port: 11434
    targetPort: 11434
  type: ClusterIP
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-pvc
  namespace: nephoran-rag
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
```

éƒ¨ç½²åˆ° Kubernetesï¼š

```bash
# æ‡‰ç”¨é…ç½®
kubectl apply -f deployments/rag-service.yaml

# åœ¨ Ollama pod ä¸­ä¸‹è¼‰æ¨¡å‹
kubectl exec -it deployment/ollama -n nephoran-rag -- ollama pull llama2:7b

# é©—è­‰æœå‹™
kubectl get pods -n nephoran-rag
kubectl logs -f deployment/rag-service -n nephoran-rag
```

---

## æ¸¬è©¦æ•´åˆ

### æœ¬åœ°æ¸¬è©¦

```bash
# 1. å•Ÿå‹• Ollama (å¦‚æœå°šæœªé‹è¡Œ)
ollama serve &

# 2. ä¸‹è¼‰æ¨¡å‹
ollama pull llama2:7b

# 3. è¨­å®šç’°å¢ƒè®Šæ•¸
export LLM_PROVIDER=ollama
export LLM_MODEL=llama2:7b
export OLLAMA_BASE_URL=http://localhost:11434
export WEAVIATE_URL=http://localhost:8080

# 4. å•Ÿå‹• RAG æœå‹™
cd rag-python
uvicorn api:app --reload --port 8000

# 5. æ¸¬è©¦å¥åº·æª¢æŸ¥
curl http://localhost:8000/health

# 6. æŸ¥çœ‹é…ç½®
curl http://localhost:8000/stats | jq '.config'

# é æœŸè¼¸å‡º:
# {
#   "llm_provider": "ollama",
#   "llm_model": "llama2:7b",
#   "ollama_base_url": "http://localhost:11434",
#   ...
# }
```

### æ¸¬è©¦æ„åœ–è™•ç†

```bash
# æ¸¬è©¦éƒ¨ç½²æ„åœ–
curl -X POST http://localhost:8000/process \
  -H "Content-Type: application/json" \
  -d '{
    "intent": "Deploy AMF with 3 replicas in namespace 5g-core"
  }' | jq

# æ¸¬è©¦æ“´å±•æ„åœ–
curl -X POST http://localhost:8000/process \
  -H "Content-Type: application/json" \
  -d '{
    "intent": "Scale UPF to 5 replicas"
  }' | jq

# æ¸¬è©¦è¤‡é›œæ„åœ–
curl -X POST http://localhost:8000/process \
  -H "Content-Type: application/json" \
  -d '{
    "intent": "Deploy SMF with high availability configuration and 4 replicas"
  }' | jq
```

### æ•ˆèƒ½æ¸¬è©¦

```bash
# æ¸¬è©¦å›æ‡‰æ™‚é–“
time curl -X POST http://localhost:8000/process \
  -H "Content-Type: application/json" \
  -d '{"intent": "Deploy AMF with 3 replicas"}'

# è² è¼‰æ¸¬è©¦ (éœ€è¦å®‰è£ hey)
hey -n 100 -c 10 -m POST \
  -H "Content-Type: application/json" \
  -d '{"intent": "Deploy AMF with 3 replicas"}' \
  http://localhost:8000/process
```

---

## ç”Ÿç”¢éƒ¨ç½²æœ€ä½³å¯¦è¸

### 1. æ¨¡å‹é¸æ“‡å»ºè­°

| ç’°å¢ƒ | æ¨è–¦æ¨¡å‹ | ç†ç”± |
|------|---------|------|
| é–‹ç™¼ | llama2:7b | å¿«é€Ÿè¿­ä»£ |
| æ¸¬è©¦ | mistral:7b | æ›´å¥½çš„è¼¸å‡ºå“è³ª |
| ç”Ÿç”¢ | llama2:13b æˆ–è‡ªå®šç¾©å¾®èª¿ | å¹³è¡¡æ•ˆèƒ½å’Œå“è³ª |

### 2. è³‡æºé…ç½®

```yaml
# Kubernetes è³‡æºé…ç½®å»ºè­°
resources:
  requests:
    memory: "4Gi"   # 7B æ¨¡å‹
    cpu: "2000m"
  limits:
    memory: "8Gi"   # ç‚ºæ¨ç†é ç•™ç©ºé–“
    cpu: "4000m"
```

### 3. ç›£æ§æŒ‡æ¨™

ç›£æ§ä»¥ä¸‹æŒ‡æ¨™ï¼š
- **æ¨ç†å»¶é²**: P50, P95, P99
- **è¨˜æ†¶é«”ä½¿ç”¨**: Ollama å®¹å™¨è¨˜æ†¶é«”
- **CPU ä½¿ç”¨ç‡**: æ¨ç†æœŸé–“çš„ CPU è² è¼‰
- **æ¨¡å‹åŠ è¼‰æ™‚é–“**: å®¹å™¨å•Ÿå‹•åˆ°æ¨¡å‹å°±ç·’
- **éŒ¯èª¤ç‡**: å¤±æ•—çš„æ¨ç†è«‹æ±‚

### 4. é«˜å¯ç”¨æ€§é…ç½®

```yaml
# å¤šå‰¯æœ¬éƒ¨ç½²
spec:
  replicas: 3  # Ollama æœå‹™
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
```

### 5. æ¨¡å‹å¿«å–ç­–ç•¥

```bash
# é åŠ è¼‰æ¨¡å‹åˆ°å¿«å–
kubectl exec -it deployment/ollama -- sh -c "
  ollama pull llama2:7b
  ollama run llama2:7b 'Test prompt' --verbose
"
```

---

## æ•…éšœæ’é™¤

### å•é¡Œ 1: Ollama é€£æ¥å¤±æ•—

**ç—‡ç‹€**:
```
ConnectionError: Failed to connect to Ollama at http://localhost:11434
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# 1. æª¢æŸ¥ Ollama æ˜¯å¦é‹è¡Œ
ps aux | grep ollama
curl http://localhost:11434/api/tags

# 2. é‡å•Ÿ Ollama
pkill ollama
ollama serve &

# 3. æª¢æŸ¥é˜²ç«ç‰†
sudo ufw allow 11434/tcp

# 4. æª¢æŸ¥ Docker ç¶²è·¯ (å¦‚æœä½¿ç”¨ Docker)
docker network inspect bridge
```

### å•é¡Œ 2: æ¨¡å‹æœªæ‰¾åˆ°

**ç—‡ç‹€**:
```
Error: model 'llama2:7b' not found
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æŸ¥çœ‹å·²ä¸‹è¼‰çš„æ¨¡å‹
ollama list

# ä¸‹è¼‰ç¼ºå°‘çš„æ¨¡å‹
ollama pull llama2:7b

# é©—è­‰æ¨¡å‹
ollama show llama2:7b
```

### å•é¡Œ 3: JSON è¼¸å‡ºæ ¼å¼éŒ¯èª¤

**ç—‡ç‹€**: LLM è¼¸å‡ºä¸æ˜¯æœ‰æ•ˆçš„ JSON

**è§£æ±ºæ–¹æ¡ˆ**:

1. ä½¿ç”¨è‡ªå®šç¾© Modelfile åŠ å¼· JSON è¼¸å‡ºï¼š
   ```dockerfile
   FROM llama2:7b

   SYSTEM """
   You MUST output valid JSON only. No explanations.
   """

   PARAMETER temperature 0
   ```

2. åœ¨ Python ä»£ç¢¼ä¸­æ·»åŠ é‡è©¦é‚è¼¯ï¼ˆå·²åœ¨ä»£ç¢¼ä¸­å¯¦ç¾ï¼‰

3. ä½¿ç”¨ `mistral:7b` - å®ƒåœ¨çµæ§‹åŒ–è¼¸å‡ºæ–¹é¢é€šå¸¸æ›´å¯é 

### å•é¡Œ 4: è¨˜æ†¶é«”ä¸è¶³

**ç—‡ç‹€**:
```
OOM killed: ollama process
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# 1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama pull llama2:7b  # è€Œé 13b

# 2. å¢åŠ  swap ç©ºé–“ (Linux)
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 3. é™åˆ¶ä¸¦ç™¼è«‹æ±‚
# åœ¨ api.py ä¸­è¨­å®š workers=1
```

### å•é¡Œ 5: æ¨ç†é€Ÿåº¦æ…¢

**å„ªåŒ–ç­–ç•¥**:
```bash
# 1. ä½¿ç”¨ GPU åŠ é€Ÿ (å¦‚æœå¯ç”¨)
OLLAMA_NUM_GPU=1 ollama serve

# 2. èª¿æ•´æ¨¡å‹åƒæ•¸
# åœ¨ Modelfile ä¸­:
PARAMETER num_thread 4

# 3. é åŠ è¼‰æ¨¡å‹åˆ°è¨˜æ†¶é«”
ollama run llama2:7b "warmup"
```

---

## èˆ‡ OpenAI çš„æ•ˆèƒ½å°æ¯”

| æŒ‡æ¨™ | OpenAI (gpt-4o-mini) | Ollama (llama2:7b) | Ollama (mistral:7b) |
|------|---------------------|-------------------|-------------------|
| **å»¶é²** | ~1-2 ç§’ | ~3-5 ç§’ | ~3-5 ç§’ |
| **æˆæœ¬** | $0.15/1M tokens | å…è²» | å…è²» |
| **å“è³ª** | å„ªç§€ | è‰¯å¥½ | è‰¯å¥½-å„ªç§€ |
| **éš±ç§** | é›²ç«¯è™•ç† | å®Œå…¨æœ¬åœ° | å®Œå…¨æœ¬åœ° |
| **é›¢ç·š** | âŒ | âœ… | âœ… |
| **è‡ªå®šç¾©** | æœ‰é™ | å®Œå…¨å¯æ§ | å®Œå…¨å¯æ§ |

---

## é€²éšåŠŸèƒ½

### å¤šæ¨¡å‹è² è¼‰å‡è¡¡

éƒ¨ç½²å¤šå€‹æ¨¡å‹å¯¦ä¾‹ï¼š

```yaml
# ä½¿ç”¨ä¸åŒæ¨¡å‹çš„å¤šå€‹å‰¯æœ¬
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-llama2
spec:
  replicas: 2
  # ... llama2:7b é…ç½®
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-mistral
spec:
  replicas: 1
  # ... mistral:7b é…ç½®
```

### æ¨¡å‹å¾®èª¿ (Fine-tuning)

ä½¿ç”¨ Ollama çš„ Modelfile é€²è¡Œé ˜åŸŸç‰¹å®šå¾®èª¿ï¼š

```dockerfile
FROM llama2:7b

# æ·»åŠ å¤§é‡é›»ä¿¡é ˜åŸŸç¯„ä¾‹
MESSAGE user Deploy AMF with 5 replicas
MESSAGE assistant {"type": "NetworkFunctionDeployment", "name": "amf", "namespace": "5g-core", "spec": {"replicas": 5}}

MESSAGE user Scale UPF to 3 instances
MESSAGE assistant {"type": "NetworkFunctionScale", "name": "upf", "namespace": "5g-core", "replicas": 3}

# ... æ·»åŠ æ›´å¤šç¯„ä¾‹
```

---

## åƒè€ƒè³‡æº

- **Ollama å®˜æ–¹æ–‡æª”**: https://ollama.com/docs
- **Ollama GitHub**: https://github.com/ollama/ollama
- **LangChain Ollama æ•´åˆ**: https://python.langchain.com/docs/integrations/llms/ollama
- **æ¨¡å‹åº«**: https://ollama.com/library
- **Llama 2 è«–æ–‡**: https://arxiv.org/abs/2307.09288
- **Mistral æ–‡æª”**: https://docs.mistral.ai/

---

## é™„éŒ„: å¿«é€Ÿé…ç½®æª¢æŸ¥æ¸…å–®

- [ ] Ollama å·²å®‰è£ä¸¦é‹è¡Œ
- [ ] æ¨¡å‹å·²ä¸‹è¼‰ (`ollama list` æª¢æŸ¥)
- [ ] `LLM_PROVIDER=ollama` ç’°å¢ƒè®Šæ•¸å·²è¨­å®š
- [ ] `LLM_MODEL` è¨­å®šç‚ºæœ‰æ•ˆçš„æ¨¡å‹åç¨±
- [ ] `OLLAMA_BASE_URL` æ­£ç¢ºæŒ‡å‘ Ollama æœå‹™
- [ ] Weaviate å·²é‹è¡Œä¸¦å¯è¨ªå•
- [ ] RAG æœå‹™å¯ä»¥é€£æ¥åˆ° Ollama (æª¢æŸ¥ `/stats` ç«¯é»)
- [ ] æ¸¬è©¦æ„åœ–è™•ç†æˆåŠŸè¿”å› JSON

---

**æœ€å¾Œæ›´æ–°**: 2026-02-14
**ç‰ˆæœ¬**: 1.0.0
**ç¶­è­·è€…**: Nephoran Intent Operator Team
